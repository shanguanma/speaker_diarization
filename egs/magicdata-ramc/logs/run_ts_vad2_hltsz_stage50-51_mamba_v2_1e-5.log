The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-12-19 14:56:36,968 (train_accelerate_ddp:853) INFO: params: {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_der': inf, 'best_valid_der': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 500, 'reset_interval': 200, 'valid_interval': 500, 'batch_size': 64, 'verbose': 1, 'world_size': 2, 'tensorboard': True, 'num_epochs': 20, 'max_updates': 40000, 'warmup_updates': 4000, 'freeze_updates': 4000, 'start_batch': 0, 'start_epoch': 1, 'seed': 1337, 'exp_dir': PosixPath('/data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2'), 'save_every_n': 1500, 'keep_last_k': 1, 'keep_last_epoch': 1, 'grad_clip': True, 'feature_grad_mult': 0.1, 'lr': 1e-05, 'average_period': 200, 'train_on_average': False, 'musan_path': '/data/maduo/datasets/musan', 'rir_path': '/data/maduo/datasets/RIRS_NOISES', 'spk_path': '/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', 'speaker_embedding_name_dir': 'cam++_zh-cn_200k_feature_dir', 'data_dir': '/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', 'dataset_name': 'magicdata-ramc', 'max_num_speaker': 4, 'speech_encoder_path': '/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', 'select_encoder_layer_nums': 6, 'wavlm_fuse_feat_post_norm': False, 'speech_encoder_config': '/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', 'speech_encoder_type': 'CAM++', 'speaker_embed_dim': 192, 'rs_len': 4, 'segment_shift': 2, 'single_backend_type': 'mamba_v2', 'multi_backend_type': 'mamba_v2', 'do_finetune': False, 'init_modules': None, 'finetune_ckpt': None}
2024-12-19 14:56:36,971 (train_accelerate_ddp:869) INFO: data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=2, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/data/maduo/datasets/musan', rir_path='/data/maduo/datasets/RIRS_NOISES')
2024-12-19 14:56:36,974 (train_accelerate_ddp:853) INFO: params: {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_der': inf, 'best_valid_der': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 500, 'reset_interval': 200, 'valid_interval': 500, 'batch_size': 64, 'verbose': 1, 'world_size': 2, 'tensorboard': True, 'num_epochs': 20, 'max_updates': 40000, 'warmup_updates': 4000, 'freeze_updates': 4000, 'start_batch': 0, 'start_epoch': 1, 'seed': 1337, 'exp_dir': PosixPath('/data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2'), 'save_every_n': 1500, 'keep_last_k': 1, 'keep_last_epoch': 1, 'grad_clip': True, 'feature_grad_mult': 0.1, 'lr': 1e-05, 'average_period': 200, 'train_on_average': False, 'musan_path': '/data/maduo/datasets/musan', 'rir_path': '/data/maduo/datasets/RIRS_NOISES', 'spk_path': '/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', 'speaker_embedding_name_dir': 'cam++_zh-cn_200k_feature_dir', 'data_dir': '/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', 'dataset_name': 'magicdata-ramc', 'max_num_speaker': 4, 'speech_encoder_path': '/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', 'select_encoder_layer_nums': 6, 'wavlm_fuse_feat_post_norm': False, 'speech_encoder_config': '/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', 'speech_encoder_type': 'CAM++', 'speaker_embed_dim': 192, 'rs_len': 4, 'segment_shift': 2, 'single_backend_type': 'mamba_v2', 'multi_backend_type': 'mamba_v2', 'do_finetune': False, 'init_modules': None, 'finetune_ckpt': None}
2024-12-19 14:56:36,974 (train_accelerate_ddp:869) INFO: data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=2, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/data/maduo/datasets/musan', rir_path='/data/maduo/datasets/RIRS_NOISES')
  0%|          | 0/38 [00:00<?, ?it/s]  0%|          | 0/38 [00:00<?, ?it/s] 63%|██████▎   | 24/38 [00:00<00:00, 233.43it/s] 71%|███████   | 27/38 [00:00<00:00, 260.51it/s]100%|██████████| 38/38 [00:00<00:00, 265.83it/s]
2024-12-19 14:56:37,133 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-19 14:56:37,134 (ts_vad_dataset:160) INFO: loaded sentence=17812, shortest sent=3840.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
100%|██████████| 38/38 [00:00<00:00, 247.74it/s]
2024-12-19 14:56:37,143 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-19 14:56:37,144 (ts_vad_dataset:160) INFO: loaded sentence=17812, shortest sent=3840.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
  0%|          | 0/578 [00:00<?, ?it/s]  0%|          | 0/578 [00:00<?, ?it/s]  5%|▍         | 27/578 [00:00<00:02, 262.37it/s]  5%|▍         | 27/578 [00:00<00:02, 259.74it/s] 10%|▉         | 56/578 [00:00<00:01, 273.86it/s] 10%|▉         | 56/578 [00:00<00:01, 273.48it/s] 15%|█▍        | 84/578 [00:00<00:02, 174.87it/s] 15%|█▍        | 84/578 [00:00<00:02, 174.30it/s] 19%|█▉        | 110/578 [00:00<00:02, 199.29it/s] 19%|█▉        | 110/578 [00:00<00:02, 198.71it/s] 23%|██▎       | 133/578 [00:00<00:02, 206.29it/s] 23%|██▎       | 135/578 [00:00<00:02, 212.40it/s] 27%|██▋       | 156/578 [00:00<00:02, 207.00it/s] 28%|██▊       | 161/578 [00:00<00:01, 225.57it/s] 31%|███▏      | 182/578 [00:00<00:01, 221.39it/s] 32%|███▏      | 186/578 [00:00<00:01, 232.00it/s] 36%|███▌      | 207/578 [00:00<00:01, 228.53it/s] 37%|███▋      | 212/578 [00:00<00:01, 238.15it/s] 40%|████      | 233/578 [00:01<00:01, 235.56it/s] 41%|████      | 237/578 [00:01<00:01, 239.75it/s] 45%|████▍     | 259/578 [00:01<00:01, 239.86it/s] 46%|████▌     | 263/578 [00:01<00:01, 243.84it/s] 49%|████▉     | 284/578 [00:01<00:01, 156.07it/s] 50%|████▉     | 288/578 [00:01<00:01, 157.47it/s] 54%|█████▎    | 310/578 [00:01<00:01, 177.83it/s] 54%|█████▍    | 313/578 [00:01<00:01, 176.70it/s] 58%|█████▊    | 336/578 [00:01<00:01, 196.03it/s] 59%|█████▉    | 340/578 [00:01<00:01, 197.76it/s] 63%|██████▎   | 366/578 [00:01<00:01, 211.80it/s] 63%|██████▎   | 363/578 [00:01<00:01, 212.31it/s] 67%|██████▋   | 389/578 [00:01<00:00, 223.69it/s] 68%|██████▊   | 393/578 [00:01<00:00, 225.11it/s] 72%|███████▏  | 419/578 [00:01<00:00, 233.24it/s] 72%|███████▏  | 416/578 [00:01<00:00, 234.59it/s] 77%|███████▋  | 445/578 [00:02<00:00, 240.19it/s] 76%|███████▋  | 441/578 [00:02<00:00, 238.30it/s] 81%|████████  | 467/578 [00:02<00:00, 242.25it/s] 81%|████████▏ | 471/578 [00:02<00:00, 243.32it/s] 85%|████████▌ | 493/578 [00:02<00:00, 245.08it/s] 86%|████████▌ | 497/578 [00:02<00:00, 245.85it/s] 90%|████████▉ | 519/578 [00:02<00:00, 141.78it/s] 90%|█████████ | 523/578 [00:02<00:00, 142.06it/s] 95%|█████████▍| 548/578 [00:02<00:00, 162.13it/s] 94%|█████████▍| 545/578 [00:02<00:00, 162.40it/s] 99%|█████████▉| 573/578 [00:02<00:00, 180.71it/s] 99%|█████████▉| 571/578 [00:02<00:00, 182.08it/s]100%|██████████| 578/578 [00:02<00:00, 203.40it/s]
2024-12-19 14:56:40,209 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
100%|██████████| 578/578 [00:02<00:00, 201.70it/s]
2024-12-19 14:56:40,220 (ts_vad_dataset:160) INFO: loaded sentence=269074, shortest sent=48640.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=True, musan=True, noise_ratio=0.8, zero_ratio=0.3 
2024-12-19 14:56:40,220 (train_accelerate_ddp:904) INFO: The scale window is set to 8192.
2024-12-19 14:56:40,224 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-19 14:56:40,234 (ts_vad_dataset:160) INFO: loaded sentence=269074, shortest sent=48640.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=True, musan=True, noise_ratio=0.8, zero_ratio=0.3 
2024-12-19 14:56:40,234 (train_accelerate_ddp:904) INFO: The scale window is set to 8192.
2024-12-19 14:56:40,732 (train_accelerate_ddp:940) INFO: model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba_v2', multi_backend_type='mamba_v2')
self.wavlm_fuse_feat_post_norm: False
2024-12-19 14:56:40,734 (train_accelerate_ddp:940) INFO: model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba_v2', multi_backend_type='mamba_v2')
self.wavlm_fuse_feat_post_norm: False
2024-12-19 14:56:41,416 (train_accelerate_ddp:943) INFO: model: TSVADModel(
  (rs_dropout): Dropout(p=0.1, inplace=False)
  (speech_encoder): CAMPPlus(
    (head): FCM(
      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layer1): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (layer2): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (xvector): Sequential(
      (tdnn): TDNNLayer(
        (linear): Conv1d(320, 128, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (block1): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(160, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(192, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(224, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit1): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block2): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (16): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (17): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (18): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (19): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (20): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (21): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (22): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (23): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit2): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block3): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit3): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (out_nonlinear): Sequential(
        (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (stats): StatsPool()
      (dense): DenseLayer(
        (linear): Conv1d(1024, 192, kernel_size=(1,), stride=(1,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
  (speech_down_or_up): Sequential(
    (0): Conv1d(512, 192, kernel_size=(5,), stride=(2,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (single_backend): MambaBlock(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (backend_down): Sequential(
    (0): Conv1d(1536, 384, kernel_size=(5,), stride=(1,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (multi_backend): MambaBlock(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
  )
  (fc): Linear(in_features=384, out_features=4, bias=True)
)
2024-12-19 14:56:41,418 (train_accelerate_ddp:945) INFO: Number of model parameters: 18890980
2024-12-19 14:56:41,439 (train_accelerate_ddp:943) INFO: model: TSVADModel(
  (rs_dropout): Dropout(p=0.1, inplace=False)
  (speech_encoder): CAMPPlus(
    (head): FCM(
      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layer1): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (layer2): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (xvector): Sequential(
      (tdnn): TDNNLayer(
        (linear): Conv1d(320, 128, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (block1): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(160, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(192, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(224, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit1): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block2): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (16): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (17): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (18): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (19): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (20): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (21): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (22): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (23): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit2): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block3): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit3): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (out_nonlinear): Sequential(
        (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (stats): StatsPool()
      (dense): DenseLayer(
        (linear): Conv1d(1024, 192, kernel_size=(1,), stride=(1,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
  (speech_down_or_up): Sequential(
    (0): Conv1d(512, 192, kernel_size=(5,), stride=(2,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (single_backend): MambaBlock(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (backend_down): Sequential(
    (0): Conv1d(1536, 384, kernel_size=(5,), stride=(1,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (multi_backend): MambaBlock(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
    )
  )
  (fc): Linear(in_features=384, out_features=4, bias=True)
)
2024-12-19 14:56:41,442 (train_accelerate_ddp:945) INFO: Number of model parameters: 18890980
hltsz02:2570705:2570705 [0] NCCL INFO Bootstrap : Using eno1:10.26.1.136<0>
hltsz02:2570705:2570705 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
hltsz02:2570705:2570705 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
hltsz02:2570705:2570705 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.6+cuda11.8
hltsz02:2570706:2570706 [1] NCCL INFO cudaDriverVersion 12050
hltsz02:2570706:2570706 [1] NCCL INFO Bootstrap : Using eno1:10.26.1.136<0>
hltsz02:2570706:2570706 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
hltsz02:2570706:2570706 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
hltsz02:2570706:2570898 [1] NCCL INFO Failed to open libibverbs.so[.1]
hltsz02:2570706:2570898 [1] NCCL INFO NET/Socket : Using [0]eno1:10.26.1.136<0> [1]eno2:192.168.1.102<0>
hltsz02:2570706:2570898 [1] NCCL INFO Using network Socket
hltsz02:2570705:2570897 [0] NCCL INFO Failed to open libibverbs.so[.1]
hltsz02:2570705:2570897 [0] NCCL INFO NET/Socket : Using [0]eno1:10.26.1.136<0> [1]eno2:192.168.1.102<0>
hltsz02:2570705:2570897 [0] NCCL INFO Using network Socket
hltsz02:2570705:2570897 [0] NCCL INFO comm 0xb427cc0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xa57a58e91106f21d - Init START
hltsz02:2570706:2570898 [1] NCCL INFO comm 0xbc7f340 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 3d000 commId 0xa57a58e91106f21d - Init START
hltsz02:2570706:2570898 [1] NCCL INFO Setting affinity for GPU 1 to 07,80000000,00007800
hltsz02:2570705:2570897 [0] NCCL INFO Setting affinity for GPU 0 to 07,80000000,00007800
hltsz02:2570705:2570897 [0] NCCL INFO Channel 00/02 :    0   1
hltsz02:2570705:2570897 [0] NCCL INFO Channel 01/02 :    0   1
hltsz02:2570706:2570898 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
hltsz02:2570705:2570897 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
hltsz02:2570706:2570898 [1] NCCL INFO P2P Chunksize set to 131072
hltsz02:2570705:2570897 [0] NCCL INFO P2P Chunksize set to 131072
hltsz02:2570705:2570897 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
hltsz02:2570705:2570897 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
hltsz02:2570706:2570898 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
hltsz02:2570706:2570898 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
hltsz02:2570706:2570898 [1] NCCL INFO Connected all rings
hltsz02:2570706:2570898 [1] NCCL INFO Connected all trees
hltsz02:2570706:2570898 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
hltsz02:2570706:2570898 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
hltsz02:2570705:2570897 [0] NCCL INFO Connected all rings
hltsz02:2570705:2570897 [0] NCCL INFO Connected all trees
hltsz02:2570705:2570897 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
hltsz02:2570705:2570897 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
hltsz02:2570706:2570898 [1] NCCL INFO comm 0xbc7f340 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 3d000 commId 0xa57a58e91106f21d - Init COMPLETE
hltsz02:2570705:2570897 [0] NCCL INFO comm 0xb427cc0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1b000 commId 0xa57a58e91106f21d - Init COMPLETE
2024-12-19 14:56:42,351 (train_accelerate_ddp:1024) INFO: start training from epoch 1
2024-12-19 14:56:42,351 (train_accelerate_ddp:1025) INFO: Train set grouped total_num_itrs = 2103
2024-12-19 14:56:42,351 (train_accelerate_ddp:1024) INFO: start training from epoch 1
2024-12-19 14:56:42,351 (train_accelerate_ddp:1025) INFO: Train set grouped total_num_itrs = 2103
2024-12-19 14:56:48,662 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 0, num_updates: 0, {'loss': 1.0934293270111084, 'DER': 2.1186503522432334, 'ACC': 0.4619140625, 'MI': 0.0003707823507601038, 'FA': 1.6831664812754912, 'CF': 0.43511308861698184}, batch size: 64, grad_norm: 7.239465713500977, grad_scale: , lr: 5.00e-09, 
2024-12-19 14:56:48,674 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 0, num_updates: 0, {'loss': 1.1062968969345093, 'DER': 2.0423546302943287, 'ACC': 0.4578515625, 'MI': 0.0003589375448671931, 'FA': 1.5935032304379038, 'CF': 0.4484924623115578}, batch size: 64, grad_norm: 7.239465713500977, grad_scale: , lr: 5.00e-09, 
2024-12-19 15:11:14,878 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 500, num_updates: 500, {'loss': 0.7420705556869507, 'DER': 1.3023048061461497, 'ACC': 0.5811328125, 'MI': 0.03644809719492585, 'FA': 0.6523137395033053, 'CF': 0.6135429694479185}, batch size: 64, grad_norm: 4.9993743896484375, grad_scale: , lr: 2.51e-06, 
2024-12-19 15:11:14,878 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:11:14,884 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 500, num_updates: 500, {'loss': 0.7091163992881775, 'DER': 1.3037105751391467, 'ACC': 0.6108203125, 'MI': 0.041187384044526903, 'FA': 0.7178107606679035, 'CF': 0.5447124304267161}, batch size: 64, grad_norm: 4.9993743896484375, grad_scale: , lr: 2.51e-06, 
2024-12-19 15:11:14,884 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:12:25,931 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 500,  validation: loss=0.6724, DER=1.129, ACC=0.5976, MI=0.1686, FA=0.1714, CF=0.7887, over 0.00 frames. 
2024-12-19 15:12:25,932 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3229MB
2024-12-19 15:12:28,708 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 500,  validation: loss=0.6723, DER=1.129, ACC=0.5981, MI=0.1702, FA=0.1719, CF=0.787, over 0.00 frames. 
2024-12-19 15:12:28,709 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3231MB
2024-12-19 15:27:33,057 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1000, num_updates: 1000, {'loss': 0.4853307008743286, 'DER': 0.9232420826623725, 'ACC': 0.7771875, 'MI': 0.7974592950438361, 'FA': 0.02844873859366613, 'CF': 0.09733404902487028}, batch size: 64, grad_norm: 0.8970113396644592, grad_scale: , lr: 5.00e-06, 
2024-12-19 15:27:33,057 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1000, num_updates: 1000, {'loss': 0.47399216890335083, 'DER': 0.9024523160762943, 'ACC': 0.7794140625, 'MI': 0.7536784741144414, 'FA': 0.025431425976385105, 'CF': 0.12334241598546776}, batch size: 64, grad_norm: 0.8970113396644592, grad_scale: , lr: 5.00e-06, 
2024-12-19 15:27:33,058 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:27:33,058 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:28:44,930 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1000,  validation: loss=0.3577, DER=0.8794, ACC=0.7985, MI=0.7907, FA=0.01074, CF=0.07796, over 0.00 frames. 
2024-12-19 15:28:44,930 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3231MB
2024-12-19 15:28:47,439 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1000,  validation: loss=0.3589, DER=0.8832, ACC=0.798, MI=0.7938, FA=0.01109, CF=0.07833, over 0.00 frames. 
2024-12-19 15:28:47,439 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3229MB
2024-12-19 15:43:28,763 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-1500.pt
2024-12-19 15:43:31,674 (checkpoint:349) WARNING: No checkpoints found in /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2
2024-12-19 15:43:33,550 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1500, num_updates: 1500, {'loss': 0.4799520969390869, 'DER': 0.9244838826512133, 'ACC': 0.7726953125, 'MI': 0.7582397681999276, 'FA': 0.03694313654473017, 'CF': 0.1293009779065556}, batch size: 64, grad_norm: 0.9179176092147827, grad_scale: , lr: 7.51e-06, 
2024-12-19 15:43:33,550 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:43:33,555 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1500, num_updates: 1500, {'loss': 0.4829290807247162, 'DER': 0.9420975965040058, 'ACC': 0.7710555207026348, 'MI': 0.7822286962855062, 'FA': 0.0387836853605244, 'CF': 0.12108521485797524}, batch size: 64, grad_norm: 0.9179176092147827, grad_scale: , lr: 7.51e-06, 
2024-12-19 15:43:33,555 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:44:39,948 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1500,  validation: loss=0.3186, DER=0.7076, ACC=0.8211, MI=0.4816, FA=0.08185, CF=0.1441, over 0.00 frames. 
2024-12-19 15:44:39,949 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3231MB
2024-12-19 15:44:42,334 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1500,  validation: loss=0.3211, DER=0.722, ACC=0.8166, MI=0.4898, FA=0.07933, CF=0.1529, over 0.00 frames. 
2024-12-19 15:44:42,334 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3229MB
2024-12-19 15:59:14,095 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 2000, num_updates: 2000, {'loss': 0.36534637212753296, 'DER': 0.7001416932341481, 'ACC': 0.8215234375, 'MI': 0.5070846617074035, 'FA': 0.08395324123273114, 'CF': 0.10910379029401346}, batch size: 64, grad_norm: 0.8204297423362732, grad_scale: , lr: 1.00e-05, 
2024-12-19 15:59:14,096 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:59:14,096 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 2000, num_updates: 2000, {'loss': 0.4060889482498169, 'DER': 0.7895870736086176, 'ACC': 0.8015234375, 'MI': 0.6003590664272891, 'FA': 0.06660682226211849, 'CF': 0.12262118491921005}, batch size: 64, grad_norm: 0.8204297423362732, grad_scale: , lr: 1.00e-05, 
2024-12-19 15:59:14,096 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:00:20,162 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 2000,  validation: loss=0.257, DER=0.486, ACC=0.8701, MI=0.2169, FA=0.141, CF=0.1281, over 0.00 frames. 
2024-12-19 16:00:20,162 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3231MB
2024-12-19 16:00:24,168 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 2000,  validation: loss=0.2575, DER=0.493, ACC=0.8665, MI=0.2182, FA=0.1344, CF=0.1404, over 0.00 frames. 
2024-12-19 16:00:24,169 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3229MB
2024-12-19 16:03:13,178 (train_accelerate_ddp:709) INFO: end of epoch 1, batch_idx: 2102 batch_idx_train: 2102, {'loss': 0.3691215515136719, 'DER': 0.7194645844952593, 'ACC': 0.8208203125, 'MI': 0.49395798475553077, 'FA': 0.0922104480386689, 'CF': 0.13329615170105968}, batch size: 64, grad_norm: 0.8998841047286987, grad_scale: , lr: 9.94e-06, 
2024-12-19 16:03:13,179 (train_accelerate_ddp:709) INFO: end of epoch 1, batch_idx: 2102 batch_idx_train: 2102, {'loss': 0.37701180577278137, 'DER': 0.7161939615736505, 'ACC': 0.8151171875, 'MI': 0.47044830741079596, 'FA': 0.0958828911253431, 'CF': 0.14986276303751145}, batch size: 64, grad_norm: 0.8998841047286987, grad_scale: , lr: 9.94e-06, 
2024-12-19 16:03:13,180 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-1.pt
2024-12-19 16:03:15,340 (train_accelerate_ddp:557) INFO:  end of epoch 1, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-1.pt 
2024-12-19 16:03:29,798 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2103, num_updates: 2000, {'loss': 0.410074919462204, 'DER': 0.7684587813620072, 'ACC': 0.7998671252149445, 'MI': 0.5347670250896057, 'FA': 0.08440860215053764, 'CF': 0.1492831541218638}, batch size: 64, grad_norm: 0.9086018800735474, grad_scale: , lr: 9.94e-06, 
2024-12-19 16:03:29,803 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2103, num_updates: 2000, {'loss': 0.4200086295604706, 'DER': 0.7835820895522388, 'ACC': 0.805234375, 'MI': 0.5623134328358209, 'FA': 0.07462686567164178, 'CF': 0.14664179104477612}, batch size: 64, grad_norm: 0.9086018800735474, grad_scale: , lr: 9.94e-06, 
2024-12-19 16:17:35,473 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2603, num_updates: 2500, {'loss': 0.3664722740650177, 'DER': 0.70492700729927, 'ACC': 0.82421875, 'MI': 0.48594890510948907, 'FA': 0.10273722627737226, 'CF': 0.11624087591240875}, batch size: 64, grad_norm: 0.7493896484375, grad_scale: , lr: 9.66e-06, 
2024-12-19 16:17:35,473 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2603, num_updates: 2500, {'loss': 0.3802776634693146, 'DER': 0.7497263772345859, 'ACC': 0.8119140625, 'MI': 0.5200656694636994, 'FA': 0.10105800802626778, 'CF': 0.12860269974461874}, batch size: 64, grad_norm: 0.7493896484375, grad_scale: , lr: 9.66e-06, 
2024-12-19 16:17:35,474 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:17:35,474 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:18:41,477 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 2603,  validation: loss=0.2434, DER=0.5157, ACC=0.8618, MI=0.1671, FA=0.215, CF=0.1336, over 0.00 frames. 
2024-12-19 16:18:41,478 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3231MB
2024-12-19 16:18:46,382 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 2603,  validation: loss=0.2407, DER=0.5091, ACC=0.8638, MI=0.1674, FA=0.2068, CF=0.1349, over 0.00 frames. 
2024-12-19 16:18:46,382 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3229MB
2024-12-19 16:30:04,498 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-3000.pt
2024-12-19 16:30:07,384 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 16:33:04,229 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3103, num_updates: 3000, {'loss': 0.3285914659500122, 'DER': 0.6621122599704579, 'ACC': 0.84015625, 'MI': 0.5005539143279173, 'FA': 0.0681314623338257, 'CF': 0.09342688330871492}, batch size: 64, grad_norm: 0.6252277493476868, grad_scale: , lr: 9.39e-06, 
2024-12-19 16:33:04,230 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:33:04,230 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3103, num_updates: 3000, {'loss': 0.3958011865615845, 'DER': 0.7862554112554112, 'ACC': 0.7970703125, 'MI': 0.4884559884559885, 'FA': 0.147005772005772, 'CF': 0.15079365079365079}, batch size: 64, grad_norm: 0.6252277493476868, grad_scale: , lr: 9.39e-06, 
2024-12-19 16:33:04,230 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:34:10,622 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3103,  validation: loss=0.2358, DER=0.5255, ACC=0.8669, MI=0.1497, FA=0.2743, CF=0.1014, over 0.00 frames. 
2024-12-19 16:34:10,623 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3231MB
2024-12-19 16:34:15,252 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3103,  validation: loss=0.2332, DER=0.5176, ACC=0.8686, MI=0.1507, FA=0.2629, CF=0.104, over 0.00 frames. 
2024-12-19 16:34:15,253 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3229MB
2024-12-19 16:48:13,103 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3603, num_updates: 3500, {'loss': 0.3159934878349304, 'DER': 0.6557991380925614, 'ACC': 0.83765625, 'MI': 0.40322278433576914, 'FA': 0.129660858160015, 'CF': 0.12291549559677721}, batch size: 64, grad_norm: 0.7439519166946411, grad_scale: , lr: 9.11e-06, 
2024-12-19 16:48:13,103 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3603, num_updates: 3500, {'loss': 0.3493095934391022, 'DER': 0.7190337813294587, 'ACC': 0.819296875, 'MI': 0.4671267707954958, 'FA': 0.13076643661460224, 'CF': 0.1211405739193607}, batch size: 64, grad_norm: 0.7439519166946411, grad_scale: , lr: 9.11e-06, 
2024-12-19 16:48:13,104 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:48:13,104 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:49:18,541 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3603,  validation: loss=0.228, DER=0.423, ACC=0.8869, MI=0.1304, FA=0.1813, CF=0.1114, over 0.00 frames. 
2024-12-19 16:49:18,541 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3231MB
2024-12-19 16:49:24,801 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3603,  validation: loss=0.2289, DER=0.4285, ACC=0.8841, MI=0.1322, FA=0.1748, CF=0.1215, over 0.00 frames. 
2024-12-19 16:49:24,801 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 3229MB
2024-12-19 17:03:44,282 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 4103, num_updates: 4000, {'loss': 0.34560346603393555, 'DER': 0.6796186364454039, 'ACC': 0.83125, 'MI': 0.46069436949091563, 'FA': 0.12142471667566108, 'CF': 0.09749955027882713}, batch size: 64, grad_norm: 0.6015808582305908, grad_scale: , lr: 8.83e-06, 
2024-12-19 17:03:44,283 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:03:44,287 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 4103, num_updates: 4000, {'loss': 0.31962287425994873, 'DER': 0.6372991514713847, 'ACC': 0.8474609375, 'MI': 0.4721068784979238, 'FA': 0.09749052175482939, 'CF': 0.06770175121863152}, batch size: 64, grad_norm: 0.6015808582305908, grad_scale: , lr: 8.83e-06, 
2024-12-19 17:03:44,288 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:04:50,092 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 4103,  validation: loss=0.2225, DER=0.4492, ACC=0.8847, MI=0.07877, FA=0.2791, CF=0.0913, over 0.00 frames. 
2024-12-19 17:04:50,093 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 17:04:55,713 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 4103,  validation: loss=0.2224, DER=0.4471, ACC=0.8842, MI=0.08077, FA=0.2676, CF=0.09878, over 0.00 frames. 
2024-12-19 17:04:55,713 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 17:08:11,548 (train_accelerate_ddp:709) INFO: end of epoch 2, batch_idx: 2102 batch_idx_train: 4205, {'loss': 0.3143346607685089, 'DER': 0.6559893858984079, 'ACC': 0.84234375, 'MI': 0.43631539044730855, 'FA': 0.1106899166034875, 'CF': 0.10898407884761183}, batch size: 64, grad_norm: 0.875023365020752, grad_scale: , lr: 8.77e-06, 
2024-12-19 17:08:11,550 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-2.pt
2024-12-19 17:08:11,553 (train_accelerate_ddp:709) INFO: end of epoch 2, batch_idx: 2102 batch_idx_train: 4205, {'loss': 0.3666180670261383, 'DER': 0.7463274811895378, 'ACC': 0.8120603407847429, 'MI': 0.48907201719813687, 'FA': 0.14206377642422072, 'CF': 0.11519168756718022}, batch size: 64, grad_norm: 0.875023365020752, grad_scale: , lr: 8.77e-06, 
2024-12-19 17:08:14,308 (train_accelerate_ddp:557) INFO:  end of epoch 2, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-2.pt 
2024-12-19 17:08:26,539 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4206, num_updates: 4000, {'loss': 0.3556337058544159, 'DER': 0.7249010435408421, 'ACC': 0.8215234375, 'MI': 0.5482187837351565, 'FA': 0.07952500899604174, 'CF': 0.09715725080964376}, batch size: 64, grad_norm: 0.6944630146026611, grad_scale: , lr: 8.77e-06, 
2024-12-19 17:08:26,540 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4206, num_updates: 4000, {'loss': 0.3166366517543793, 'DER': 0.6721279500682661, 'ACC': 0.84875, 'MI': 0.4745465184318315, 'FA': 0.11449190559781548, 'CF': 0.08308952603861908}, batch size: 64, grad_norm: 0.6944630146026611, grad_scale: , lr: 8.77e-06, 
2024-12-19 17:17:14,118 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-4500.pt
2024-12-19 17:17:17,794 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 17:17:17,851 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 17:23:36,065 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4706, num_updates: 4500, {'loss': 0.30926093459129333, 'DER': 0.6051439956529614, 'ACC': 0.8485546875, 'MI': 0.42582865422930627, 'FA': 0.08223147980438326, 'CF': 0.09708386161927188}, batch size: 64, grad_norm: 0.6529200673103333, grad_scale: , lr: 8.50e-06, 
2024-12-19 17:23:36,066 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:23:36,066 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4706, num_updates: 4500, {'loss': 0.36133119463920593, 'DER': 0.700076074553062, 'ACC': 0.835625, 'MI': 0.4809813617344998, 'FA': 0.11886648915937618, 'CF': 0.100228223659186}, batch size: 64, grad_norm: 0.6529200673103333, grad_scale: , lr: 8.50e-06, 
2024-12-19 17:23:36,066 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:24:42,318 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 4706,  validation: loss=0.2163, DER=0.3879, ACC=0.8865, MI=0.08184, FA=0.1651, CF=0.1409, over 0.00 frames. 
2024-12-19 17:24:42,318 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 17:24:45,919 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 4706,  validation: loss=0.2168, DER=0.3896, ACC=0.8863, MI=0.08293, FA=0.1638, CF=0.1428, over 0.00 frames. 
2024-12-19 17:24:45,919 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 17:39:42,992 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5206, num_updates: 5000, {'loss': 0.29491403698921204, 'DER': 0.5688255457333574, 'ACC': 0.857265625, 'MI': 0.3564856575861447, 'FA': 0.12195561970052318, 'CF': 0.09038426844668952}, batch size: 64, grad_norm: 0.7088105082511902, grad_scale: , lr: 8.22e-06, 
2024-12-19 17:39:42,992 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:39:42,995 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5206, num_updates: 5000, {'loss': 0.3231464922428131, 'DER': 0.6182266009852216, 'ACC': 0.8385546875, 'MI': 0.44106263194933143, 'FA': 0.06826178747361013, 'CF': 0.10890218156228008}, batch size: 64, grad_norm: 0.7088105082511902, grad_scale: , lr: 8.22e-06, 
2024-12-19 17:39:42,995 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:40:48,917 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5206,  validation: loss=0.1989, DER=0.365, ACC=0.8968, MI=0.06195, FA=0.1868, CF=0.1163, over 0.00 frames. 
2024-12-19 17:40:48,917 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 17:40:52,186 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5206,  validation: loss=0.1965, DER=0.3515, ACC=0.9002, MI=0.06094, FA=0.1735, CF=0.1171, over 0.00 frames. 
2024-12-19 17:40:52,186 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 17:55:56,391 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5706, num_updates: 5500, {'loss': 0.2609231173992157, 'DER': 0.5362373977553738, 'ACC': 0.8708203125, 'MI': 0.30511698687464334, 'FA': 0.13829180140764694, 'CF': 0.09282860947308351}, batch size: 64, grad_norm: 0.7606803178787231, grad_scale: , lr: 7.94e-06, 
2024-12-19 17:55:56,392 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5706, num_updates: 5500, {'loss': 0.2639722228050232, 'DER': 0.48769574944071586, 'ACC': 0.87609375, 'MI': 0.2917598806860552, 'FA': 0.09228187919463088, 'CF': 0.10365398956002983}, batch size: 64, grad_norm: 0.7606803178787231, grad_scale: , lr: 7.94e-06, 
2024-12-19 17:55:56,392 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:55:56,392 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:57:02,032 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5706,  validation: loss=0.1875, DER=0.3164, ACC=0.9125, MI=0.06092, FA=0.1593, CF=0.09613, over 0.00 frames. 
2024-12-19 17:57:02,033 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 17:57:09,710 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5706,  validation: loss=0.1879, DER=0.3187, ACC=0.9113, MI=0.06307, FA=0.1537, CF=0.102, over 0.00 frames. 
2024-12-19 17:57:09,710 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 18:05:56,878 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-6000.pt
2024-12-19 18:06:00,455 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 18:12:10,465 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 6206, num_updates: 6000, {'loss': 0.2808944284915924, 'DER': 0.5137061403508771, 'ACC': 0.8607421875, 'MI': 0.2680921052631579, 'FA': 0.10782163742690058, 'CF': 0.13779239766081872}, batch size: 64, grad_norm: 0.8084244132041931, grad_scale: , lr: 7.66e-06, 
2024-12-19 18:12:10,465 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:12:10,471 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 6206, num_updates: 6000, {'loss': 0.274149090051651, 'DER': 0.5372859510219112, 'ACC': 0.8665390026574957, 'MI': 0.31228134781808137, 'FA': 0.133492911066102, 'CF': 0.09151169213772786}, batch size: 64, grad_norm: 0.8084244132041931, grad_scale: , lr: 7.66e-06, 
2024-12-19 18:12:10,472 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:13:16,251 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 6206,  validation: loss=0.1822, DER=0.3128, ACC=0.9139, MI=0.05539, FA=0.1648, CF=0.0926, over 0.00 frames. 
2024-12-19 18:13:16,252 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 18:13:20,103 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 6206,  validation: loss=0.1825, DER=0.3121, ACC=0.9132, MI=0.05337, FA=0.1594, CF=0.09938, over 0.00 frames. 
2024-12-19 18:13:20,103 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 18:16:36,339 (train_accelerate_ddp:709) INFO: end of epoch 3, batch_idx: 2102 batch_idx_train: 6308, {'loss': 0.30722057819366455, 'DER': 0.6118941880671944, 'ACC': 0.850390625, 'MI': 0.3342344081869087, 'FA': 0.15002896312029348, 'CF': 0.12763081675999227}, batch size: 64, grad_norm: 0.8060552477836609, grad_scale: , lr: 7.61e-06, 
2024-12-19 18:16:36,341 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-3.pt
2024-12-19 18:16:36,341 (train_accelerate_ddp:709) INFO: end of epoch 3, batch_idx: 2102 batch_idx_train: 6308, {'loss': 0.292941689491272, 'DER': 0.5232578862947781, 'ACC': 0.863359375, 'MI': 0.3154517911245767, 'FA': 0.10764569595437533, 'CF': 0.10016039921582606}, batch size: 64, grad_norm: 0.8060552477836609, grad_scale: , lr: 7.61e-06, 
2024-12-19 18:16:39,126 (train_accelerate_ddp:557) INFO:  end of epoch 3, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-3.pt 
2024-12-19 18:16:48,747 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6309, num_updates: 6000, {'loss': 0.26530587673187256, 'DER': 0.4965581395348837, 'ACC': 0.877890625, 'MI': 0.30623255813953487, 'FA': 0.10530232558139535, 'CF': 0.0850232558139535}, batch size: 64, grad_norm: 0.7131061553955078, grad_scale: , lr: 7.61e-06, 
2024-12-19 18:16:48,749 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6309, num_updates: 6000, {'loss': 0.3035246729850769, 'DER': 0.5750322224268091, 'ACC': 0.8469938938468765, 'MI': 0.33400846989504696, 'FA': 0.09629902412078807, 'CF': 0.14472472841097403}, batch size: 64, grad_norm: 0.7131061553955078, grad_scale: , lr: 7.61e-06, 
2024-12-19 18:31:52,336 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6809, num_updates: 6500, {'loss': 0.27002963423728943, 'DER': 0.4944684042752672, 'ACC': 0.868671875, 'MI': 0.2679542471404463, 'FA': 0.09056816051003187, 'CF': 0.13594599662478904}, batch size: 64, grad_norm: 0.9302008748054504, grad_scale: , lr: 7.33e-06, 
2024-12-19 18:31:52,336 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:31:52,336 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6809, num_updates: 6500, {'loss': 0.27011337876319885, 'DER': 0.525940212150434, 'ACC': 0.863125, 'MI': 0.2742526518804243, 'FA': 0.10183220829315333, 'CF': 0.14985535197685632}, batch size: 64, grad_norm: 0.9302008748054504, grad_scale: , lr: 7.33e-06, 
2024-12-19 18:31:52,337 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:32:57,953 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 6809,  validation: loss=0.1776, DER=0.309, ACC=0.9138, MI=0.04436, FA=0.1682, CF=0.09641, over 0.00 frames. 
2024-12-19 18:32:57,954 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 18:33:04,767 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 6809,  validation: loss=0.1768, DER=0.3056, ACC=0.9146, MI=0.04372, FA=0.1627, CF=0.09912, over 0.00 frames. 
2024-12-19 18:33:04,767 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 18:47:56,392 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7309, num_updates: 7000, {'loss': 0.25998225808143616, 'DER': 0.4981308411214953, 'ACC': 0.864375, 'MI': 0.2274766355140187, 'FA': 0.11981308411214953, 'CF': 0.1508411214953271}, batch size: 64, grad_norm: 1.0563995838165283, grad_scale: , lr: 7.05e-06, 
2024-12-19 18:47:56,393 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:47:56,394 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7309, num_updates: 7000, {'loss': 0.31241461634635925, 'DER': 0.5628977910894796, 'ACC': 0.85375, 'MI': 0.3199176338450019, 'FA': 0.10501684762261325, 'CF': 0.13796330962186448}, batch size: 64, grad_norm: 1.0563995838165283, grad_scale: , lr: 7.05e-06, 
2024-12-19 18:47:56,394 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:49:01,611 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7309,  validation: loss=0.1728, DER=0.2912, ACC=0.9177, MI=0.04705, FA=0.1484, CF=0.09578, over 0.00 frames. 
2024-12-19 18:49:01,611 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 18:49:09,614 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7309,  validation: loss=0.1712, DER=0.2907, ACC=0.918, MI=0.0463, FA=0.1466, CF=0.09787, over 0.00 frames. 
2024-12-19 18:49:09,614 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 18:54:57,180 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-7500.pt
2024-12-19 18:55:00,948 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 18:55:01,014 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 19:04:09,928 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7809, num_updates: 7500, {'loss': 0.25475791096687317, 'DER': 0.4476155404161296, 'ACC': 0.879453125, 'MI': 0.2581476707788621, 'FA': 0.06886392929478917, 'CF': 0.12060394034247837}, batch size: 64, grad_norm: 0.9070897102355957, grad_scale: , lr: 6.77e-06, 
2024-12-19 19:04:09,928 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:04:09,931 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7809, num_updates: 7500, {'loss': 0.22166931629180908, 'DER': 0.36038903625110524, 'ACC': 0.898515625, 'MI': 0.20159151193633953, 'FA': 0.059770114942528735, 'CF': 0.09902740937223696}, batch size: 64, grad_norm: 0.9070897102355957, grad_scale: , lr: 6.77e-06, 
2024-12-19 19:04:09,932 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:05:15,196 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7809,  validation: loss=0.1702, DER=0.2705, ACC=0.9229, MI=0.0518, FA=0.125, CF=0.09374, over 0.00 frames. 
2024-12-19 19:05:15,197 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 19:05:21,189 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7809,  validation: loss=0.1692, DER=0.2705, ACC=0.9228, MI=0.05123, FA=0.1228, CF=0.09646, over 0.00 frames. 
2024-12-19 19:05:21,189 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 19:20:07,591 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 8309, num_updates: 8000, {'loss': 0.19456318020820618, 'DER': 0.3475897064153679, 'ACC': 0.9090234375, 'MI': 0.1689017760057992, 'FA': 0.10420442189198985, 'CF': 0.07448350851757883}, batch size: 64, grad_norm: 0.8694062829017639, grad_scale: , lr: 6.49e-06, 
2024-12-19 19:20:07,592 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:20:07,596 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 8309, num_updates: 8000, {'loss': 0.21716947853565216, 'DER': 0.40970654627539504, 'ACC': 0.8966796875, 'MI': 0.23118886380737397, 'FA': 0.09066967644845748, 'CF': 0.08784800601956358}, batch size: 64, grad_norm: 0.8694062829017639, grad_scale: , lr: 6.49e-06, 
2024-12-19 19:20:07,597 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:21:14,832 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 8309,  validation: loss=0.1652, DER=0.248, ACC=0.9267, MI=0.05354, FA=0.09666, CF=0.09782, over 0.00 frames. 
2024-12-19 19:21:14,833 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 19:21:17,602 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 8309,  validation: loss=0.1641, DER=0.2468, ACC=0.9271, MI=0.05281, FA=0.09382, CF=0.1002, over 0.00 frames. 
2024-12-19 19:21:17,603 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 19:24:13,276 (train_accelerate_ddp:709) INFO: end of epoch 4, batch_idx: 2102 batch_idx_train: 8411, {'loss': 0.23628300428390503, 'DER': 0.38983693591202123, 'ACC': 0.8968359375, 'MI': 0.1937808115282518, 'FA': 0.08513462267728479, 'CF': 0.11092150170648464}, batch size: 64, grad_norm: 0.868425726890564, grad_scale: , lr: 6.44e-06, 
2024-12-19 19:24:13,278 (train_accelerate_ddp:709) INFO: end of epoch 4, batch_idx: 2102 batch_idx_train: 8411, {'loss': 0.20971925556659698, 'DER': 0.35264841805901176, 'ACC': 0.8985546875, 'MI': 0.1724137931034483, 'FA': 0.07127621756132244, 'CF': 0.10895840739424102}, batch size: 64, grad_norm: 0.868425726890564, grad_scale: , lr: 6.44e-06, 
2024-12-19 19:24:13,280 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-4.pt
2024-12-19 19:24:16,150 (train_accelerate_ddp:557) INFO:  end of epoch 4, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-4.pt 
2024-12-19 19:24:27,382 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8412, num_updates: 8000, {'loss': 0.23634032905101776, 'DER': 0.4025541365907829, 'ACC': 0.8897208718833307, 'MI': 0.21210438645197113, 'FA': 0.07236720340551546, 'CF': 0.11808254673329632}, batch size: 64, grad_norm: 1.0521513223648071, grad_scale: , lr: 6.44e-06, 
2024-12-19 19:24:27,388 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8412, num_updates: 8000, {'loss': 0.24189095199108124, 'DER': 0.41336542021473527, 'ACC': 0.8928515625, 'MI': 0.23731951129211404, 'FA': 0.08163643095149944, 'CF': 0.09440947797112181}, batch size: 64, grad_norm: 1.0521513223648071, grad_scale: , lr: 6.44e-06, 
2024-12-19 19:39:14,582 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8912, num_updates: 8500, {'loss': 0.26224273443222046, 'DER': 0.4334144970968346, 'ACC': 0.882265625, 'MI': 0.22607229818318036, 'FA': 0.07623150402697135, 'CF': 0.1311106948866829}, batch size: 64, grad_norm: 0.8522670269012451, grad_scale: , lr: 6.16e-06, 
2024-12-19 19:39:14,583 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:39:14,588 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8912, num_updates: 8500, {'loss': 0.2036992609500885, 'DER': 0.33303297891511985, 'ACC': 0.9148828125, 'MI': 0.19426923770048657, 'FA': 0.07911335375743377, 'CF': 0.05965038745719949}, batch size: 64, grad_norm: 0.8522670269012451, grad_scale: , lr: 6.16e-06, 
2024-12-19 19:39:14,589 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:40:19,360 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 8912,  validation: loss=0.16, DER=0.2652, ACC=0.925, MI=0.04656, FA=0.1312, CF=0.08745, over 0.00 frames. 
2024-12-19 19:40:19,360 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 19:40:25,133 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 8912,  validation: loss=0.1578, DER=0.2598, ACC=0.9268, MI=0.04618, FA=0.1265, CF=0.08706, over 0.00 frames. 
2024-12-19 19:40:25,133 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 19:43:01,478 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-9000.pt
2024-12-19 19:43:05,072 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 19:43:05,161 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 19:55:29,575 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9412, num_updates: 9000, {'loss': 0.21782901883125305, 'DER': 0.3880053650124545, 'ACC': 0.8983984375, 'MI': 0.17570415788465224, 'FA': 0.10193523663537075, 'CF': 0.1103659704924315}, batch size: 64, grad_norm: 0.950589120388031, grad_scale: , lr: 5.88e-06, 
2024-12-19 19:55:29,576 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:55:29,587 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9412, num_updates: 9000, {'loss': 0.207929328083992, 'DER': 0.3970503734916683, 'ACC': 0.9024542754416133, 'MI': 0.19957862478452404, 'FA': 0.1164527868224478, 'CF': 0.08101896188469641}, batch size: 64, grad_norm: 0.950589120388031, grad_scale: , lr: 5.88e-06, 
2024-12-19 19:55:29,587 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:56:35,017 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9412,  validation: loss=0.1538, DER=0.2327, ACC=0.9322, MI=0.05552, FA=0.08959, CF=0.08762, over 0.00 frames. 
2024-12-19 19:56:35,017 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 19:56:40,030 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9412,  validation: loss=0.1531, DER=0.2311, ACC=0.9328, MI=0.05599, FA=0.0864, CF=0.08876, over 0.00 frames. 
2024-12-19 19:56:40,030 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 20:11:51,397 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9912, num_updates: 9500, {'loss': 0.2350253462791443, 'DER': 0.4214828897338403, 'ACC': 0.8875, 'MI': 0.19068441064638783, 'FA': 0.10475285171102662, 'CF': 0.12604562737642586}, batch size: 64, grad_norm: 0.8656032085418701, grad_scale: , lr: 5.60e-06, 
2024-12-19 20:11:51,397 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9912, num_updates: 9500, {'loss': 0.1935873180627823, 'DER': 0.3224204524553982, 'ACC': 0.918984375, 'MI': 0.18778738274783888, 'FA': 0.07559315799153946, 'CF': 0.05903991171601986}, batch size: 64, grad_norm: 0.8656032085418701, grad_scale: , lr: 5.60e-06, 
2024-12-19 20:11:51,398 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:11:51,398 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:12:56,856 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9912,  validation: loss=0.1542, DER=0.2418, ACC=0.9296, MI=0.05667, FA=0.09517, CF=0.08992, over 0.00 frames. 
2024-12-19 20:12:56,857 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 20:13:04,482 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9912,  validation: loss=0.1526, DER=0.2382, ACC=0.9308, MI=0.05598, FA=0.09248, CF=0.08977, over 0.00 frames. 
2024-12-19 20:13:04,483 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 20:27:39,632 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 10412, num_updates: 10000, {'loss': 0.2574836015701294, 'DER': 0.39957902122434663, 'ACC': 0.8825390625, 'MI': 0.214699175583231, 'FA': 0.05700754253639712, 'CF': 0.12787230310471848}, batch size: 64, grad_norm: 0.9865685701370239, grad_scale: , lr: 5.33e-06, 
2024-12-19 20:27:39,632 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:27:39,639 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 10412, num_updates: 10000, {'loss': 0.22855371236801147, 'DER': 0.39867799811142585, 'ACC': 0.8941796875, 'MI': 0.20679886685552407, 'FA': 0.0789423984891407, 'CF': 0.1129367327667611}, batch size: 64, grad_norm: 0.9865685701370239, grad_scale: , lr: 5.33e-06, 
2024-12-19 20:27:39,639 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:28:44,753 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 10412,  validation: loss=0.1478, DER=0.2221, ACC=0.9334, MI=0.04773, FA=0.08257, CF=0.09179, over 0.00 frames. 
2024-12-19 20:28:44,754 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 20:28:52,636 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 10412,  validation: loss=0.1467, DER=0.218, ACC=0.9347, MI=0.04718, FA=0.07763, CF=0.09315, over 0.00 frames. 
2024-12-19 20:28:52,636 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 20:31:30,124 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-10500.pt
2024-12-19 20:31:33,668 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 20:31:59,820 (train_accelerate_ddp:709) INFO: end of epoch 5, batch_idx: 2102 batch_idx_train: 10514, {'loss': 0.20874708890914917, 'DER': 0.36580251620282117, 'ACC': 0.9116796875, 'MI': 0.18223408311094166, 'FA': 0.11837590545177278, 'CF': 0.06519252764010675}, batch size: 64, grad_norm: 1.338975429534912, grad_scale: , lr: 5.27e-06, 
2024-12-19 20:31:59,822 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-5.pt
2024-12-19 20:31:59,830 (train_accelerate_ddp:709) INFO: end of epoch 5, batch_idx: 2102 batch_idx_train: 10514, {'loss': 0.2461884468793869, 'DER': 0.4227941176470588, 'ACC': 0.8838011604202604, 'MI': 0.24301470588235294, 'FA': 0.05772058823529412, 'CF': 0.12205882352941176}, batch size: 64, grad_norm: 1.338975429534912, grad_scale: , lr: 5.27e-06, 
2024-12-19 20:32:02,553 (train_accelerate_ddp:557) INFO:  end of epoch 5, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-5.pt 
2024-12-19 20:32:15,072 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 10515, num_updates: 10000, {'loss': 0.23274151980876923, 'DER': 0.39868565169769987, 'ACC': 0.889140625, 'MI': 0.19989047097480833, 'FA': 0.07940854326396495, 'CF': 0.11938663745892661}, batch size: 64, grad_norm: 1.0179678201675415, grad_scale: , lr: 5.27e-06, 
2024-12-19 20:32:15,078 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 10515, num_updates: 10000, {'loss': 0.2358814924955368, 'DER': 0.38368690563277247, 'ACC': 0.8944921875, 'MI': 0.1781272860277981, 'FA': 0.09528163862472568, 'CF': 0.11027798098024871}, batch size: 64, grad_norm: 1.0179678201675415, grad_scale: , lr: 5.27e-06, 
2024-12-19 20:47:04,852 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11015, num_updates: 10500, {'loss': 0.203489750623703, 'DER': 0.3151943462897526, 'ACC': 0.9075390625, 'MI': 0.1704946996466431, 'FA': 0.04169611307420495, 'CF': 0.1030035335689046}, batch size: 64, grad_norm: 1.0567346811294556, grad_scale: , lr: 4.99e-06, 
2024-12-19 20:47:04,853 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:47:04,859 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11015, num_updates: 10500, {'loss': 0.1985989809036255, 'DER': 0.32103064066852366, 'ACC': 0.9052734375, 'MI': 0.1693941504178273, 'FA': 0.0504874651810585, 'CF': 0.10114902506963788}, batch size: 64, grad_norm: 1.0567346811294556, grad_scale: , lr: 4.99e-06, 
2024-12-19 20:47:04,859 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:48:10,817 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11015,  validation: loss=0.1418, DER=0.2085, ACC=0.9382, MI=0.06406, FA=0.06135, CF=0.08312, over 0.00 frames. 
2024-12-19 20:48:10,817 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 20:48:14,530 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11015,  validation: loss=0.1412, DER=0.2076, ACC=0.9388, MI=0.0635, FA=0.06039, CF=0.08368, over 0.00 frames. 
2024-12-19 20:48:14,531 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:02:59,680 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11515, num_updates: 11000, {'loss': 0.2351691871881485, 'DER': 0.3935368043087971, 'ACC': 0.89, 'MI': 0.20628366247755836, 'FA': 0.07522441651705565, 'CF': 0.11202872531418312}, batch size: 64, grad_norm: 1.0655025243759155, grad_scale: , lr: 4.71e-06, 
2024-12-19 21:02:59,681 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:02:59,686 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11515, num_updates: 11000, {'loss': 0.1987982839345932, 'DER': 0.3382648401826484, 'ACC': 0.9116796875, 'MI': 0.19799086757990866, 'FA': 0.06557077625570776, 'CF': 0.07470319634703196}, batch size: 64, grad_norm: 1.0655025243759155, grad_scale: , lr: 4.71e-06, 
2024-12-19 21:02:59,686 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:04:05,590 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11515,  validation: loss=0.1407, DER=0.2097, ACC=0.9373, MI=0.05652, FA=0.06761, CF=0.08561, over 0.00 frames. 
2024-12-19 21:04:05,590 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:04:11,621 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11515,  validation: loss=0.1397, DER=0.2074, ACC=0.9384, MI=0.05596, FA=0.06597, CF=0.08548, over 0.00 frames. 
2024-12-19 21:04:11,621 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:18:30,331 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-12000.pt
2024-12-19 21:18:34,188 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 21:18:34,285 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 21:19:03,051 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12015, num_updates: 11500, {'loss': 0.22812514007091522, 'DER': 0.36337522441651704, 'ACC': 0.89015625, 'MI': 0.13913824057450627, 'FA': 0.08276481149012567, 'CF': 0.1414721723518851}, batch size: 64, grad_norm: 1.0380158424377441, grad_scale: , lr: 4.44e-06, 
2024-12-19 21:19:03,051 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:19:03,056 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12015, num_updates: 11500, {'loss': 0.19454161822795868, 'DER': 0.32710280373831774, 'ACC': 0.9108984375, 'MI': 0.1557632398753894, 'FA': 0.0804471321238776, 'CF': 0.09089243173905076}, batch size: 64, grad_norm: 1.0380158424377441, grad_scale: , lr: 4.44e-06, 
2024-12-19 21:19:03,056 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:20:08,580 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12015,  validation: loss=0.1395, DER=0.2007, ACC=0.9395, MI=0.05466, FA=0.06097, CF=0.0851, over 0.00 frames. 
2024-12-19 21:20:08,581 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:20:13,504 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12015,  validation: loss=0.1382, DER=0.1973, ACC=0.941, MI=0.05396, FA=0.06, CF=0.08337, over 0.00 frames. 
2024-12-19 21:20:13,504 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:34:59,620 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12515, num_updates: 12000, {'loss': 0.20102590322494507, 'DER': 0.31581848245452876, 'ACC': 0.9134375, 'MI': 0.16002204666544184, 'FA': 0.06448649641741687, 'CF': 0.09130993937167003}, batch size: 64, grad_norm: 1.0232501029968262, grad_scale: , lr: 4.16e-06, 
2024-12-19 21:34:59,621 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:34:59,629 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12515, num_updates: 12000, {'loss': 0.19449712336063385, 'DER': 0.32611090708777085, 'ACC': 0.913515625, 'MI': 0.1911494674990819, 'FA': 0.05453543885420492, 'CF': 0.08042600073448403}, batch size: 64, grad_norm: 1.0232501029968262, grad_scale: , lr: 4.16e-06, 
2024-12-19 21:34:59,630 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:36:04,693 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12515,  validation: loss=0.1372, DER=0.191, ACC=0.9424, MI=0.05365, FA=0.05534, CF=0.08203, over 0.00 frames. 
2024-12-19 21:36:04,693 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:36:10,952 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12515,  validation: loss=0.1375, DER=0.1923, ACC=0.9424, MI=0.05363, FA=0.05572, CF=0.08297, over 0.00 frames. 
2024-12-19 21:36:10,952 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:38:59,784 (train_accelerate_ddp:709) INFO: end of epoch 6, batch_idx: 2102 batch_idx_train: 12617, {'loss': 0.23059183359146118, 'DER': 0.40541522805742325, 'ACC': 0.8935546875, 'MI': 0.2240596038524441, 'FA': 0.09158640741413775, 'CF': 0.08976921679084136}, batch size: 64, grad_norm: 1.2082014083862305, grad_scale: , lr: 4.10e-06, 
2024-12-19 21:38:59,791 (train_accelerate_ddp:709) INFO: end of epoch 6, batch_idx: 2102 batch_idx_train: 12617, {'loss': 0.21840181946754456, 'DER': 0.34717882742142986, 'ACC': 0.906328125, 'MI': 0.17790847270722293, 'FA': 0.0757213747472891, 'CF': 0.09354897996691784}, batch size: 64, grad_norm: 1.2082014083862305, grad_scale: , lr: 4.10e-06, 
2024-12-19 21:38:59,793 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-6.pt
2024-12-19 21:39:02,981 (train_accelerate_ddp:557) INFO:  end of epoch 6, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-6.pt 
2024-12-19 21:39:14,387 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 12618, num_updates: 12000, {'loss': 0.1812090277671814, 'DER': 0.2941286812980679, 'ACC': 0.9227734375, 'MI': 0.1269930594635153, 'FA': 0.09041455636841118, 'CF': 0.07672106546614144}, batch size: 64, grad_norm: 1.1670345067977905, grad_scale: , lr: 4.10e-06, 
2024-12-19 21:39:14,388 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 12618, num_updates: 12000, {'loss': 0.23012927174568176, 'DER': 0.35497914021403953, 'ACC': 0.901015625, 'MI': 0.18592417921276982, 'FA': 0.0643932523127154, 'CF': 0.10466170868855433}, batch size: 64, grad_norm: 1.1670345067977905, grad_scale: , lr: 4.10e-06, 
2024-12-19 21:53:54,579 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13118, num_updates: 12500, {'loss': 0.21341286599636078, 'DER': 0.3395028125567048, 'ACC': 0.9005859375, 'MI': 0.15296679368535657, 'FA': 0.06423516603157321, 'CF': 0.122300852839775}, batch size: 64, grad_norm: 0.909956693649292, grad_scale: , lr: 3.82e-06, 
2024-12-19 21:53:54,580 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:53:54,584 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13118, num_updates: 12500, {'loss': 0.19003607332706451, 'DER': 0.3214079630698211, 'ACC': 0.913984375, 'MI': 0.12521638776687824, 'FA': 0.09405654933641085, 'CF': 0.10213502596653203}, batch size: 64, grad_norm: 0.909956693649292, grad_scale: , lr: 3.82e-06, 
2024-12-19 21:53:54,584 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:55:00,014 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13118,  validation: loss=0.1327, DER=0.1909, ACC=0.9423, MI=0.04882, FA=0.06002, CF=0.08202, over 0.00 frames. 
2024-12-19 21:55:00,014 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 21:55:08,041 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13118,  validation: loss=0.1326, DER=0.1907, ACC=0.9427, MI=0.04859, FA=0.05939, CF=0.08271, over 0.00 frames. 
2024-12-19 21:55:08,041 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 22:06:14,364 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-13500.pt
2024-12-19 22:06:17,908 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 22:06:17,995 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 22:09:44,867 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13618, num_updates: 13000, {'loss': 0.18503770232200623, 'DER': 0.2827428168517539, 'ACC': 0.9212109375, 'MI': 0.1634056054997356, 'FA': 0.046536224219989424, 'CF': 0.07280098713202891}, batch size: 64, grad_norm: 0.9309118390083313, grad_scale: , lr: 3.54e-06, 
2024-12-19 22:09:44,867 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:09:44,875 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13618, num_updates: 13000, {'loss': 0.18237218260765076, 'DER': 0.2801243371731578, 'ACC': 0.9263671875, 'MI': 0.1336624611446334, 'FA': 0.08191625525690255, 'CF': 0.06454562077162188}, batch size: 64, grad_norm: 0.9309118390083313, grad_scale: , lr: 3.54e-06, 
2024-12-19 22:09:44,875 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:10:50,290 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13618,  validation: loss=0.1338, DER=0.1923, ACC=0.9425, MI=0.05938, FA=0.05304, CF=0.07988, over 0.00 frames. 
2024-12-19 22:10:50,291 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 22:10:54,914 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13618,  validation: loss=0.1334, DER=0.1927, ACC=0.9424, MI=0.05875, FA=0.05289, CF=0.08105, over 0.00 frames. 
2024-12-19 22:10:54,914 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 22:25:50,344 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14118, num_updates: 13500, {'loss': 0.23827944695949554, 'DER': 0.3996986249764551, 'ACC': 0.8889453125, 'MI': 0.16330759088340555, 'FA': 0.10058391410811829, 'CF': 0.13580711998493125}, batch size: 64, grad_norm: 1.0397672653198242, grad_scale: , lr: 3.27e-06, 
2024-12-19 22:25:50,344 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14118, num_updates: 13500, {'loss': 0.22516116499900818, 'DER': 0.3301766527044254, 'ACC': 0.90390625, 'MI': 0.16408668730650156, 'FA': 0.04826079038426516, 'CF': 0.11782917501365871}, batch size: 64, grad_norm: 1.0397672653198242, grad_scale: , lr: 3.27e-06, 
2024-12-19 22:25:50,345 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:25:50,345 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:26:58,758 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14118,  validation: loss=0.1344, DER=0.192, ACC=0.942, MI=0.05959, FA=0.05035, CF=0.08204, over 0.00 frames. 
2024-12-19 22:26:58,758 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 22:27:00,945 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14118,  validation: loss=0.134, DER=0.1927, ACC=0.9419, MI=0.05843, FA=0.05077, CF=0.08347, over 0.00 frames. 
2024-12-19 22:27:00,946 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 22:41:51,464 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14618, num_updates: 14000, {'loss': 0.20998911559581757, 'DER': 0.33811512620301437, 'ACC': 0.9071875, 'MI': 0.17468676230252406, 'FA': 0.07009260940621027, 'CF': 0.09333575449428001}, batch size: 64, grad_norm: 1.0124073028564453, grad_scale: , lr: 2.99e-06, 
2024-12-19 22:41:51,465 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:41:51,468 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14618, num_updates: 14000, {'loss': 0.19861602783203125, 'DER': 0.33673273042139695, 'ACC': 0.9076953125, 'MI': 0.13180681162208965, 'FA': 0.08697325380026939, 'CF': 0.1179526649990379}, batch size: 64, grad_norm: 1.0124073028564453, grad_scale: , lr: 2.99e-06, 
2024-12-19 22:41:51,468 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:42:59,640 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14618,  validation: loss=0.1328, DER=0.19, ACC=0.9432, MI=0.05547, FA=0.05549, CF=0.07904, over 0.00 frames. 
2024-12-19 22:42:59,641 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 22:43:00,670 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14618,  validation: loss=0.132, DER=0.1893, ACC=0.9434, MI=0.05469, FA=0.05532, CF=0.07928, over 0.00 frames. 
2024-12-19 22:43:00,670 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 22:46:06,981 (train_accelerate_ddp:709) INFO: end of epoch 7, batch_idx: 2102 batch_idx_train: 14720, {'loss': 0.19753986597061157, 'DER': 0.3325393917185782, 'ACC': 0.911796875, 'MI': 0.19457676804690363, 'FA': 0.05679736167094174, 'CF': 0.08116526200073287}, batch size: 64, grad_norm: 0.9775561094284058, grad_scale: , lr: 2.93e-06, 
2024-12-19 22:46:06,987 (train_accelerate_ddp:709) INFO: end of epoch 7, batch_idx: 2102 batch_idx_train: 14720, {'loss': 0.18179717659950256, 'DER': 0.299545282303903, 'ACC': 0.9255078125, 'MI': 0.15517241379310345, 'FA': 0.0826070481242895, 'CF': 0.06176582038651004}, batch size: 64, grad_norm: 0.9775561094284058, grad_scale: , lr: 2.93e-06, 
2024-12-19 22:46:06,989 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-7.pt
2024-12-19 22:46:09,947 (train_accelerate_ddp:557) INFO:  end of epoch 7, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-7.pt 
2024-12-19 22:46:21,485 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 14721, num_updates: 14000, {'loss': 0.2122594565153122, 'DER': 0.36478454680534916, 'ACC': 0.8975, 'MI': 0.16437592867756315, 'FA': 0.07782317979197623, 'CF': 0.1225854383358098}, batch size: 64, grad_norm: 0.9986999034881592, grad_scale: , lr: 2.93e-06, 
2024-12-19 22:46:21,487 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 14721, num_updates: 14000, {'loss': 0.19544172286987305, 'DER': 0.32278596108383345, 'ACC': 0.908671875, 'MI': 0.154755410074559, 'FA': 0.06564829969085288, 'CF': 0.10238225131842153}, batch size: 64, grad_norm: 0.9986999034881592, grad_scale: , lr: 2.93e-06, 
2024-12-19 22:54:36,220 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-15000.pt
2024-12-19 22:54:39,943 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 22:54:40,020 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 23:01:11,271 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15221, num_updates: 14500, {'loss': 0.1963566094636917, 'DER': 0.2931156848828957, 'ACC': 0.916953125, 'MI': 0.17814052519517387, 'FA': 0.030872959545777148, 'CF': 0.08410220014194464}, batch size: 64, grad_norm: 1.1733531951904297, grad_scale: , lr: 2.65e-06, 
2024-12-19 23:01:11,272 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:01:11,276 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15221, num_updates: 14500, {'loss': 0.20512165129184723, 'DER': 0.33048803117461495, 'ACC': 0.909609375, 'MI': 0.16960475041751716, 'FA': 0.06197810354425682, 'CF': 0.09890517721284098}, batch size: 64, grad_norm: 1.1733531951904297, grad_scale: , lr: 2.65e-06, 
2024-12-19 23:01:11,277 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:02:19,006 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15221,  validation: loss=0.1303, DER=0.1918, ACC=0.9427, MI=0.06371, FA=0.04897, CF=0.07908, over 0.00 frames. 
2024-12-19 23:02:19,007 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:02:22,876 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15221,  validation: loss=0.1288, DER=0.1894, ACC=0.9438, MI=0.06238, FA=0.04906, CF=0.07798, over 0.00 frames. 
2024-12-19 23:02:22,876 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:17:16,888 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15721, num_updates: 15000, {'loss': 0.18643541634082794, 'DER': 0.2813234496474417, 'ACC': 0.9218359375, 'MI': 0.13596094738745254, 'FA': 0.06490688844693546, 'CF': 0.0804556138130537}, batch size: 64, grad_norm: 0.9963915944099426, grad_scale: , lr: 2.38e-06, 
2024-12-19 23:17:16,888 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:17:16,891 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15721, num_updates: 15000, {'loss': 0.24415788054466248, 'DER': 0.4003391107761869, 'ACC': 0.8865625, 'MI': 0.18048229088168802, 'FA': 0.07309721175584025, 'CF': 0.14675960813865863}, batch size: 64, grad_norm: 0.9963915944099426, grad_scale: , lr: 2.38e-06, 
2024-12-19 23:17:16,891 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:18:24,794 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15721,  validation: loss=0.129, DER=0.1902, ACC=0.9436, MI=0.0633, FA=0.05021, CF=0.07669, over 0.00 frames. 
2024-12-19 23:18:24,794 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:18:28,621 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15721,  validation: loss=0.1278, DER=0.1892, ACC=0.9441, MI=0.06239, FA=0.05053, CF=0.0763, over 0.00 frames. 
2024-12-19 23:18:28,622 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:33:29,833 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16221, num_updates: 15500, {'loss': 0.2161942720413208, 'DER': 0.3604004449388209, 'ACC': 0.8996875, 'MI': 0.14757137560252132, 'FA': 0.0971449758991472, 'CF': 0.11568409343715239}, batch size: 64, grad_norm: 0.9136956334114075, grad_scale: , lr: 2.10e-06, 
2024-12-19 23:33:29,833 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:33:29,840 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16221, num_updates: 15500, {'loss': 0.19088280200958252, 'DER': 0.32138979370249726, 'ACC': 0.9154296875, 'MI': 0.19960188201230547, 'FA': 0.051393412956930874, 'CF': 0.07039449873326095}, batch size: 64, grad_norm: 0.9136956334114075, grad_scale: , lr: 2.10e-06, 
2024-12-19 23:33:29,840 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:34:38,523 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16221,  validation: loss=0.1269, DER=0.1875, ACC=0.9434, MI=0.05517, FA=0.05269, CF=0.07969, over 0.00 frames. 
2024-12-19 23:34:38,524 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:34:39,427 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16221,  validation: loss=0.1254, DER=0.1847, ACC=0.9447, MI=0.05381, FA=0.05173, CF=0.07917, over 0.00 frames. 
2024-12-19 23:34:39,427 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:42:44,004 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-16500.pt
2024-12-19 23:42:47,964 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 23:49:29,223 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16721, num_updates: 16000, {'loss': 0.1727418750524521, 'DER': 0.2800785433773652, 'ACC': 0.9201953125, 'MI': 0.14012852552659763, 'FA': 0.05533737950731882, 'CF': 0.08461263834344877}, batch size: 64, grad_norm: 0.9770945906639099, grad_scale: , lr: 1.82e-06, 
2024-12-19 23:49:29,224 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:49:29,226 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16721, num_updates: 16000, {'loss': 0.2188202291727066, 'DER': 0.3492291143779132, 'ACC': 0.8978961363991866, 'MI': 0.17371817855862315, 'FA': 0.05665112943707422, 'CF': 0.11885980638221585}, batch size: 64, grad_norm: 0.9770945906639099, grad_scale: , lr: 1.82e-06, 
2024-12-19 23:49:29,227 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:50:34,545 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16721,  validation: loss=0.1251, DER=0.1816, ACC=0.9454, MI=0.06035, FA=0.04422, CF=0.07707, over 0.00 frames. 
2024-12-19 23:50:34,546 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:50:38,806 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16721,  validation: loss=0.1242, DER=0.1809, ACC=0.946, MI=0.0593, FA=0.04446, CF=0.07709, over 0.00 frames. 
2024-12-19 23:50:38,807 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-19 23:53:38,891 (train_accelerate_ddp:709) INFO: end of epoch 8, batch_idx: 2102 batch_idx_train: 16823, {'loss': 0.2420945018529892, 'DER': 0.4240506329113924, 'ACC': 0.882109375, 'MI': 0.22332730560578662, 'FA': 0.07902350813743218, 'CF': 0.1216998191681736}, batch size: 64, grad_norm: 1.2827699184417725, grad_scale: , lr: 1.76e-06, 
2024-12-19 23:53:38,893 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-8.pt
2024-12-19 23:53:38,895 (train_accelerate_ddp:709) INFO: end of epoch 8, batch_idx: 2102 batch_idx_train: 16823, {'loss': 0.1981249302625656, 'DER': 0.3418867924528302, 'ACC': 0.9108203125, 'MI': 0.17433962264150943, 'FA': 0.07867924528301887, 'CF': 0.08886792452830189}, batch size: 64, grad_norm: 1.2827699184417725, grad_scale: , lr: 1.76e-06, 
2024-12-19 23:53:41,829 (train_accelerate_ddp:557) INFO:  end of epoch 8, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-8.pt 
2024-12-19 23:53:53,103 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 16824, num_updates: 16000, {'loss': 0.20984140038490295, 'DER': 0.3468755693204591, 'ACC': 0.9049609375, 'MI': 0.18181818181818182, 'FA': 0.0686828201858262, 'CF': 0.09637456731645108}, batch size: 64, grad_norm: 0.9928978085517883, grad_scale: , lr: 1.76e-06, 
2024-12-19 23:53:53,105 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 16824, num_updates: 16000, {'loss': 0.1878945529460907, 'DER': 0.3058297634568821, 'ACC': 0.9209375, 'MI': 0.15552244365803689, 'FA': 0.07915813000558763, 'CF': 0.07114918979325759}, batch size: 64, grad_norm: 0.9928978085517883, grad_scale: , lr: 1.76e-06, 
2024-12-20 00:08:44,446 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17324, num_updates: 16500, {'loss': 0.24456864595413208, 'DER': 0.3638035681441972, 'ACC': 0.895703125, 'MI': 0.17031451167923486, 'FA': 0.06621298510207835, 'CF': 0.12727607136288394}, batch size: 64, grad_norm: 1.2125941514968872, grad_scale: , lr: 1.49e-06, 
2024-12-20 00:08:44,447 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:08:44,457 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17324, num_updates: 16500, {'loss': 0.21584133803844452, 'DER': 0.3149908592321755, 'ACC': 0.9119140625, 'MI': 0.1522851919561243, 'FA': 0.06544789762340036, 'CF': 0.09725776965265082}, batch size: 64, grad_norm: 1.2125941514968872, grad_scale: , lr: 1.49e-06, 
2024-12-20 00:08:44,457 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:09:50,335 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17324,  validation: loss=0.1265, DER=0.1826, ACC=0.9451, MI=0.05712, FA=0.04826, CF=0.07726, over 0.00 frames. 
2024-12-20 00:09:50,336 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 00:09:53,461 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17324,  validation: loss=0.1255, DER=0.1831, ACC=0.9451, MI=0.05621, FA=0.04844, CF=0.0784, over 0.00 frames. 
2024-12-20 00:09:53,462 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 00:24:35,919 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17824, num_updates: 17000, {'loss': 0.17542797327041626, 'DER': 0.27746908048037283, 'ACC': 0.9266015625, 'MI': 0.17386628428033699, 'FA': 0.044273167234271375, 'CF': 0.05932962896576447}, batch size: 64, grad_norm: 1.13306725025177, grad_scale: , lr: 1.21e-06, 
2024-12-20 00:24:35,920 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:24:35,923 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17824, num_updates: 17000, {'loss': 0.1918993890285492, 'DER': 0.2958827634333566, 'ACC': 0.9147265625, 'MI': 0.1674808094905792, 'FA': 0.04344033496161898, 'CF': 0.08496161898115841}, batch size: 64, grad_norm: 1.13306725025177, grad_scale: , lr: 1.21e-06, 
2024-12-20 00:24:35,923 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:25:41,472 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17824,  validation: loss=0.1282, DER=0.1838, ACC=0.9446, MI=0.05968, FA=0.04541, CF=0.07876, over 0.00 frames. 
2024-12-20 00:25:41,473 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 00:25:49,247 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17824,  validation: loss=0.1275, DER=0.184, ACC=0.9447, MI=0.05887, FA=0.04599, CF=0.07918, over 0.00 frames. 
2024-12-20 00:25:49,247 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 00:30:59,678 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-18000.pt
2024-12-20 00:31:03,282 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 00:31:03,373 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 00:40:35,402 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18324, num_updates: 17500, {'loss': 0.22083736956119537, 'DER': 0.35924292539507535, 'ACC': 0.8969140625, 'MI': 0.16244027930907753, 'FA': 0.0711135611907387, 'CF': 0.1256890848952591}, batch size: 64, grad_norm: 1.040625810623169, grad_scale: , lr: 9.31e-07, 
2024-12-20 00:40:35,402 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18324, num_updates: 17500, {'loss': 0.1711685061454773, 'DER': 0.26946870451237265, 'ACC': 0.9237890625, 'MI': 0.1168122270742358, 'FA': 0.06713973799126638, 'CF': 0.08551673944687045}, batch size: 64, grad_norm: 1.040625810623169, grad_scale: , lr: 9.31e-07, 
2024-12-20 00:40:35,403 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:40:35,403 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:41:40,351 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18324,  validation: loss=0.1283, DER=0.1822, ACC=0.9449, MI=0.05286, FA=0.05036, CF=0.07893, over 0.00 frames. 
2024-12-20 00:41:40,351 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 00:41:45,386 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18324,  validation: loss=0.1274, DER=0.182, ACC=0.9454, MI=0.05236, FA=0.05108, CF=0.07858, over 0.00 frames. 
2024-12-20 00:41:45,386 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 00:56:57,366 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18824, num_updates: 18000, {'loss': 0.17929399013519287, 'DER': 0.31737422853936786, 'ACC': 0.91984375, 'MI': 0.16663549654011595, 'FA': 0.08434636244623153, 'CF': 0.06639236955302039}, batch size: 64, grad_norm: 0.9207759499549866, grad_scale: , lr: 6.53e-07, 
2024-12-20 00:56:57,366 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:56:57,369 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18824, num_updates: 18000, {'loss': 0.1720205843448639, 'DER': 0.28302950573200525, 'ACC': 0.9258984375, 'MI': 0.11557977823717347, 'FA': 0.09396729937981582, 'CF': 0.07348242811501597}, batch size: 64, grad_norm: 0.9207759499549866, grad_scale: , lr: 6.53e-07, 
2024-12-20 00:56:57,369 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:58:03,354 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18824,  validation: loss=0.1259, DER=0.1817, ACC=0.9453, MI=0.05833, FA=0.04602, CF=0.07737, over 0.00 frames. 
2024-12-20 00:58:03,355 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 00:58:08,890 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18824,  validation: loss=0.1248, DER=0.1805, ACC=0.946, MI=0.05725, FA=0.04631, CF=0.07691, over 0.00 frames. 
2024-12-20 00:58:08,890 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 01:01:17,714 (train_accelerate_ddp:709) INFO: end of epoch 9, batch_idx: 2102 batch_idx_train: 18926, {'loss': 0.16322848200798035, 'DER': 0.2589395807644883, 'ACC': 0.9306640625, 'MI': 0.14620398097586754, 'FA': 0.059010040514356175, 'CF': 0.053725559274264575}, batch size: 64, grad_norm: 0.8854435086250305, grad_scale: , lr: 5.96e-07, 
2024-12-20 01:01:17,715 (train_accelerate_ddp:709) INFO: end of epoch 9, batch_idx: 2102 batch_idx_train: 18926, {'loss': 0.1901712268590927, 'DER': 0.31222913133597135, 'ACC': 0.9148828125, 'MI': 0.13943847748257018, 'FA': 0.07442999811569626, 'CF': 0.09836065573770492}, batch size: 64, grad_norm: 0.8854435086250305, grad_scale: , lr: 5.96e-07, 
2024-12-20 01:01:17,717 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-9.pt
2024-12-20 01:01:20,597 (train_accelerate_ddp:557) INFO:  end of epoch 9, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-9.pt 
2024-12-20 01:01:23,348 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 18927, num_updates: 18000, {'loss': 0.1879618912935257, 'DER': 0.30410106188209446, 'ACC': 0.918046875, 'MI': 0.16294397656536067, 'FA': 0.06114976199194434, 'CF': 0.08000732332478945}, batch size: 64, grad_norm: 0.9847105741500854, grad_scale: , lr: 5.96e-07, 
2024-12-20 01:01:23,356 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 18927, num_updates: 18000, {'loss': 0.18133094906806946, 'DER': 0.3003264417845484, 'ACC': 0.9148828125, 'MI': 0.12803772216177003, 'FA': 0.0774392455567646, 'CF': 0.09484947406601378}, batch size: 64, grad_norm: 0.9847105741500854, grad_scale: , lr: 5.96e-07, 
2024-12-20 01:17:07,734 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19427, num_updates: 18500, {'loss': 0.1732943207025528, 'DER': 0.2882753675152384, 'ACC': 0.9216307066916823, 'MI': 0.15991394765148798, 'FA': 0.05718895661527429, 'CF': 0.07117246324847616}, batch size: 64, grad_norm: 0.8238286375999451, grad_scale: , lr: 3.18e-07, 
2024-12-20 01:17:07,734 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:17:07,735 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19427, num_updates: 18500, {'loss': 0.15734577178955078, 'DER': 0.24798091042584436, 'ACC': 0.933515625, 'MI': 0.12022760646108664, 'FA': 0.06332599118942732, 'CF': 0.0644273127753304}, batch size: 64, grad_norm: 0.8238286375999451, grad_scale: , lr: 3.18e-07, 
2024-12-20 01:17:07,735 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:18:14,043 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19427,  validation: loss=0.1285, DER=0.1843, ACC=0.9448, MI=0.06102, FA=0.04574, CF=0.07751, over 0.00 frames. 
2024-12-20 01:18:14,043 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 01:18:19,403 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19427,  validation: loss=0.1276, DER=0.1835, ACC=0.9452, MI=0.06031, FA=0.04609, CF=0.07707, over 0.00 frames. 
2024-12-20 01:18:19,403 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 01:20:34,167 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-19500.pt
2024-12-20 01:20:38,342 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 01:20:38,444 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 01:33:41,195 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19927, num_updates: 19000, {'loss': 0.21332137286663055, 'DER': 0.33802045288531773, 'ACC': 0.9035546875, 'MI': 0.14992695398100803, 'FA': 0.07523739956172389, 'CF': 0.11285609934258582}, batch size: 64, grad_norm: 0.9797579646110535, grad_scale: , lr: 4.00e-08, 
2024-12-20 01:33:41,196 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:33:41,199 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19927, num_updates: 19000, {'loss': 0.19956514239311218, 'DER': 0.32501376399339327, 'ACC': 0.907734375, 'MI': 0.1418608919067719, 'FA': 0.07469260414755001, 'CF': 0.10846026793907139}, batch size: 64, grad_norm: 0.9797579646110535, grad_scale: , lr: 4.00e-08, 
2024-12-20 01:33:41,199 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:34:47,109 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19927,  validation: loss=0.1277, DER=0.1826, ACC=0.945, MI=0.05877, FA=0.04618, CF=0.07766, over 0.00 frames. 
2024-12-20 01:34:47,110 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 01:34:51,462 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19927,  validation: loss=0.1267, DER=0.182, ACC=0.9455, MI=0.05823, FA=0.04672, CF=0.07706, over 0.00 frames. 
2024-12-20 01:34:51,462 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 01:50:00,748 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20427, num_updates: 19500, {'loss': 0.1781599223613739, 'DER': 0.2747561890472618, 'ACC': 0.9265625, 'MI': 0.1282820705176294, 'FA': 0.06864216054013503, 'CF': 0.07783195798949738}, batch size: 64, grad_norm: 0.891035258769989, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:50:00,748 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:50:00,751 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20427, num_updates: 19500, {'loss': 0.17703931033611298, 'DER': 0.27454769014579306, 'ACC': 0.918515625, 'MI': 0.13384858598278587, 'FA': 0.04883189882311611, 'CF': 0.0918672053398911}, batch size: 64, grad_norm: 0.891035258769989, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:50:00,752 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:51:07,005 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20427,  validation: loss=0.1285, DER=0.1818, ACC=0.9449, MI=0.05339, FA=0.04912, CF=0.07933, over 0.00 frames. 
2024-12-20 01:51:07,006 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 01:51:13,475 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20427,  validation: loss=0.1274, DER=0.1812, ACC=0.9454, MI=0.05307, FA=0.04946, CF=0.07866, over 0.00 frames. 
2024-12-20 01:51:13,476 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 02:06:26,820 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20927, num_updates: 20000, {'loss': 0.16719841957092285, 'DER': 0.26944140197152244, 'ACC': 0.9291015625, 'MI': 0.1288791529755385, 'FA': 0.07867834976268712, 'CF': 0.061883899233296825}, batch size: 64, grad_norm: 0.8991148471832275, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:06:26,820 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:06:26,820 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20927, num_updates: 20000, {'loss': 0.16986024379730225, 'DER': 0.2703524780819467, 'ACC': 0.925, 'MI': 0.14886383968509573, 'FA': 0.04830917874396135, 'CF': 0.0731794596528896}, batch size: 64, grad_norm: 0.8991148471832275, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:06:26,821 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:07:32,508 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20927,  validation: loss=0.1255, DER=0.1812, ACC=0.9454, MI=0.05695, FA=0.04698, CF=0.07729, over 0.00 frames. 
2024-12-20 02:07:32,508 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 02:07:36,520 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20927,  validation: loss=0.1243, DER=0.18, ACC=0.9461, MI=0.05561, FA=0.04737, CF=0.07705, over 0.00 frames. 
2024-12-20 02:07:36,520 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 02:09:53,992 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-21000.pt
2024-12-20 02:09:57,709 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 02:10:50,889 (train_accelerate_ddp:709) INFO: end of epoch 10, batch_idx: 2102 batch_idx_train: 21029, {'loss': 0.20310041308403015, 'DER': 0.2867726779906035, 'ACC': 0.9155859375, 'MI': 0.134260932417781, 'FA': 0.04878930249367546, 'CF': 0.10372244307914709}, batch size: 64, grad_norm: 1.251914620399475, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:10:50,891 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-10.pt
2024-12-20 02:10:50,898 (train_accelerate_ddp:709) INFO: end of epoch 10, batch_idx: 2102 batch_idx_train: 21029, {'loss': 0.23387360572814941, 'DER': 0.3834898843930636, 'ACC': 0.8928125, 'MI': 0.1941835260115607, 'FA': 0.07713150289017341, 'CF': 0.11217485549132948}, batch size: 64, grad_norm: 1.251914620399475, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:10:53,598 (train_accelerate_ddp:557) INFO:  end of epoch 10, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-10.pt 
2024-12-20 02:11:05,453 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21030, num_updates: 20000, {'loss': 0.20465083420276642, 'DER': 0.3417085427135678, 'ACC': 0.90921875, 'MI': 0.13258600695786626, 'FA': 0.10166215693853885, 'CF': 0.10746037881716274}, batch size: 64, grad_norm: 1.1736806631088257, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:11:05,455 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21030, num_updates: 20000, {'loss': 0.1844022572040558, 'DER': 0.3014772934524895, 'ACC': 0.9121875, 'MI': 0.12292540579974466, 'FA': 0.0700346525624658, 'CF': 0.10851723509027904}, batch size: 64, grad_norm: 1.1736806631088257, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:26:30,014 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21530, num_updates: 20500, {'loss': 0.21130970120429993, 'DER': 0.35791469194312797, 'ACC': 0.906484375, 'MI': 0.1776303317535545, 'FA': 0.08436018957345971, 'CF': 0.09592417061611375}, batch size: 64, grad_norm: 1.0857797861099243, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:26:30,015 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:26:30,019 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21530, num_updates: 20500, {'loss': 0.19604720175266266, 'DER': 0.33546566321730953, 'ACC': 0.9081640625, 'MI': 0.13226716839134525, 'FA': 0.09633113828786453, 'CF': 0.10686735653809971}, batch size: 64, grad_norm: 1.0857797861099243, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:26:30,019 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:27:35,831 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 21530,  validation: loss=0.1256, DER=0.181, ACC=0.9452, MI=0.05601, FA=0.04674, CF=0.07824, over 0.00 frames. 
2024-12-20 02:27:35,832 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 02:27:41,589 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 21530,  validation: loss=0.1246, DER=0.18, ACC=0.9459, MI=0.05514, FA=0.04685, CF=0.07798, over 0.00 frames. 
2024-12-20 02:27:41,589 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 02:43:00,982 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22030, num_updates: 21000, {'loss': 0.1858314424753189, 'DER': 0.29116179615110477, 'ACC': 0.921171875, 'MI': 0.15342124019957235, 'FA': 0.06931575196008553, 'CF': 0.0684248039914469}, batch size: 64, grad_norm: 1.0772480964660645, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:43:00,983 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:43:00,985 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22030, num_updates: 21000, {'loss': 0.2230718731880188, 'DER': 0.3632270168855535, 'ACC': 0.8974609375, 'MI': 0.1395872420262664, 'FA': 0.09437148217636022, 'CF': 0.12926829268292683}, batch size: 64, grad_norm: 1.0772480964660645, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:43:00,985 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:44:06,517 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22030,  validation: loss=0.1282, DER=0.1827, ACC=0.9451, MI=0.05944, FA=0.04554, CF=0.07775, over 0.00 frames. 
2024-12-20 02:44:06,518 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 02:44:10,053 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22030,  validation: loss=0.1272, DER=0.1821, ACC=0.9454, MI=0.05878, FA=0.04577, CF=0.07755, over 0.00 frames. 
2024-12-20 02:44:10,053 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 02:58:41,051 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-22500.pt
2024-12-20 02:58:44,976 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 02:58:45,078 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 02:59:37,405 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22530, num_updates: 21500, {'loss': 0.19600972533226013, 'DER': 0.32656546489563565, 'ACC': 0.910546875, 'MI': 0.12466793168880455, 'FA': 0.09392789373814042, 'CF': 0.1079696394686907}, batch size: 64, grad_norm: 1.0608285665512085, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:59:37,406 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:59:37,414 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22530, num_updates: 21500, {'loss': 0.1710851490497589, 'DER': 0.2803339517625232, 'ACC': 0.9259375, 'MI': 0.12634508348794063, 'FA': 0.08256029684601113, 'CF': 0.07142857142857142}, batch size: 64, grad_norm: 1.0608285665512085, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:59:37,414 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:00:42,880 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22530,  validation: loss=0.1276, DER=0.1833, ACC=0.9448, MI=0.05611, FA=0.04924, CF=0.07791, over 0.00 frames. 
2024-12-20 03:00:42,880 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 03:00:47,753 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22530,  validation: loss=0.1262, DER=0.1819, ACC=0.9456, MI=0.05589, FA=0.04941, CF=0.07658, over 0.00 frames. 
2024-12-20 03:00:47,753 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 03:16:16,979 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 23030, num_updates: 22000, {'loss': 0.21127618849277496, 'DER': 0.3384418901660281, 'ACC': 0.9041015625, 'MI': 0.14942528735632185, 'FA': 0.07954752782338989, 'CF': 0.10946907498631636}, batch size: 64, grad_norm: 1.0669808387756348, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:16:16,979 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:16:16,984 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 23030, num_updates: 22000, {'loss': 0.21066701412200928, 'DER': 0.34333515184579017, 'ACC': 0.905625, 'MI': 0.1873067830514639, 'FA': 0.06001091107474086, 'CF': 0.09601745771958538}, batch size: 64, grad_norm: 1.0669808387756348, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:16:16,984 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:17:22,943 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 23030,  validation: loss=0.1255, DER=0.1809, ACC=0.9455, MI=0.05688, FA=0.04678, CF=0.0772, over 0.00 frames. 
2024-12-20 03:17:22,944 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 03:17:29,092 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 23030,  validation: loss=0.1244, DER=0.1803, ACC=0.9461, MI=0.05622, FA=0.0472, CF=0.07687, over 0.00 frames. 
2024-12-20 03:17:29,093 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 03:20:34,594 (train_accelerate_ddp:709) INFO: end of epoch 11, batch_idx: 2102 batch_idx_train: 23132, {'loss': 0.20731070637702942, 'DER': 0.3493975903614458, 'ACC': 0.9059765625, 'MI': 0.1681186283595922, 'FA': 0.0845227062094532, 'CF': 0.09675625579240037}, batch size: 64, grad_norm: 1.076887845993042, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:20:34,599 (train_accelerate_ddp:709) INFO: end of epoch 11, batch_idx: 2102 batch_idx_train: 23132, {'loss': 0.19734138250350952, 'DER': 0.34062140391254314, 'ACC': 0.9123046875, 'MI': 0.14307633294975067, 'FA': 0.10759493670886076, 'CF': 0.08995013425393172}, batch size: 64, grad_norm: 1.076887845993042, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:20:34,601 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-11.pt
2024-12-20 03:20:37,537 (train_accelerate_ddp:557) INFO:  end of epoch 11, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-11.pt 
2024-12-20 03:20:50,036 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23133, num_updates: 22000, {'loss': 0.13495561480522156, 'DER': 0.20205353868720205, 'ACC': 0.9437109375, 'MI': 0.09259259259259259, 'FA': 0.047304730473047306, 'CF': 0.06215621562156216}, batch size: 64, grad_norm: 0.9268971085548401, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:20:50,046 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23133, num_updates: 22000, {'loss': 0.22406762838363647, 'DER': 0.3665773011617516, 'ACC': 0.891015625, 'MI': 0.17140303842716711, 'FA': 0.06309204647006256, 'CF': 0.1320822162645219}, batch size: 64, grad_norm: 0.9268971085548401, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:36:10,666 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23633, num_updates: 22500, {'loss': 0.14806142449378967, 'DER': 0.24025018395879322, 'ACC': 0.93546875, 'MI': 0.1280353200883002, 'FA': 0.04856512141280353, 'CF': 0.06364974245768948}, batch size: 64, grad_norm: 0.7709112167358398, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:36:10,667 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:36:10,672 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23633, num_updates: 22500, {'loss': 0.16713926196098328, 'DER': 0.25652582159624415, 'ACC': 0.9301953125, 'MI': 0.08863849765258217, 'FA': 0.0888262910798122, 'CF': 0.07906103286384976}, batch size: 64, grad_norm: 0.7709112167358398, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:36:10,672 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:37:16,302 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 23633,  validation: loss=0.1281, DER=0.1839, ACC=0.9448, MI=0.06031, FA=0.04597, CF=0.07764, over 0.00 frames. 
2024-12-20 03:37:16,303 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 03:37:23,035 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 23633,  validation: loss=0.1271, DER=0.1831, ACC=0.9452, MI=0.05974, FA=0.04622, CF=0.07713, over 0.00 frames. 
2024-12-20 03:37:23,036 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 03:48:55,585 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-24000.pt
2024-12-20 03:48:59,536 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 03:48:59,633 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 03:53:04,250 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24133, num_updates: 23000, {'loss': 0.19697269797325134, 'DER': 0.36200185356811865, 'ACC': 0.8998828125, 'MI': 0.1670064874884152, 'FA': 0.0819277108433735, 'CF': 0.11306765523632993}, batch size: 64, grad_norm: 1.1856316328048706, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:53:04,251 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:53:04,256 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24133, num_updates: 23000, {'loss': 0.2034568190574646, 'DER': 0.34376199616122843, 'ACC': 0.9039453125, 'MI': 0.1291746641074856, 'FA': 0.08637236084452975, 'CF': 0.12821497120921305}, batch size: 64, grad_norm: 1.1856316328048706, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:53:04,256 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:54:10,539 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24133,  validation: loss=0.1286, DER=0.184, ACC=0.9447, MI=0.05574, FA=0.05001, CF=0.07826, over 0.00 frames. 
2024-12-20 03:54:10,540 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 03:54:15,641 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24133,  validation: loss=0.1277, DER=0.1833, ACC=0.945, MI=0.05533, FA=0.05023, CF=0.07778, over 0.00 frames. 
2024-12-20 03:54:15,641 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 04:09:45,571 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24633, num_updates: 23500, {'loss': 0.17356324195861816, 'DER': 0.2774407060121346, 'ACC': 0.9191796875, 'MI': 0.10424710424710425, 'FA': 0.07023349880492738, 'CF': 0.10296010296010295}, batch size: 64, grad_norm: 0.9377624988555908, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:09:45,572 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:09:45,575 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24633, num_updates: 23500, {'loss': 0.20152099430561066, 'DER': 0.3294332723948812, 'ACC': 0.9056640625, 'MI': 0.14771480804387568, 'FA': 0.06965265082266911, 'CF': 0.11206581352833637}, batch size: 64, grad_norm: 0.9377624988555908, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:09:45,575 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:10:50,949 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24633,  validation: loss=0.1284, DER=0.1843, ACC=0.9446, MI=0.05763, FA=0.04862, CF=0.07807, over 0.00 frames. 
2024-12-20 04:10:50,950 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 04:10:58,978 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24633,  validation: loss=0.1275, DER=0.1835, ACC=0.9451, MI=0.05716, FA=0.0489, CF=0.07741, over 0.00 frames. 
2024-12-20 04:10:58,979 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 04:25:59,409 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 25133, num_updates: 24000, {'loss': 0.19885356724262238, 'DER': 0.34276173639756435, 'ACC': 0.910234375, 'MI': 0.13789039481437831, 'FA': 0.09624828128069142, 'CF': 0.10862306030249459}, batch size: 64, grad_norm: 0.823029100894928, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:25:59,410 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:25:59,412 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 25133, num_updates: 24000, {'loss': 0.17094740271568298, 'DER': 0.28836121604605147, 'ACC': 0.9196484375, 'MI': 0.15164597949271452, 'FA': 0.05504587155963303, 'CF': 0.0816693649937039}, batch size: 64, grad_norm: 0.823029100894928, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:25:59,412 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:27:04,821 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 25133,  validation: loss=0.1255, DER=0.1813, ACC=0.9453, MI=0.05526, FA=0.04827, CF=0.0778, over 0.00 frames. 
2024-12-20 04:27:04,821 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 04:27:10,090 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 25133,  validation: loss=0.1242, DER=0.18, ACC=0.9461, MI=0.05413, FA=0.04873, CF=0.07718, over 0.00 frames. 
2024-12-20 04:27:10,090 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 04:30:14,619 (train_accelerate_ddp:709) INFO: end of epoch 12, batch_idx: 2102 batch_idx_train: 25235, {'loss': 0.21041806042194366, 'DER': 0.32100324792493684, 'ACC': 0.91015625, 'MI': 0.16618549260194876, 'FA': 0.06080837242872609, 'CF': 0.094009382894262}, batch size: 64, grad_norm: 1.1381356716156006, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:30:14,623 (train_accelerate_ddp:709) INFO: end of epoch 12, batch_idx: 2102 batch_idx_train: 25235, {'loss': 0.2035496085882187, 'DER': 0.30061122430079645, 'ACC': 0.9115234375, 'MI': 0.1213187627338396, 'FA': 0.060381552139285054, 'CF': 0.1189109094276718}, batch size: 64, grad_norm: 1.1381356716156006, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:30:14,625 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-12.pt
2024-12-20 04:30:17,486 (train_accelerate_ddp:557) INFO:  end of epoch 12, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-12.pt 
2024-12-20 04:30:20,353 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25236, num_updates: 24000, {'loss': 0.1721714735031128, 'DER': 0.2663912340578409, 'ACC': 0.93015625, 'MI': 0.16831327465421234, 'FA': 0.04329082090892761, 'CF': 0.05478713849470092}, batch size: 64, grad_norm: 0.903765082359314, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:30:20,355 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25236, num_updates: 24000, {'loss': 0.22355301678180695, 'DER': 0.359139389481427, 'ACC': 0.9025390625, 'MI': 0.1969474071349761, 'FA': 0.0625229863920559, 'CF': 0.099668995954395}, batch size: 64, grad_norm: 0.903765082359314, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:38:09,576 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-25500.pt
2024-12-20 04:38:13,188 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 04:38:13,299 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 04:45:05,244 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25736, num_updates: 24500, {'loss': 0.20493412017822266, 'DER': 0.3163356593308284, 'ACC': 0.9101171875, 'MI': 0.15727321524422974, 'FA': 0.06369654678833422, 'CF': 0.09536589729826445}, batch size: 64, grad_norm: 1.1558849811553955, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:45:05,244 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:45:05,246 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25736, num_updates: 24500, {'loss': 0.24748007953166962, 'DER': 0.3791099000908265, 'ACC': 0.8915190468725506, 'MI': 0.17547683923705723, 'FA': 0.07992733878292461, 'CF': 0.12370572207084468}, batch size: 64, grad_norm: 1.1558849811553955, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:45:05,246 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:46:10,978 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 25736,  validation: loss=0.1257, DER=0.1808, ACC=0.9452, MI=0.05549, FA=0.04666, CF=0.07861, over 0.00 frames. 
2024-12-20 04:46:10,979 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 04:46:18,715 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 25736,  validation: loss=0.1245, DER=0.1796, ACC=0.946, MI=0.05442, FA=0.04697, CF=0.07817, over 0.00 frames. 
2024-12-20 04:46:18,715 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:01:00,267 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26236, num_updates: 25000, {'loss': 0.1830652952194214, 'DER': 0.3097103297504099, 'ACC': 0.9215234375, 'MI': 0.17999635634906175, 'FA': 0.07341956640553834, 'CF': 0.0562944069958098}, batch size: 64, grad_norm: 0.8784043192863464, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:01:00,268 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:01:00,270 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26236, num_updates: 25000, {'loss': 0.15872499346733093, 'DER': 0.24158803631831938, 'ACC': 0.93421875, 'MI': 0.137795976499911, 'FA': 0.045575930211856866, 'CF': 0.05821612960655154}, batch size: 64, grad_norm: 0.8784043192863464, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:01:00,270 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:02:05,910 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26236,  validation: loss=0.1258, DER=0.1816, ACC=0.9454, MI=0.05881, FA=0.04616, CF=0.07668, over 0.00 frames. 
2024-12-20 05:02:05,911 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:02:09,692 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26236,  validation: loss=0.1245, DER=0.1804, ACC=0.9462, MI=0.05782, FA=0.04659, CF=0.07598, over 0.00 frames. 
2024-12-20 05:02:09,693 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:17:18,112 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26736, num_updates: 25500, {'loss': 0.19522903859615326, 'DER': 0.3210732054015636, 'ACC': 0.909296875, 'MI': 0.16879886282871356, 'FA': 0.060767590618336885, 'CF': 0.09150675195451315}, batch size: 64, grad_norm: 1.155491590499878, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:17:18,113 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:17:18,119 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26736, num_updates: 25500, {'loss': 0.22408317029476166, 'DER': 0.3449814126394052, 'ACC': 0.898984375, 'MI': 0.1574349442379182, 'FA': 0.051858736059479556, 'CF': 0.13568773234200743}, batch size: 64, grad_norm: 1.155491590499878, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:17:18,119 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:18:23,608 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26736,  validation: loss=0.1273, DER=0.1824, ACC=0.9453, MI=0.05866, FA=0.04695, CF=0.07684, over 0.00 frames. 
2024-12-20 05:18:23,609 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:18:31,305 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26736,  validation: loss=0.1261, DER=0.1816, ACC=0.9458, MI=0.05818, FA=0.04727, CF=0.07613, over 0.00 frames. 
2024-12-20 05:18:31,305 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:26:07,400 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-27000.pt
2024-12-20 05:26:11,086 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 05:33:11,789 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 27236, num_updates: 26000, {'loss': 0.21326637268066406, 'DER': 0.3263276412084819, 'ACC': 0.9097265625, 'MI': 0.14430474760743103, 'FA': 0.07468568211671983, 'CF': 0.10733721148433102}, batch size: 64, grad_norm: 1.0917218923568726, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:33:11,790 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:33:11,795 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 27236, num_updates: 26000, {'loss': 0.22867316007614136, 'DER': 0.3772685609532539, 'ACC': 0.8952734375, 'MI': 0.184967919340055, 'FA': 0.07809349220898258, 'CF': 0.11420714940421632}, batch size: 64, grad_norm: 1.0917218923568726, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:33:11,796 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:34:17,035 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 27236,  validation: loss=0.1285, DER=0.1823, ACC=0.945, MI=0.05845, FA=0.04537, CF=0.07846, over 0.00 frames. 
2024-12-20 05:34:17,036 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:34:23,107 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 27236,  validation: loss=0.1276, DER=0.1822, ACC=0.9453, MI=0.05789, FA=0.04608, CF=0.07818, over 0.00 frames. 
2024-12-20 05:34:23,108 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:37:26,464 (train_accelerate_ddp:709) INFO: end of epoch 13, batch_idx: 2102 batch_idx_train: 27338, {'loss': 0.20250727236270905, 'DER': 0.33618923449431926, 'ACC': 0.9070703125, 'MI': 0.1545911715403241, 'FA': 0.07468802384056622, 'CF': 0.10691003911342895}, batch size: 64, grad_norm: 1.3081508874893188, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:37:26,467 (train_accelerate_ddp:709) INFO: end of epoch 13, batch_idx: 2102 batch_idx_train: 27338, {'loss': 0.21542158722877502, 'DER': 0.34559099437148216, 'ACC': 0.903984375, 'MI': 0.14427767354596624, 'FA': 0.08574108818011257, 'CF': 0.11557223264540338}, batch size: 64, grad_norm: 1.3081508874893188, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:37:26,469 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-13.pt
2024-12-20 05:37:29,277 (train_accelerate_ddp:557) INFO:  end of epoch 13, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-13.pt 
2024-12-20 05:37:42,188 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27339, num_updates: 26000, {'loss': 0.24241262674331665, 'DER': 0.386615748317876, 'ACC': 0.8935546875, 'MI': 0.2005819239861793, 'FA': 0.07710492816875796, 'CF': 0.10892889616293872}, batch size: 64, grad_norm: 1.1951080560684204, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:37:42,193 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27339, num_updates: 26000, {'loss': 0.21272000670433044, 'DER': 0.315666125833784, 'ACC': 0.90703125, 'MI': 0.1499909861186227, 'FA': 0.05228051198846223, 'CF': 0.11339462772669912}, batch size: 64, grad_norm: 1.1951080560684204, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:52:34,358 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27839, num_updates: 26500, {'loss': 0.17945988476276398, 'DER': 0.2856130403968816, 'ACC': 0.92234375, 'MI': 0.1645995747696669, 'FA': 0.054394046775336644, 'CF': 0.0666194188518781}, batch size: 64, grad_norm: 0.8549338579177856, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:52:34,359 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:52:34,359 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27839, num_updates: 26500, {'loss': 0.17638298869132996, 'DER': 0.245218945487042, 'ACC': 0.9298046875, 'MI': 0.11081322609472744, 'FA': 0.05844504021447721, 'CF': 0.07596067917783736}, batch size: 64, grad_norm: 0.8549338579177856, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:52:34,360 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:53:39,871 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 27839,  validation: loss=0.1278, DER=0.1828, ACC=0.9448, MI=0.05782, FA=0.04655, CF=0.07845, over 0.00 frames. 
2024-12-20 05:53:39,872 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 05:53:46,969 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 27839,  validation: loss=0.1269, DER=0.1827, ACC=0.9452, MI=0.05756, FA=0.04709, CF=0.07809, over 0.00 frames. 
2024-12-20 05:53:46,970 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 06:08:30,308 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28339, num_updates: 27000, {'loss': 0.1624622493982315, 'DER': 0.25249410484309814, 'ACC': 0.9303125, 'MI': 0.12679122075095228, 'FA': 0.05459822238345728, 'CF': 0.07110466170868855}, batch size: 64, grad_norm: 0.9585641622543335, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:08:30,308 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:08:30,309 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28339, num_updates: 27000, {'loss': 0.20428144931793213, 'DER': 0.32597904026475455, 'ACC': 0.9141015625, 'MI': 0.18679904394190108, 'FA': 0.060856775142489425, 'CF': 0.07832322118036404}, batch size: 64, grad_norm: 0.9585641622543335, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:08:30,309 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:09:37,196 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28339,  validation: loss=0.1272, DER=0.1818, ACC=0.9451, MI=0.0588, FA=0.04494, CF=0.07809, over 0.00 frames. 
2024-12-20 06:09:37,197 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 06:09:42,927 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28339,  validation: loss=0.1261, DER=0.1814, ACC=0.9456, MI=0.05814, FA=0.04554, CF=0.0777, over 0.00 frames. 
2024-12-20 06:09:42,927 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 06:14:31,688 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-28500.pt
2024-12-20 06:14:35,231 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 06:14:35,327 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 06:24:32,084 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28839, num_updates: 27500, {'loss': 0.20460672676563263, 'DER': 0.33297081747326446, 'ACC': 0.9061328125, 'MI': 0.1839767989849556, 'FA': 0.04640203008881639, 'CF': 0.10259198839949248}, batch size: 64, grad_norm: 1.1412310600280762, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:24:32,085 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:24:32,088 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28839, num_updates: 27500, {'loss': 0.19141452014446259, 'DER': 0.28909026297086, 'ACC': 0.916171875, 'MI': 0.1494314143567875, 'FA': 0.04744136460554371, 'CF': 0.09221748400852879}, batch size: 64, grad_norm: 1.1412310600280762, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:24:32,089 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:25:37,561 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28839,  validation: loss=0.1275, DER=0.1829, ACC=0.945, MI=0.06107, FA=0.04444, CF=0.07743, over 0.00 frames. 
2024-12-20 06:25:37,562 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 06:25:45,318 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28839,  validation: loss=0.1265, DER=0.1827, ACC=0.9454, MI=0.06054, FA=0.04505, CF=0.0771, over 0.00 frames. 
2024-12-20 06:25:45,319 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 06:40:53,998 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 29339, num_updates: 28000, {'loss': 0.18785317242145538, 'DER': 0.2867726779906035, 'ACC': 0.9178515625, 'MI': 0.1288398988073726, 'FA': 0.06469100108420672, 'CF': 0.09324177809902422}, batch size: 64, grad_norm: 0.9178053140640259, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:40:53,999 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:40:53,999 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 29339, num_updates: 28000, {'loss': 0.17156541347503662, 'DER': 0.2845342706502636, 'ACC': 0.92265625, 'MI': 0.17293497363796134, 'FA': 0.04815465729349736, 'CF': 0.06344463971880492}, batch size: 64, grad_norm: 0.9178053140640259, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:40:53,999 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:41:59,607 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 29339,  validation: loss=0.1256, DER=0.1823, ACC=0.9453, MI=0.06042, FA=0.04515, CF=0.0767, over 0.00 frames. 
2024-12-20 06:41:59,608 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 06:42:07,044 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 29339,  validation: loss=0.1244, DER=0.1816, ACC=0.9458, MI=0.05932, FA=0.04586, CF=0.07646, over 0.00 frames. 
2024-12-20 06:42:07,044 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 06:45:01,324 (train_accelerate_ddp:709) INFO: end of epoch 14, batch_idx: 2102 batch_idx_train: 29441, {'loss': 0.19945071637630463, 'DER': 0.3075675675675676, 'ACC': 0.9140234375, 'MI': 0.16216216216216217, 'FA': 0.056396396396396396, 'CF': 0.08900900900900902}, batch size: 64, grad_norm: 0.9980738162994385, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:45:01,327 (train_accelerate_ddp:709) INFO: end of epoch 14, batch_idx: 2102 batch_idx_train: 29441, {'loss': 0.16874133050441742, 'DER': 0.2639602907285942, 'ACC': 0.9269921875, 'MI': 0.14979613543697926, 'FA': 0.04680021272823967, 'CF': 0.06736394256337529}, batch size: 64, grad_norm: 0.9980738162994385, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:45:01,329 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-14.pt
2024-12-20 06:45:04,122 (train_accelerate_ddp:557) INFO:  end of epoch 14, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-14.pt 
2024-12-20 06:45:07,570 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29442, num_updates: 28000, {'loss': 0.17879638075828552, 'DER': 0.31418346204558295, 'ACC': 0.9174609375, 'MI': 0.14918063665473724, 'FA': 0.08118289696741382, 'CF': 0.0838199284234319}, batch size: 64, grad_norm: 1.0478360652923584, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:45:07,571 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29442, num_updates: 28000, {'loss': 0.1936943382024765, 'DER': 0.33339366515837104, 'ACC': 0.907890625, 'MI': 0.14751131221719457, 'FA': 0.09248868778280543, 'CF': 0.09339366515837104}, batch size: 64, grad_norm: 1.0478360652923584, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:00:00,993 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29942, num_updates: 28500, {'loss': 0.16927601397037506, 'DER': 0.23904160475482913, 'ACC': 0.933203125, 'MI': 0.09843982169390787, 'FA': 0.062035661218424965, 'CF': 0.07856612184249628}, batch size: 64, grad_norm: 0.9737115502357483, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:00:00,994 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:00:01,003 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29942, num_updates: 28500, {'loss': 0.1867402046918869, 'DER': 0.2798898071625344, 'ACC': 0.9203125, 'MI': 0.12341597796143251, 'FA': 0.06170798898071626, 'CF': 0.09476584022038567}, batch size: 64, grad_norm: 0.9737115502357483, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:00:01,003 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:01:06,173 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 29942,  validation: loss=0.1268, DER=0.1822, ACC=0.9452, MI=0.05795, FA=0.04715, CF=0.0771, over 0.00 frames. 
2024-12-20 07:01:06,174 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:01:14,244 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 29942,  validation: loss=0.1257, DER=0.1817, ACC=0.9457, MI=0.0571, FA=0.04784, CF=0.07672, over 0.00 frames. 
2024-12-20 07:01:14,244 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:02:55,088 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-30000.pt
2024-12-20 07:02:58,658 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 07:02:58,741 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 07:16:16,490 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30442, num_updates: 29000, {'loss': 0.19146926701068878, 'DER': 0.293222438506459, 'ACC': 0.912109375, 'MI': 0.1433374623960361, 'FA': 0.044947796850115025, 'CF': 0.1049371792603079}, batch size: 64, grad_norm: 1.1181328296661377, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:16:16,491 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:16:16,495 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30442, num_updates: 29000, {'loss': 0.16069306433200836, 'DER': 0.2643236317041918, 'ACC': 0.9316015625, 'MI': 0.1566904631155043, 'FA': 0.051436939410580265, 'CF': 0.056196229178107265}, batch size: 64, grad_norm: 1.1181328296661377, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:16:16,496 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:17:21,998 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30442,  validation: loss=0.1263, DER=0.1818, ACC=0.945, MI=0.05675, FA=0.04653, CF=0.07854, over 0.00 frames. 
2024-12-20 07:17:21,998 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:17:29,513 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30442,  validation: loss=0.1254, DER=0.1814, ACC=0.9455, MI=0.05537, FA=0.04713, CF=0.07888, over 0.00 frames. 
2024-12-20 07:17:29,514 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:32:10,348 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30942, num_updates: 29500, {'loss': 0.16499346494674683, 'DER': 0.28395292359424623, 'ACC': 0.92421875, 'MI': 0.13674575004670278, 'FA': 0.06874649729123856, 'CF': 0.07846067625630487}, batch size: 64, grad_norm: 0.8623085618019104, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:32:10,349 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:32:10,356 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30942, num_updates: 29500, {'loss': 0.16161951422691345, 'DER': 0.26559971228196366, 'ACC': 0.928203125, 'MI': 0.14134148534436253, 'FA': 0.05934184499190793, 'CF': 0.06491638194569323}, batch size: 64, grad_norm: 0.8623085618019104, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:32:10,356 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:33:15,766 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30942,  validation: loss=0.1289, DER=0.1839, ACC=0.9447, MI=0.06141, FA=0.04433, CF=0.07819, over 0.00 frames. 
2024-12-20 07:33:15,767 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:33:23,227 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30942,  validation: loss=0.1281, DER=0.1834, ACC=0.945, MI=0.0607, FA=0.04466, CF=0.07799, over 0.00 frames. 
2024-12-20 07:33:23,227 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:48:17,584 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 31442, num_updates: 30000, {'loss': 0.1786976158618927, 'DER': 0.30140098447557745, 'ACC': 0.919609375, 'MI': 0.12608860280196896, 'FA': 0.08708822415751609, 'CF': 0.08822415751609239}, batch size: 64, grad_norm: 1.046881914138794, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:48:17,585 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:48:17,587 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 31442, num_updates: 30000, {'loss': 0.1771809607744217, 'DER': 0.2782128871484514, 'ACC': 0.924921875, 'MI': 0.15521537913848343, 'FA': 0.05909576361694553, 'CF': 0.06390174439302243}, batch size: 64, grad_norm: 1.046881914138794, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:48:17,587 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:49:23,166 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 31442,  validation: loss=0.1307, DER=0.1871, ACC=0.9437, MI=0.0582, FA=0.0491, CF=0.07982, over 0.00 frames. 
2024-12-20 07:49:23,167 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:49:30,963 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 31442,  validation: loss=0.1297, DER=0.186, ACC=0.9441, MI=0.058, FA=0.04866, CF=0.07931, over 0.00 frames. 
2024-12-20 07:49:30,964 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 07:51:15,160 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-31500.pt
2024-12-20 07:51:18,799 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 07:52:37,204 (train_accelerate_ddp:709) INFO: end of epoch 15, batch_idx: 2102 batch_idx_train: 31544, {'loss': 0.2061857432126999, 'DER': 0.3296058661778185, 'ACC': 0.91109375, 'MI': 0.1904674610449129, 'FA': 0.05151237396883593, 'CF': 0.08762603116406967}, batch size: 64, grad_norm: 0.9465385675430298, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:52:37,205 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-15.pt
2024-12-20 07:52:37,207 (train_accelerate_ddp:709) INFO: end of epoch 15, batch_idx: 2102 batch_idx_train: 31544, {'loss': 0.16877728700637817, 'DER': 0.29343200740055503, 'ACC': 0.922421875, 'MI': 0.1322849213691027, 'FA': 0.08714153561517114, 'CF': 0.07400555041628122}, batch size: 64, grad_norm: 0.9465385675430298, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:52:39,928 (train_accelerate_ddp:557) INFO:  end of epoch 15, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-15.pt 
2024-12-20 07:52:42,780 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 31545, num_updates: 30000, {'loss': 0.2090780884027481, 'DER': 0.33808740171877855, 'ACC': 0.906640625, 'MI': 0.1545072225269702, 'FA': 0.08465898701773633, 'CF': 0.09892119217407204}, batch size: 64, grad_norm: 0.9821906685829163, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:52:42,785 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 31545, num_updates: 30000, {'loss': 0.17029045522212982, 'DER': 0.2689867681711075, 'ACC': 0.923828125, 'MI': 0.12796809860431393, 'FA': 0.05655247417074497, 'CF': 0.08446619539604858}, batch size: 64, grad_norm: 0.9821906685829163, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:07:30,075 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32045, num_updates: 30500, {'loss': 0.23252689838409424, 'DER': 0.3577428780131483, 'ACC': 0.897257053291536, 'MI': 0.1700146092037984, 'FA': 0.066654492330168, 'CF': 0.12107377647918188}, batch size: 64, grad_norm: 1.269361972808838, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:07:30,076 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:07:30,079 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32045, num_updates: 30500, {'loss': 0.22158263623714447, 'DER': 0.31784884759692694, 'ACC': 0.9065625, 'MI': 0.14632839020904057, 'FA': 0.061997498659996424, 'CF': 0.10952295872788995}, batch size: 64, grad_norm: 1.269361972808838, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:07:30,079 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:08:35,324 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32045,  validation: loss=0.1255, DER=0.1817, ACC=0.9454, MI=0.05636, FA=0.04883, CF=0.07653, over 0.00 frames. 
2024-12-20 08:08:35,325 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:08:39,999 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32045,  validation: loss=0.1242, DER=0.1803, ACC=0.9462, MI=0.05546, FA=0.0493, CF=0.07554, over 0.00 frames. 
2024-12-20 08:08:40,000 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:23:24,222 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32545, num_updates: 31000, {'loss': 0.21203827857971191, 'DER': 0.3602372820420636, 'ACC': 0.89609375, 'MI': 0.17059140751393134, 'FA': 0.07172388998741686, 'CF': 0.11792198454071544}, batch size: 64, grad_norm: 0.8636441230773926, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:23:24,223 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:23:24,223 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32545, num_updates: 31000, {'loss': 0.14542461931705475, 'DER': 0.23266090297790587, 'ACC': 0.9401171875, 'MI': 0.09317963496637849, 'FA': 0.0776176753121998, 'CF': 0.06186359269932757}, batch size: 64, grad_norm: 0.8636441230773926, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:23:24,224 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:24:29,563 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32545,  validation: loss=0.127, DER=0.1826, ACC=0.9452, MI=0.0582, FA=0.04725, CF=0.07715, over 0.00 frames. 
2024-12-20 08:24:29,564 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:24:37,579 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32545,  validation: loss=0.1258, DER=0.182, ACC=0.9456, MI=0.05746, FA=0.04756, CF=0.07697, over 0.00 frames. 
2024-12-20 08:24:37,579 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:38:00,220 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-33000.pt
2024-12-20 08:38:03,847 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 08:38:03,938 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 08:39:29,895 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33045, num_updates: 31500, {'loss': 0.17036853730678558, 'DER': 0.2755026061057334, 'ACC': 0.9260546875, 'MI': 0.11783320923306032, 'FA': 0.08078927773641102, 'CF': 0.0768801191362621}, batch size: 64, grad_norm: 1.1056175231933594, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:39:29,896 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:39:29,897 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33045, num_updates: 31500, {'loss': 0.23442597687244415, 'DER': 0.384360625574977, 'ACC': 0.88671875, 'MI': 0.16780128794848206, 'FA': 0.06734130634774609, 'CF': 0.14921803127874886}, batch size: 64, grad_norm: 1.1056175231933594, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:39:29,897 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:40:34,412 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33045,  validation: loss=0.1276, DER=0.1827, ACC=0.945, MI=0.05694, FA=0.04812, CF=0.07767, over 0.00 frames. 
2024-12-20 08:40:34,412 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:40:39,539 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33045,  validation: loss=0.1267, DER=0.1825, ACC=0.9453, MI=0.05628, FA=0.04866, CF=0.07753, over 0.00 frames. 
2024-12-20 08:40:39,540 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:55:26,519 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33545, num_updates: 32000, {'loss': 0.18402224779129028, 'DER': 0.3313931656027037, 'ACC': 0.91671875, 'MI': 0.17067217423957942, 'FA': 0.09181374389785955, 'CF': 0.06890724746526473}, batch size: 64, grad_norm: 0.982280433177948, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:55:26,520 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33545, num_updates: 32000, {'loss': 0.17099899053573608, 'DER': 0.2515689438766362, 'ACC': 0.9272265625, 'MI': 0.10256410256410256, 'FA': 0.06652322036937422, 'CF': 0.0824816209431594}, batch size: 64, grad_norm: 0.982280433177948, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:55:26,520 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:55:26,520 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:56:31,771 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33545,  validation: loss=0.1275, DER=0.1825, ACC=0.9451, MI=0.05767, FA=0.04739, CF=0.07749, over 0.00 frames. 
2024-12-20 08:56:31,771 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:56:37,887 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33545,  validation: loss=0.1265, DER=0.1819, ACC=0.9456, MI=0.05707, FA=0.04797, CF=0.07688, over 0.00 frames. 
2024-12-20 08:56:37,888 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 08:59:32,073 (train_accelerate_ddp:709) INFO: end of epoch 16, batch_idx: 2102 batch_idx_train: 33647, {'loss': 0.21831358969211578, 'DER': 0.34162610420046874, 'ACC': 0.8953515625, 'MI': 0.12871822606814495, 'FA': 0.07157021813592933, 'CF': 0.14133765999639444}, batch size: 64, grad_norm: 1.1825644969940186, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:59:32,075 (train_accelerate_ddp:709) INFO: end of epoch 16, batch_idx: 2102 batch_idx_train: 33647, {'loss': 0.16726595163345337, 'DER': 0.27260934025203853, 'ACC': 0.92546875, 'MI': 0.12194217939214233, 'FA': 0.06968124536693847, 'CF': 0.08098591549295775}, batch size: 64, grad_norm: 1.1825644969940186, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:59:32,077 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-16.pt
2024-12-20 08:59:34,857 (train_accelerate_ddp:557) INFO:  end of epoch 16, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-16.pt 
2024-12-20 08:59:37,664 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 33648, num_updates: 32000, {'loss': 0.18725793063640594, 'DER': 0.31373989713445993, 'ACC': 0.9161328125, 'MI': 0.1541146216017634, 'FA': 0.07898603967670831, 'CF': 0.08063923585598824}, batch size: 64, grad_norm: 1.0714335441589355, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:59:37,666 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 33648, num_updates: 32000, {'loss': 0.19203461706638336, 'DER': 0.3433252653137218, 'ACC': 0.9077734375, 'MI': 0.16216719419102588, 'FA': 0.08490039098864272, 'CF': 0.09625768013405325}, batch size: 64, grad_norm: 1.0714335441589355, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:14:22,070 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34148, num_updates: 32500, {'loss': 0.18476973474025726, 'DER': 0.3125229526257804, 'ACC': 0.9169921875, 'MI': 0.16819684171869262, 'FA': 0.06665442526625046, 'CF': 0.07767168564083732}, batch size: 64, grad_norm: 1.1101288795471191, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:14:22,071 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:14:22,073 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34148, num_updates: 32500, {'loss': 0.2083534598350525, 'DER': 0.35682242990654206, 'ACC': 0.8992578125, 'MI': 0.14242990654205606, 'FA': 0.0891588785046729, 'CF': 0.1252336448598131}, batch size: 64, grad_norm: 1.1101288795471191, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:14:22,074 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:15:27,561 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34148,  validation: loss=0.1286, DER=0.1835, ACC=0.9447, MI=0.05943, FA=0.04565, CF=0.07846, over 0.00 frames. 
2024-12-20 09:15:27,562 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 09:15:33,187 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34148,  validation: loss=0.1277, DER=0.1838, ACC=0.9449, MI=0.05915, FA=0.04618, CF=0.07845, over 0.00 frames. 
2024-12-20 09:15:33,187 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 09:25:57,215 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-34500.pt
2024-12-20 09:26:01,126 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 09:26:01,208 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 09:30:27,721 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34648, num_updates: 33000, {'loss': 0.18616580963134766, 'DER': 0.2769482884195193, 'ACC': 0.918203125, 'MI': 0.11671522214129643, 'FA': 0.055899490167516386, 'CF': 0.10433357611070648}, batch size: 64, grad_norm: 0.9027732014656067, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:30:27,721 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34648, num_updates: 33000, {'loss': 0.16779571771621704, 'DER': 0.2852410747147589, 'ACC': 0.926015625, 'MI': 0.1658078763341921, 'FA': 0.056128082443871914, 'CF': 0.06330511593669488}, batch size: 64, grad_norm: 0.9027732014656067, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:30:27,721 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:30:27,721 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:31:32,820 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34648,  validation: loss=0.1288, DER=0.1848, ACC=0.9444, MI=0.05591, FA=0.05012, CF=0.07876, over 0.00 frames. 
2024-12-20 09:31:32,821 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 09:31:37,087 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34648,  validation: loss=0.1278, DER=0.1837, ACC=0.9448, MI=0.05577, FA=0.04986, CF=0.0781, over 0.00 frames. 
2024-12-20 09:31:37,087 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 09:46:22,224 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35148, num_updates: 33500, {'loss': 0.1819927841424942, 'DER': 0.294921514312096, 'ACC': 0.9227734375, 'MI': 0.15937211449676825, 'FA': 0.06537396121883657, 'CF': 0.07017543859649122}, batch size: 64, grad_norm: 0.8654624223709106, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:46:22,224 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:46:22,225 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35148, num_updates: 33500, {'loss': 0.15002524852752686, 'DER': 0.23782234957020057, 'ACC': 0.932890625, 'MI': 0.12714899713467048, 'FA': 0.04083094555873926, 'CF': 0.06984240687679083}, batch size: 64, grad_norm: 0.8654624223709106, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:46:22,226 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:47:27,496 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35148,  validation: loss=0.1255, DER=0.1814, ACC=0.9454, MI=0.05988, FA=0.04443, CF=0.07704, over 0.00 frames. 
2024-12-20 09:47:27,496 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 09:47:35,087 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35148,  validation: loss=0.1244, DER=0.1805, ACC=0.946, MI=0.05841, FA=0.04505, CF=0.07706, over 0.00 frames. 
2024-12-20 09:47:35,088 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:02:16,744 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35648, num_updates: 34000, {'loss': 0.19320303201675415, 'DER': 0.3151283451665756, 'ACC': 0.91609375, 'MI': 0.1827780811942472, 'FA': 0.05643546331694885, 'CF': 0.07591480065537957}, batch size: 64, grad_norm: 1.0987766981124878, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:02:16,744 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35648, num_updates: 34000, {'loss': 0.18451471626758575, 'DER': 0.3158582089552239, 'ACC': 0.9166796875, 'MI': 0.14253731343283582, 'FA': 0.0912313432835821, 'CF': 0.08208955223880597}, batch size: 64, grad_norm: 1.0987766981124878, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:02:16,745 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:02:16,745 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:03:22,634 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35648,  validation: loss=0.1269, DER=0.1822, ACC=0.9452, MI=0.05711, FA=0.04782, CF=0.07724, over 0.00 frames. 
2024-12-20 10:03:22,634 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:03:28,312 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35648,  validation: loss=0.1259, DER=0.1818, ACC=0.9457, MI=0.05638, FA=0.04854, CF=0.07683, over 0.00 frames. 
2024-12-20 10:03:28,312 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:06:25,713 (train_accelerate_ddp:709) INFO: end of epoch 17, batch_idx: 2102 batch_idx_train: 35750, {'loss': 0.19038255512714386, 'DER': 0.2946725860155383, 'ACC': 0.911796875, 'MI': 0.10210876803551609, 'FA': 0.06955234924158342, 'CF': 0.12301146873843877}, batch size: 64, grad_norm: 0.9420754313468933, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:06:25,715 (train_accelerate_ddp:709) INFO: end of epoch 17, batch_idx: 2102 batch_idx_train: 35750, {'loss': 0.1936236470937729, 'DER': 0.3029525032092426, 'ACC': 0.91671875, 'MI': 0.14909224280212727, 'FA': 0.06583532000733541, 'CF': 0.08802494039977994}, batch size: 64, grad_norm: 0.9420754313468933, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:06:25,717 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-17.pt
2024-12-20 10:06:29,041 (train_accelerate_ddp:557) INFO:  end of epoch 17, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-17.pt 
2024-12-20 10:06:32,340 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 35751, num_updates: 34000, {'loss': 0.17166616022586823, 'DER': 0.3031437125748503, 'ACC': 0.9203125, 'MI': 0.15363023952095808, 'FA': 0.07092065868263472, 'CF': 0.07859281437125748}, batch size: 64, grad_norm: 1.099510908126831, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:06:32,343 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 35751, num_updates: 34000, {'loss': 0.1873050183057785, 'DER': 0.32017464071311624, 'ACC': 0.9144921875, 'MI': 0.1719119519738039, 'FA': 0.07022012006549026, 'CF': 0.07804256867382209}, batch size: 64, grad_norm: 1.099510908126831, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:14:02,033 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-36000.pt
2024-12-20 10:14:05,758 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 10:14:05,827 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 10:21:38,288 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36251, num_updates: 34500, {'loss': 0.185768723487854, 'DER': 0.30873493975903615, 'ACC': 0.916953125, 'MI': 0.15512048192771086, 'FA': 0.06212349397590362, 'CF': 0.09149096385542169}, batch size: 64, grad_norm: 0.8553134202957153, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:21:38,288 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36251, num_updates: 34500, {'loss': 0.15856142342090607, 'DER': 0.25337775310012955, 'ACC': 0.9339615264310291, 'MI': 0.14825097168239867, 'FA': 0.0459004256894318, 'CF': 0.059226355728299096}, batch size: 64, grad_norm: 0.8553134202957153, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:21:38,288 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:21:38,288 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:22:43,640 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36251,  validation: loss=0.1249, DER=0.181, ACC=0.9452, MI=0.05633, FA=0.04621, CF=0.07846, over 0.00 frames. 
2024-12-20 10:22:43,640 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:22:48,021 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36251,  validation: loss=0.1237, DER=0.1794, ACC=0.9461, MI=0.05506, FA=0.04639, CF=0.07791, over 0.00 frames. 
2024-12-20 10:22:48,021 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:37:39,345 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36751, num_updates: 35000, {'loss': 0.1600242704153061, 'DER': 0.28187919463087246, 'ACC': 0.9328515625, 'MI': 0.14170661553211888, 'FA': 0.09242569511025887, 'CF': 0.04774688398849473}, batch size: 64, grad_norm: 0.8281562328338623, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:37:39,345 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:37:39,352 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36751, num_updates: 35000, {'loss': 0.21948367357254028, 'DER': 0.36812464801952316, 'ACC': 0.9026171875, 'MI': 0.18884925849446219, 'FA': 0.07940679556973906, 'CF': 0.09986859395532194}, batch size: 64, grad_norm: 0.8281562328338623, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:37:39,352 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:38:47,702 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36751,  validation: loss=0.1261, DER=0.1823, ACC=0.9451, MI=0.056, FA=0.04858, CF=0.07769, over 0.00 frames. 
2024-12-20 10:38:47,702 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:38:50,605 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36751,  validation: loss=0.125, DER=0.1813, ACC=0.9457, MI=0.05529, FA=0.04905, CF=0.07696, over 0.00 frames. 
2024-12-20 10:38:50,605 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:54:08,252 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37251, num_updates: 35500, {'loss': 0.25314921140670776, 'DER': 0.38009700017962994, 'ACC': 0.8872265625, 'MI': 0.1632836357104365, 'FA': 0.07831866355308065, 'CF': 0.1384947009161128}, batch size: 64, grad_norm: 1.3336524963378906, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:54:08,253 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:54:08,260 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37251, num_updates: 35500, {'loss': 0.1944177895784378, 'DER': 0.31525360977415773, 'ACC': 0.9101953125, 'MI': 0.1419844502036283, 'FA': 0.06293965198074787, 'CF': 0.11032950758978156}, batch size: 64, grad_norm: 1.3336524963378906, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:54:08,260 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:55:16,389 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37251,  validation: loss=0.1264, DER=0.1812, ACC=0.9454, MI=0.05731, FA=0.04645, CF=0.07744, over 0.00 frames. 
2024-12-20 10:55:16,390 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 10:55:18,069 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37251,  validation: loss=0.1252, DER=0.1805, ACC=0.946, MI=0.05653, FA=0.04711, CF=0.07689, over 0.00 frames. 
2024-12-20 10:55:18,069 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 11:02:56,481 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-37500.pt
2024-12-20 11:02:59,807 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 11:10:53,540 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37751, num_updates: 36000, {'loss': 0.16235925257205963, 'DER': 0.25058379737740255, 'ACC': 0.93484375, 'MI': 0.1386743308783905, 'FA': 0.06287048679719777, 'CF': 0.04903897970181426}, batch size: 64, grad_norm: 1.0114390850067139, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:10:53,540 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:10:53,547 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37751, num_updates: 36000, {'loss': 0.2170586884021759, 'DER': 0.3229703322082075, 'ACC': 0.907578125, 'MI': 0.16592645230058625, 'FA': 0.05969088648072482, 'CF': 0.09735299342689643}, batch size: 64, grad_norm: 1.0114390850067139, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:10:53,547 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:12:01,782 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37751,  validation: loss=0.1282, DER=0.1839, ACC=0.9447, MI=0.05764, FA=0.04825, CF=0.07802, over 0.00 frames. 
2024-12-20 11:12:01,783 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 11:12:04,614 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37751,  validation: loss=0.1269, DER=0.1829, ACC=0.9453, MI=0.05708, FA=0.04856, CF=0.07731, over 0.00 frames. 
2024-12-20 11:12:04,614 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 11:15:08,453 (train_accelerate_ddp:709) INFO: end of epoch 18, batch_idx: 2102 batch_idx_train: 37853, {'loss': 0.1838987171649933, 'DER': 0.2862932940309506, 'ACC': 0.9205078125, 'MI': 0.1296978629329403, 'FA': 0.06798084008843036, 'CF': 0.08861459100957995}, batch size: 64, grad_norm: 0.8866541981697083, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:15:08,456 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-18.pt
2024-12-20 11:15:08,457 (train_accelerate_ddp:709) INFO: end of epoch 18, batch_idx: 2102 batch_idx_train: 37853, {'loss': 0.18176187574863434, 'DER': 0.3062636562272396, 'ACC': 0.9130078125, 'MI': 0.13820101966496723, 'FA': 0.06882738528769118, 'CF': 0.09923525127458122}, batch size: 64, grad_norm: 0.8866541981697083, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:15:11,320 (train_accelerate_ddp:557) INFO:  end of epoch 18, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-18.pt 
2024-12-20 11:15:23,883 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 37854, num_updates: 36000, {'loss': 0.18294095993041992, 'DER': 0.30418322922581864, 'ACC': 0.922109375, 'MI': 0.12436115843270869, 'FA': 0.10656823774370623, 'CF': 0.07325383304940375}, batch size: 64, grad_norm: 0.8205016255378723, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:15:23,887 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 37854, num_updates: 36000, {'loss': 0.2038274109363556, 'DER': 0.3355457227138643, 'ACC': 0.9088671875, 'MI': 0.1862094395280236, 'FA': 0.054756637168141595, 'CF': 0.09457964601769911}, batch size: 64, grad_norm: 0.8205016255378723, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:30:42,079 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38354, num_updates: 36500, {'loss': 0.24733921885490417, 'DER': 0.3849090909090909, 'ACC': 0.885625, 'MI': 0.16618181818181818, 'FA': 0.07127272727272728, 'CF': 0.14745454545454545}, batch size: 64, grad_norm: 1.4176716804504395, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:30:42,080 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:30:42,081 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38354, num_updates: 36500, {'loss': 0.24627289175987244, 'DER': 0.42295141406816533, 'ACC': 0.876953125, 'MI': 0.20087019579405366, 'FA': 0.07396664249456128, 'CF': 0.1481145757795504}, batch size: 64, grad_norm: 1.4176716804504395, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:30:42,082 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:31:47,453 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38354,  validation: loss=0.1273, DER=0.1825, ACC=0.9451, MI=0.05957, FA=0.04526, CF=0.07764, over 0.00 frames. 
2024-12-20 11:31:47,454 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 11:31:52,076 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38354,  validation: loss=0.1263, DER=0.1819, ACC=0.9455, MI=0.05882, FA=0.04565, CF=0.07744, over 0.00 frames. 
2024-12-20 11:31:52,076 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 11:46:36,811 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38854, num_updates: 37000, {'loss': 0.18510329723358154, 'DER': 0.3119496855345912, 'ACC': 0.9151953125, 'MI': 0.1757412398921833, 'FA': 0.05804132973944295, 'CF': 0.07816711590296496}, batch size: 64, grad_norm: 0.9148223400115967, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:46:36,812 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38854, num_updates: 37000, {'loss': 0.17969249188899994, 'DER': 0.3156960227272727, 'ACC': 0.9152734375, 'MI': 0.19300426136363635, 'FA': 0.053267045454545456, 'CF': 0.06942471590909091}, batch size: 64, grad_norm: 0.9148223400115967, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:46:36,812 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:46:36,812 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:47:43,346 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38854,  validation: loss=0.1265, DER=0.1826, ACC=0.9454, MI=0.0609, FA=0.04463, CF=0.07711, over 0.00 frames. 
2024-12-20 11:47:43,347 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 11:47:43,719 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38854,  validation: loss=0.1276, DER=0.1828, ACC=0.9451, MI=0.06162, FA=0.04397, CF=0.07719, over 0.00 frames. 
2024-12-20 11:47:43,720 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 11:51:55,979 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-39000.pt
2024-12-20 11:51:59,519 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 11:51:59,610 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 12:02:14,667 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39354, num_updates: 37500, {'loss': 0.16148924827575684, 'DER': 0.24506842011729163, 'ACC': 0.931015625, 'MI': 0.1384396658965701, 'FA': 0.03785320774835614, 'CF': 0.06877554647236538}, batch size: 64, grad_norm: 0.8538036942481995, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:02:14,668 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:02:14,675 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39354, num_updates: 37500, {'loss': 0.20402367413043976, 'DER': 0.32216819737079055, 'ACC': 0.904609375, 'MI': 0.1419052764271565, 'FA': 0.06266882766072393, 'CF': 0.11759409328291014}, batch size: 64, grad_norm: 0.8538036942481995, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:02:14,675 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:03:22,262 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39354,  validation: loss=0.1283, DER=0.1846, ACC=0.9446, MI=0.05713, FA=0.04927, CF=0.07824, over 0.00 frames. 
2024-12-20 12:03:22,262 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 12:03:22,335 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39354,  validation: loss=0.1273, DER=0.1837, ACC=0.945, MI=0.0567, FA=0.04933, CF=0.07769, over 0.00 frames. 
2024-12-20 12:03:22,335 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 12:17:37,510 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39854, num_updates: 38000, {'loss': 0.17726537585258484, 'DER': 0.3026909878995846, 'ACC': 0.9159375, 'MI': 0.14556619107820118, 'FA': 0.0711576666064656, 'CF': 0.08596713021491782}, batch size: 64, grad_norm: 1.1287603378295898, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:17:37,511 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:17:37,516 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39854, num_updates: 38000, {'loss': 0.24809695780277252, 'DER': 0.4072885196374622, 'ACC': 0.8866796875, 'MI': 0.15993202416918428, 'FA': 0.10687311178247734, 'CF': 0.1404833836858006}, batch size: 64, grad_norm: 1.1287603378295898, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:17:37,517 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:18:42,487 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39854,  validation: loss=0.126, DER=0.1816, ACC=0.9449, MI=0.05698, FA=0.04522, CF=0.07935, over 0.00 frames. 
2024-12-20 12:18:42,487 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 12:18:44,324 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39854,  validation: loss=0.1251, DER=0.1806, ACC=0.9456, MI=0.05589, FA=0.04542, CF=0.07928, over 0.00 frames. 
2024-12-20 12:18:44,325 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 12:21:41,702 (train_accelerate_ddp:709) INFO: end of epoch 19, batch_idx: 2102 batch_idx_train: 39956, {'loss': 0.20328590273857117, 'DER': 0.3546140452698781, 'ACC': 0.9128515625, 'MI': 0.1725672277036177, 'FA': 0.10504933255948927, 'CF': 0.07699748500677113}, batch size: 64, grad_norm: 1.062306523323059, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:21:41,704 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-19.pt
2024-12-20 12:21:41,706 (train_accelerate_ddp:709) INFO: end of epoch 19, batch_idx: 2102 batch_idx_train: 39956, {'loss': 0.22337788343429565, 'DER': 0.32768462361755263, 'ACC': 0.9051171875, 'MI': 0.1724937566892615, 'FA': 0.04958972529432751, 'CF': 0.10560114163396361}, batch size: 64, grad_norm: 1.062306523323059, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:21:44,615 (train_accelerate_ddp:557) INFO:  end of epoch 19, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-19.pt 
2024-12-20 12:21:48,750 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 39957, num_updates: 38000, {'loss': 0.1757202297449112, 'DER': 0.27471205088533607, 'ACC': 0.9228125, 'MI': 0.1820526044352759, 'FA': 0.027677496991576414, 'CF': 0.06498194945848375}, batch size: 64, grad_norm: 0.9596081376075745, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:21:48,755 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 39957, num_updates: 38000, {'loss': 0.20288275182247162, 'DER': 0.3598233995584989, 'ACC': 0.90171875, 'MI': 0.19665194996320823, 'FA': 0.06015452538631347, 'CF': 0.10301692420897719}, batch size: 64, grad_norm: 0.9596081376075745, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:36:18,562 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40457, num_updates: 38500, {'loss': 0.21301276981830597, 'DER': 0.3244511165321824, 'ACC': 0.9115625, 'MI': 0.14918371176580972, 'FA': 0.07487333458434979, 'CF': 0.1003940701820229}, batch size: 64, grad_norm: 1.33102285861969, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:36:18,562 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:36:18,563 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40457, num_updates: 38500, {'loss': 0.18869440257549286, 'DER': 0.31778741865509763, 'ACC': 0.9063671875, 'MI': 0.1473246565437455, 'FA': 0.0549530007230658, 'CF': 0.11550976138828634}, batch size: 64, grad_norm: 1.33102285861969, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:36:18,563 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:37:22,836 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40457,  validation: loss=0.1272, DER=0.1823, ACC=0.9454, MI=0.05476, FA=0.05023, CF=0.0773, over 0.00 frames. 
2024-12-20 12:37:22,836 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 12:37:23,851 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40457,  validation: loss=0.1284, DER=0.1838, ACC=0.9447, MI=0.0552, FA=0.05015, CF=0.07841, over 0.00 frames. 
2024-12-20 12:37:23,852 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 12:38:37,831 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-40500.pt
2024-12-20 12:38:41,555 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 12:38:41,622 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 12:51:50,064 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40957, num_updates: 39000, {'loss': 0.21971097588539124, 'DER': 0.3568221745071247, 'ACC': 0.9059765625, 'MI': 0.15244973648252977, 'FA': 0.09135272301385906, 'CF': 0.11301971501073589}, batch size: 64, grad_norm: 1.2279465198516846, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:51:50,065 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:51:50,070 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40957, num_updates: 39000, {'loss': 0.22194498777389526, 'DER': 0.3397790055248619, 'ACC': 0.906015625, 'MI': 0.1685082872928177, 'FA': 0.06795580110497237, 'CF': 0.10331491712707182}, batch size: 64, grad_norm: 1.2279465198516846, grad_scale: , lr: 0.00e+00, 
2024-12-20 12:51:50,070 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 12:52:54,089 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40957,  validation: loss=0.1263, DER=0.1811, ACC=0.9456, MI=0.05623, FA=0.04698, CF=0.07793, over 0.00 frames. 
2024-12-20 12:52:54,089 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 12:52:54,933 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40957,  validation: loss=0.1274, DER=0.1816, ACC=0.9451, MI=0.05689, FA=0.04651, CF=0.07819, over 0.00 frames. 
2024-12-20 12:52:54,933 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 13:07:06,057 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41457, num_updates: 39500, {'loss': 0.16479407250881195, 'DER': 0.2765249537892791, 'ACC': 0.9273046875, 'MI': 0.133086876155268, 'FA': 0.07597042513863217, 'CF': 0.06746765249537892}, batch size: 64, grad_norm: 0.8187980651855469, grad_scale: , lr: 0.00e+00, 
2024-12-20 13:07:06,057 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 13:07:06,058 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41457, num_updates: 39500, {'loss': 0.20585404336452484, 'DER': 0.33949826039186964, 'ACC': 0.9066796875, 'MI': 0.17890496246108772, 'FA': 0.06262589269364585, 'CF': 0.09796740523713605}, batch size: 64, grad_norm: 0.8187980651855469, grad_scale: , lr: 0.00e+00, 
2024-12-20 13:07:06,058 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 13:08:10,748 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41457,  validation: loss=0.126, DER=0.1812, ACC=0.9456, MI=0.05586, FA=0.04763, CF=0.07769, over 0.00 frames. 
2024-12-20 13:08:10,748 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 13:08:10,833 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41457,  validation: loss=0.1272, DER=0.1818, ACC=0.9451, MI=0.05648, FA=0.04726, CF=0.07809, over 0.00 frames. 
2024-12-20 13:08:10,834 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 13:22:26,092 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41957, num_updates: 40000, {'loss': 0.20889949798583984, 'DER': 0.3435158501440922, 'ACC': 0.9078125, 'MI': 0.1437079731027858, 'FA': 0.08991354466858789, 'CF': 0.10989433237271853}, batch size: 64, grad_norm: 0.8681772351264954, grad_scale: , lr: 0.00e+00, 
2024-12-20 13:22:26,093 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 13:22:26,097 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41957, num_updates: 40000, {'loss': 0.13755442202091217, 'DER': 0.2264426588750913, 'ACC': 0.94015625, 'MI': 0.10920379839298758, 'FA': 0.06391526661796933, 'CF': 0.053323593864134405}, batch size: 64, grad_norm: 0.8681772351264954, grad_scale: , lr: 0.00e+00, 
2024-12-20 13:22:26,097 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 13:23:29,919 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41957,  validation: loss=0.1249, DER=0.1803, ACC=0.946, MI=0.05623, FA=0.04688, CF=0.07717, over 0.00 frames. 
2024-12-20 13:23:29,919 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 13:23:30,861 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41957,  validation: loss=0.126, DER=0.1807, ACC=0.9456, MI=0.05716, FA=0.0463, CF=0.07721, over 0.00 frames. 
2024-12-20 13:23:30,861 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 6099MB
2024-12-20 13:24:46,903 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/checkpoint-42000.pt
2024-12-20 13:24:50,416 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 13:26:29,299 (train_accelerate_ddp:709) INFO: end of epoch 20, batch_idx: 2102 batch_idx_train: 42059, {'loss': 0.17798170447349548, 'DER': 0.2869015356820235, 'ACC': 0.924921875, 'MI': 0.17380307136404696, 'FA': 0.05275519421860885, 'CF': 0.06034327009936766}, batch size: 64, grad_norm: 0.8101692795753479, grad_scale: , lr: 0.00e+00, 
2024-12-20 13:26:29,299 (train_accelerate_ddp:709) INFO: end of epoch 20, batch_idx: 2102 batch_idx_train: 42059, {'loss': 0.16027037799358368, 'DER': 0.25853658536585367, 'ACC': 0.9269921875, 'MI': 0.15365853658536585, 'FA': 0.03780487804878049, 'CF': 0.06707317073170732}, batch size: 64, grad_norm: 0.8101692795753479, grad_scale: , lr: 0.00e+00, 
2024-12-20 13:26:29,301 (train_accelerate_ddp:1058) INFO: Done!
2024-12-20 13:26:29,301 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-20.pt
hltsz02:2570706:2570900 [1] NCCL INFO [Service thread] Connection closed by localRank 1
hltsz02:2570706:2570706 [1] NCCL INFO comm 0xbc7f340 rank 1 nranks 2 cudaDev 1 busId 3d000 - Abort COMPLETE
2024-12-20 13:26:32,054 (train_accelerate_ddp:557) INFO:  end of epoch 20, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/epoch-20.pt 
2024-12-20 13:26:40,872 (train_accelerate_ddp:1058) INFO: Done!
hltsz02:2570705:2570899 [0] NCCL INFO [Service thread] Connection closed by localRank 0
hltsz02:2570705:2570705 [0] NCCL INFO comm 0xb427cc0 rank 0 nranks 2 cudaDev 0 busId 1b000 - Abort COMPLETE
2024-12-20 13:26:50,111 (infer:252) INFO: infer data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=1, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/mntcephfs/lee_dataset/asr/musan', rir_path='/mntcephfs/lee_dataset/asr/RIRS_NOISES')
2024-12-20 13:26:50,111 (infer:253) INFO: currently, it will infer dev set.
  0%|          | 0/38 [00:00<?, ?it/s] 71%|███████   | 27/38 [00:00<00:00, 262.28it/s]100%|██████████| 38/38 [00:00<00:00, 263.19it/s]
2024-12-20 13:26:50,275 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 13:26:50,276 (ts_vad_dataset:160) INFO: loaded sentence=35613, shortest sent=3200.0, longest sent=64000.0, rs_len=4, segment_shift=1,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
2024-12-20 13:26:50,564 (infer:275) INFO: Device: cuda:0
2024-12-20 13:26:50,565 (infer:290) INFO: infer model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba_v2', multi_backend_type='mamba_v2')
2024-12-20 13:26:51,805 (infer:201) INFO: params.model_file: /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/best-valid-der.pt
2024-12-20 13:32:02,678 (infer:89) INFO: frame_len: 0.04!!
self.wavlm_fuse_feat_post_norm: False
Model DER:  0.18229321528589693
Model ACC:  0.9453705144788408
  0%|          | 0/38 [00:00<?, ?it/s]  3%|▎         | 1/38 [00:00<00:35,  1.04it/s]  5%|▌         | 2/38 [00:01<00:34,  1.05it/s]  8%|▊         | 3/38 [00:02<00:33,  1.04it/s] 11%|█         | 4/38 [00:03<00:32,  1.03it/s] 13%|█▎        | 5/38 [00:04<00:31,  1.03it/s] 16%|█▌        | 6/38 [00:05<00:30,  1.03it/s] 18%|█▊        | 7/38 [00:06<00:30,  1.02it/s] 21%|██        | 8/38 [00:08<00:35,  1.19s/it] 24%|██▎       | 9/38 [00:09<00:32,  1.11s/it] 26%|██▋       | 10/38 [00:10<00:29,  1.06s/it] 29%|██▉       | 11/38 [00:11<00:27,  1.03s/it] 32%|███▏      | 12/38 [00:12<00:26,  1.01s/it] 34%|███▍      | 13/38 [00:13<00:25,  1.01s/it] 37%|███▋      | 14/38 [00:14<00:24,  1.01s/it] 39%|███▉      | 15/38 [00:15<00:22,  1.00it/s] 42%|████▏     | 16/38 [00:16<00:21,  1.01it/s] 45%|████▍     | 17/38 [00:17<00:21,  1.00s/it] 47%|████▋     | 18/38 [00:18<00:20,  1.01s/it] 50%|█████     | 19/38 [00:19<00:18,  1.00it/s] 53%|█████▎    | 20/38 [00:20<00:21,  1.18s/it] 55%|█████▌    | 21/38 [00:21<00:18,  1.11s/it] 58%|█████▊    | 22/38 [00:22<00:17,  1.07s/it] 61%|██████    | 23/38 [00:23<00:15,  1.04s/it] 63%|██████▎   | 24/38 [00:24<00:14,  1.02s/it] 66%|██████▌   | 25/38 [00:25<00:13,  1.01s/it] 68%|██████▊   | 26/38 [00:26<00:12,  1.00s/it] 71%|███████   | 27/38 [00:27<00:10,  1.02it/s] 74%|███████▎  | 28/38 [00:28<00:09,  1.04it/s] 76%|███████▋  | 29/38 [00:29<00:08,  1.04it/s] 79%|███████▉  | 30/38 [00:30<00:07,  1.05it/s] 82%|████████▏ | 31/38 [00:31<00:06,  1.04it/s] 84%|████████▍ | 32/38 [00:32<00:05,  1.04it/s] 87%|████████▋ | 33/38 [00:33<00:04,  1.03it/s] 89%|████████▉ | 34/38 [00:34<00:04,  1.15s/it] 92%|█████████▏| 35/38 [00:35<00:03,  1.09s/it] 95%|█████████▍| 36/38 [00:36<00:02,  1.04s/it] 97%|█████████▋| 37/38 [00:37<00:01,  1.03s/it]100%|██████████| 38/38 [00:38<00:00,  1.02s/it]100%|██████████| 38/38 [00:38<00:00,  1.02s/it]
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
Eval for threshold 0.2 DER=31.52, miss=0.53, falarm=27.98, confusion=3.01


Eval for threshold 0.3 DER=24.98, miss=1.01, falarm=19.33, confusion=4.64


Eval for threshold 0.35 DER=22.08, miss=1.33, falarm=15.47, confusion=5.27


Eval for threshold 0.4 DER=19.42, miss=1.82, falarm=11.51, confusion=6.09


Eval for threshold 0.45 DER=17.71, miss=2.58, falarm=8.32, confusion=6.80


Eval for threshold 0.5 DER=17.26, miss=3.99, falarm=6.25, confusion=7.02


Eval for threshold 0.55 DER=18.12, miss=6.49, falarm=5.20, confusion=6.43


Eval for threshold 0.6 DER=19.99, miss=10.01, falarm=4.64, confusion=5.34


Eval for threshold 0.7 DER=24.19, miss=16.85, falarm=3.73, confusion=3.61


Eval for threshold 0.8 DER=29.40, miss=24.68, falarm=2.79, confusion=1.94


2024-12-20 13:32:59,051 (infer:252) INFO: infer data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=1, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/mntcephfs/lee_dataset/asr/musan', rir_path='/mntcephfs/lee_dataset/asr/RIRS_NOISES')
2024-12-20 13:32:59,051 (infer:253) INFO: currently, it will infer test set.
  0%|          | 0/86 [00:00<?, ?it/s] 37%|███▋      | 32/86 [00:00<00:00, 315.86it/s] 74%|███████▍  | 64/86 [00:00<00:00, 172.64it/s]100%|██████████| 86/86 [00:00<00:00, 198.56it/s]
2024-12-20 13:32:59,522 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 13:32:59,524 (ts_vad_dataset:160) INFO: loaded sentence=74337, shortest sent=640.0, longest sent=64000.0, rs_len=4, segment_shift=1,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
2024-12-20 13:32:59,808 (infer:275) INFO: Device: cuda:0
2024-12-20 13:32:59,808 (infer:290) INFO: infer model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba_v2', multi_backend_type='mamba_v2')
2024-12-20 13:33:01,135 (infer:201) INFO: params.model_file: /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_v2/best-valid-der.pt
2024-12-20 13:44:27,550 (infer:89) INFO: frame_len: 0.04!!
self.wavlm_fuse_feat_post_norm: False
Model DER:  0.19011261252460646
Model ACC:  0.9483957519566986
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:01<01:34,  1.11s/it]  2%|▏         | 2/86 [00:02<01:35,  1.14s/it]  3%|▎         | 3/86 [00:02<01:13,  1.13it/s]  5%|▍         | 4/86 [00:03<01:01,  1.33it/s]  6%|▌         | 5/86 [00:03<00:54,  1.50it/s]  7%|▋         | 6/86 [00:04<00:49,  1.61it/s]  8%|▊         | 7/86 [00:05<00:47,  1.65it/s]  9%|▉         | 8/86 [00:05<00:46,  1.68it/s] 10%|█         | 9/86 [00:06<00:44,  1.74it/s] 12%|█▏        | 10/86 [00:06<00:42,  1.78it/s] 13%|█▎        | 11/86 [00:07<00:42,  1.78it/s] 14%|█▍        | 12/86 [00:07<00:41,  1.78it/s] 15%|█▌        | 13/86 [00:08<00:40,  1.79it/s] 16%|█▋        | 14/86 [00:08<00:40,  1.78it/s] 17%|█▋        | 15/86 [00:10<00:53,  1.32it/s] 19%|█▊        | 16/86 [00:11<01:02,  1.13it/s] 20%|█▉        | 17/86 [00:12<01:07,  1.03it/s] 21%|██        | 18/86 [00:13<01:09,  1.03s/it] 22%|██▏       | 19/86 [00:14<01:09,  1.04s/it] 23%|██▎       | 20/86 [00:15<01:09,  1.06s/it] 24%|██▍       | 21/86 [00:16<01:10,  1.08s/it] 26%|██▌       | 22/86 [00:19<01:33,  1.46s/it] 27%|██▋       | 23/86 [00:20<01:26,  1.37s/it] 28%|██▊       | 24/86 [00:21<01:21,  1.31s/it] 29%|██▉       | 25/86 [00:22<01:16,  1.26s/it] 30%|███       | 26/86 [00:23<01:13,  1.22s/it] 31%|███▏      | 27/86 [00:25<01:10,  1.20s/it] 33%|███▎      | 28/86 [00:26<01:08,  1.19s/it] 34%|███▎      | 29/86 [00:27<01:07,  1.18s/it] 35%|███▍      | 30/86 [00:28<01:05,  1.17s/it] 36%|███▌      | 31/86 [00:29<01:04,  1.17s/it] 37%|███▋      | 32/86 [00:30<01:03,  1.17s/it] 38%|███▊      | 33/86 [00:31<01:01,  1.16s/it] 40%|███▉      | 34/86 [00:33<01:00,  1.16s/it] 41%|████      | 35/86 [00:34<00:59,  1.17s/it] 42%|████▏     | 36/86 [00:35<00:58,  1.17s/it] 43%|████▎     | 37/86 [00:36<00:57,  1.18s/it] 44%|████▍     | 38/86 [00:37<00:56,  1.18s/it] 45%|████▌     | 39/86 [00:39<00:55,  1.17s/it] 47%|████▋     | 40/86 [00:40<00:53,  1.17s/it] 48%|████▊     | 41/86 [00:41<00:51,  1.15s/it] 49%|████▉     | 42/86 [00:42<00:50,  1.14s/it] 50%|█████     | 43/86 [00:43<00:49,  1.14s/it] 51%|█████     | 44/86 [00:44<00:47,  1.14s/it] 52%|█████▏    | 45/86 [00:45<00:46,  1.14s/it] 53%|█████▎    | 46/86 [00:47<00:45,  1.15s/it] 55%|█████▍    | 47/86 [00:49<00:58,  1.50s/it] 56%|█████▌    | 48/86 [00:50<00:52,  1.39s/it] 57%|█████▋    | 49/86 [00:51<00:49,  1.34s/it] 58%|█████▊    | 50/86 [00:52<00:46,  1.29s/it] 59%|█████▉    | 51/86 [00:54<00:43,  1.26s/it] 60%|██████    | 52/86 [00:55<00:41,  1.22s/it] 62%|██████▏   | 53/86 [00:56<00:39,  1.19s/it] 63%|██████▎   | 54/86 [00:57<00:37,  1.17s/it] 64%|██████▍   | 55/86 [00:58<00:36,  1.17s/it] 65%|██████▌   | 56/86 [00:59<00:35,  1.17s/it] 66%|██████▋   | 57/86 [01:00<00:34,  1.18s/it] 67%|██████▋   | 58/86 [01:02<00:32,  1.17s/it] 69%|██████▊   | 59/86 [01:03<00:31,  1.17s/it] 70%|██████▉   | 60/86 [01:04<00:30,  1.16s/it] 71%|███████   | 61/86 [01:05<00:29,  1.17s/it] 72%|███████▏  | 62/86 [01:06<00:28,  1.18s/it] 73%|███████▎  | 63/86 [01:07<00:26,  1.17s/it] 74%|███████▍  | 64/86 [01:09<00:25,  1.16s/it] 76%|███████▌  | 65/86 [01:10<00:24,  1.16s/it] 77%|███████▋  | 66/86 [01:11<00:22,  1.14s/it] 78%|███████▊  | 67/86 [01:12<00:21,  1.15s/it] 79%|███████▉  | 68/86 [01:13<00:20,  1.15s/it] 80%|████████  | 69/86 [01:14<00:19,  1.16s/it] 81%|████████▏ | 70/86 [01:16<00:18,  1.17s/it] 83%|████████▎ | 71/86 [01:17<00:17,  1.16s/it] 84%|████████▎ | 72/86 [01:19<00:20,  1.49s/it] 85%|████████▍ | 73/86 [01:20<00:18,  1.40s/it] 86%|████████▌ | 74/86 [01:21<00:16,  1.35s/it] 87%|████████▋ | 75/86 [01:23<00:14,  1.30s/it] 88%|████████▊ | 76/86 [01:24<00:12,  1.27s/it] 90%|████████▉ | 77/86 [01:25<00:11,  1.23s/it] 91%|█████████ | 78/86 [01:26<00:09,  1.21s/it] 92%|█████████▏| 79/86 [01:27<00:08,  1.20s/it] 93%|█████████▎| 80/86 [01:28<00:07,  1.18s/it] 94%|█████████▍| 81/86 [01:29<00:05,  1.16s/it] 95%|█████████▌| 82/86 [01:31<00:04,  1.15s/it] 97%|█████████▋| 83/86 [01:32<00:03,  1.16s/it] 98%|█████████▊| 84/86 [01:33<00:02,  1.17s/it] 99%|█████████▉| 85/86 [01:34<00:01,  1.17s/it]100%|██████████| 86/86 [01:35<00:00,  1.17s/it]100%|██████████| 86/86 [01:35<00:00,  1.11s/it]
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
Eval for threshold 0.2 DER=30.24, miss=0.63, falarm=27.22, confusion=2.39


Eval for threshold 0.3 DER=23.15, miss=1.23, falarm=18.66, confusion=3.27


Eval for threshold 0.35 DER=20.47, miss=1.69, falarm=14.98, confusion=3.80


Eval for threshold 0.4 DER=18.43, miss=2.42, falarm=11.59, confusion=4.41


Eval for threshold 0.45 DER=17.16, miss=3.70, falarm=8.62, confusion=4.85


Eval for threshold 0.5 DER=17.01, miss=5.70, falarm=6.52, confusion=4.80


Eval for threshold 0.55 DER=17.96, miss=8.47, falarm=5.26, confusion=4.22


Eval for threshold 0.6 DER=19.91, miss=11.97, falarm=4.67, confusion=3.27


Eval for threshold 0.7 DER=24.32, miss=18.38, falarm=3.87, confusion=2.07


Eval for threshold 0.8 DER=29.83, miss=25.13, falarm=3.11, confusion=1.59


