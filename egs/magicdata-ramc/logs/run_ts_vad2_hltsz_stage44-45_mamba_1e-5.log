The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-12-20 14:24:15,348 (train_accelerate_ddp:854) INFO: params: {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_der': inf, 'best_valid_der': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 500, 'reset_interval': 200, 'valid_interval': 500, 'batch_size': 64, 'verbose': 1, 'world_size': 2, 'tensorboard': True, 'num_epochs': 20, 'max_updates': 40000, 'warmup_updates': 4000, 'freeze_updates': 4000, 'start_batch': 0, 'start_epoch': 1, 'seed': 1337, 'exp_dir': PosixPath('/data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba'), 'save_every_n': 1500, 'keep_last_k': 1, 'keep_last_epoch': 1, 'grad_clip': True, 'feature_grad_mult': 0.1, 'lr': 1e-05, 'average_period': 200, 'train_on_average': False, 'musan_path': '/data/maduo/datasets/musan', 'rir_path': '/data/maduo/datasets/RIRS_NOISES', 'spk_path': '/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', 'speaker_embedding_name_dir': 'cam++_zh-cn_200k_feature_dir', 'data_dir': '/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', 'dataset_name': 'magicdata-ramc', 'max_num_speaker': 4, 'speech_encoder_path': '/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', 'select_encoder_layer_nums': 6, 'wavlm_fuse_feat_post_norm': False, 'speech_encoder_config': '/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', 'num_transformer_layer': 4, 'speech_encoder_type': 'CAM++', 'speaker_embed_dim': 192, 'rs_len': 4, 'segment_shift': 2, 'single_backend_type': 'mamba', 'multi_backend_type': 'mamba', 'do_finetune': False, 'init_modules': None, 'finetune_ckpt': None}
2024-12-20 14:24:15,348 (train_accelerate_ddp:854) INFO: params: {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_der': inf, 'best_valid_der': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 500, 'reset_interval': 200, 'valid_interval': 500, 'batch_size': 64, 'verbose': 1, 'world_size': 2, 'tensorboard': True, 'num_epochs': 20, 'max_updates': 40000, 'warmup_updates': 4000, 'freeze_updates': 4000, 'start_batch': 0, 'start_epoch': 1, 'seed': 1337, 'exp_dir': PosixPath('/data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba'), 'save_every_n': 1500, 'keep_last_k': 1, 'keep_last_epoch': 1, 'grad_clip': True, 'feature_grad_mult': 0.1, 'lr': 1e-05, 'average_period': 200, 'train_on_average': False, 'musan_path': '/data/maduo/datasets/musan', 'rir_path': '/data/maduo/datasets/RIRS_NOISES', 'spk_path': '/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', 'speaker_embedding_name_dir': 'cam++_zh-cn_200k_feature_dir', 'data_dir': '/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', 'dataset_name': 'magicdata-ramc', 'max_num_speaker': 4, 'speech_encoder_path': '/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', 'select_encoder_layer_nums': 6, 'wavlm_fuse_feat_post_norm': False, 'speech_encoder_config': '/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', 'num_transformer_layer': 4, 'speech_encoder_type': 'CAM++', 'speaker_embed_dim': 192, 'rs_len': 4, 'segment_shift': 2, 'single_backend_type': 'mamba', 'multi_backend_type': 'mamba', 'do_finetune': False, 'init_modules': None, 'finetune_ckpt': None}
2024-12-20 14:24:15,349 (train_accelerate_ddp:870) INFO: data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=2, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/data/maduo/datasets/musan', rir_path='/data/maduo/datasets/RIRS_NOISES')
2024-12-20 14:24:15,349 (train_accelerate_ddp:870) INFO: data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=2, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/data/maduo/datasets/musan', rir_path='/data/maduo/datasets/RIRS_NOISES')
  0%|          | 0/38 [00:00<?, ?it/s]  0%|          | 0/38 [00:00<?, ?it/s] 71%|███████   | 27/38 [00:00<00:00, 267.33it/s] 71%|███████   | 27/38 [00:00<00:00, 266.60it/s]100%|██████████| 38/38 [00:00<00:00, 271.97it/s]
100%|██████████| 38/38 [00:00<00:00, 272.32it/s]
2024-12-20 14:24:15,506 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 14:24:15,506 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 14:24:15,507 (ts_vad_dataset:160) INFO: loaded sentence=17812, shortest sent=3840.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
2024-12-20 14:24:15,507 (ts_vad_dataset:160) INFO: loaded sentence=17812, shortest sent=3840.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
  0%|          | 0/578 [00:00<?, ?it/s]  0%|          | 0/578 [00:00<?, ?it/s]  5%|▍         | 27/578 [00:00<00:02, 264.58it/s]  5%|▍         | 27/578 [00:00<00:02, 266.67it/s] 10%|▉         | 55/578 [00:00<00:01, 272.41it/s] 10%|▉         | 55/578 [00:00<00:01, 273.27it/s] 14%|█▍        | 83/578 [00:00<00:02, 172.74it/s] 14%|█▍        | 83/578 [00:00<00:02, 172.94it/s] 19%|█▉        | 110/578 [00:00<00:02, 199.66it/s] 19%|█▉        | 110/578 [00:00<00:02, 199.78it/s] 24%|██▎       | 136/578 [00:00<00:02, 217.15it/s] 24%|██▎       | 136/578 [00:00<00:02, 216.34it/s] 28%|██▊       | 162/578 [00:00<00:01, 229.01it/s] 28%|██▊       | 163/578 [00:00<00:01, 231.18it/s] 33%|███▎      | 188/578 [00:00<00:01, 236.72it/s] 33%|███▎      | 190/578 [00:00<00:01, 240.47it/s] 37%|███▋      | 214/578 [00:00<00:01, 243.52it/s] 38%|███▊      | 217/578 [00:00<00:01, 247.03it/s] 42%|████▏     | 241/578 [00:01<00:01, 249.46it/s] 42%|████▏     | 244/578 [00:01<00:01, 252.14it/s] 46%|████▋     | 268/578 [00:01<00:01, 254.26it/s] 47%|████▋     | 271/578 [00:01<00:01, 256.90it/s] 51%|█████     | 294/578 [00:01<00:01, 170.70it/s] 52%|█████▏    | 298/578 [00:01<00:01, 173.42it/s] 55%|█████▌    | 320/578 [00:01<00:01, 189.74it/s] 56%|█████▌    | 324/578 [00:01<00:01, 192.01it/s] 60%|█████▉    | 346/578 [00:01<00:01, 206.08it/s] 61%|██████    | 350/578 [00:01<00:01, 207.34it/s] 64%|██████▍   | 372/578 [00:01<00:00, 218.27it/s] 65%|██████▌   | 377/578 [00:01<00:00, 222.23it/s] 70%|██████▉   | 403/578 [00:01<00:00, 229.76it/s] 69%|██████▊   | 396/578 [00:01<00:00, 212.17it/s] 74%|███████▍  | 429/578 [00:01<00:00, 236.92it/s] 73%|███████▎  | 421/578 [00:01<00:00, 220.06it/s] 79%|███████▊  | 455/578 [00:02<00:00, 240.78it/s] 77%|███████▋  | 447/578 [00:02<00:00, 229.76it/s] 83%|████████▎ | 482/578 [00:02<00:00, 247.37it/s] 82%|████████▏ | 473/578 [00:02<00:00, 235.69it/s] 88%|████████▊ | 509/578 [00:02<00:00, 249.35it/s] 86%|████████▋ | 499/578 [00:02<00:00, 242.53it/s] 93%|█████████▎| 535/578 [00:02<00:00, 151.74it/s] 91%|█████████ | 524/578 [00:02<00:00, 148.09it/s] 97%|█████████▋| 561/578 [00:02<00:00, 172.16it/s] 95%|█████████▍| 549/578 [00:02<00:00, 168.16it/s]100%|██████████| 578/578 [00:02<00:00, 210.82it/s]
2024-12-20 14:24:18,478 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 14:24:18,488 (ts_vad_dataset:160) INFO: loaded sentence=269074, shortest sent=48640.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=True, musan=True, noise_ratio=0.8, zero_ratio=0.3 
2024-12-20 14:24:18,489 (train_accelerate_ddp:905) INFO: The scale window is set to 8192.
 99%|█████████▉| 575/578 [00:02<00:00, 187.49it/s]100%|██████████| 578/578 [00:02<00:00, 207.25it/s]
2024-12-20 14:24:18,526 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 14:24:18,537 (ts_vad_dataset:160) INFO: loaded sentence=269074, shortest sent=48640.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=True, musan=True, noise_ratio=0.8, zero_ratio=0.3 
2024-12-20 14:24:18,537 (train_accelerate_ddp:905) INFO: The scale window is set to 8192.
2024-12-20 14:24:18,967 (train_accelerate_ddp:941) INFO: model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=4, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
self.wavlm_fuse_feat_post_norm: False
2024-12-20 14:24:18,969 (train_accelerate_ddp:941) INFO: model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=4, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
self.wavlm_fuse_feat_post_norm: False
2024-12-20 14:24:19,867 (train_accelerate_ddp:944) INFO: model: TSVADModel(
  (rs_dropout): Dropout(p=0.1, inplace=False)
  (speech_encoder): CAMPPlus(
    (head): FCM(
      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layer1): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (layer2): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (xvector): Sequential(
      (tdnn): TDNNLayer(
        (linear): Conv1d(320, 128, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (block1): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(160, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(192, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(224, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit1): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block2): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (16): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (17): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (18): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (19): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (20): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (21): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (22): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (23): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit2): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block3): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit3): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (out_nonlinear): Sequential(
        (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (stats): StatsPool()
      (dense): DenseLayer(
        (linear): Conv1d(1024, 192, kernel_size=(1,), stride=(1,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
  (speech_down_or_up): Sequential(
    (0): Conv1d(512, 192, kernel_size=(5,), stride=(2,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (single_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (backend_down): Sequential(
    (0): Conv1d(3072, 384, kernel_size=(5,), stride=(1,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (multi_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
)
2024-12-20 14:24:19,868 (train_accelerate_ddp:944) INFO: model: TSVADModel(
  (rs_dropout): Dropout(p=0.1, inplace=False)
  (speech_encoder): CAMPPlus(
    (head): FCM(
      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layer1): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (layer2): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (xvector): Sequential(
      (tdnn): TDNNLayer(
        (linear): Conv1d(320, 128, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (block1): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(160, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(192, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(224, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit1): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block2): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (16): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (17): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (18): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (19): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (20): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (21): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (22): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (23): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit2): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block3): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit3): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (out_nonlinear): Sequential(
        (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (stats): StatsPool()
      (dense): DenseLayer(
        (linear): Conv1d(1024, 192, kernel_size=(1,), stride=(1,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
  (speech_down_or_up): Sequential(
    (0): Conv1d(512, 192, kernel_size=(5,), stride=(2,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (single_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (backend_down): Sequential(
    (0): Conv1d(3072, 384, kernel_size=(5,), stride=(1,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (multi_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-3): 4 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
)
2024-12-20 14:24:19,869 (train_accelerate_ddp:946) INFO: Number of model parameters: 47631076
2024-12-20 14:24:19,871 (train_accelerate_ddp:946) INFO: Number of model parameters: 47631076
hltsz02:2630240:2630240 [0] NCCL INFO Bootstrap : Using eno1:10.26.1.136<0>
hltsz02:2630240:2630240 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
hltsz02:2630240:2630240 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
hltsz02:2630240:2630240 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.6+cuda11.8
hltsz02:2630241:2630241 [1] NCCL INFO cudaDriverVersion 12050
hltsz02:2630241:2630241 [1] NCCL INFO Bootstrap : Using eno1:10.26.1.136<0>
hltsz02:2630241:2630241 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
hltsz02:2630241:2630241 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
hltsz02:2630241:2630280 [1] NCCL INFO Failed to open libibverbs.so[.1]
hltsz02:2630241:2630280 [1] NCCL INFO NET/Socket : Using [0]eno1:10.26.1.136<0> [1]eno2:192.168.1.102<0>
hltsz02:2630241:2630280 [1] NCCL INFO Using network Socket
hltsz02:2630240:2630279 [0] NCCL INFO Failed to open libibverbs.so[.1]
hltsz02:2630240:2630279 [0] NCCL INFO NET/Socket : Using [0]eno1:10.26.1.136<0> [1]eno2:192.168.1.102<0>
hltsz02:2630240:2630279 [0] NCCL INFO Using network Socket
hltsz02:2630240:2630279 [0] NCCL INFO comm 0x1e729d40 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3e000 commId 0x887fc2e142c193c7 - Init START
hltsz02:2630241:2630280 [1] NCCL INFO comm 0x1e9a1780 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 88000 commId 0x887fc2e142c193c7 - Init START
hltsz02:2630240:2630279 [0] NCCL INFO Setting affinity for GPU 0 to 78,00000000,00078000
hltsz02:2630241:2630280 [1] NCCL INFO Setting affinity for GPU 1 to 3c0000,00000003,c0000000
hltsz02:2630241:2630280 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
hltsz02:2630241:2630280 [1] NCCL INFO P2P Chunksize set to 131072
hltsz02:2630240:2630279 [0] NCCL INFO Channel 00/02 :    0   1
hltsz02:2630240:2630279 [0] NCCL INFO Channel 01/02 :    0   1
hltsz02:2630240:2630279 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
hltsz02:2630240:2630279 [0] NCCL INFO P2P Chunksize set to 131072
hltsz02:2630241:2630280 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
hltsz02:2630241:2630280 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
hltsz02:2630240:2630279 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
hltsz02:2630240:2630279 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
hltsz02:2630240:2630279 [0] NCCL INFO Connected all rings
hltsz02:2630240:2630279 [0] NCCL INFO Connected all trees
hltsz02:2630240:2630279 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
hltsz02:2630240:2630279 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
hltsz02:2630241:2630280 [1] NCCL INFO Connected all rings
hltsz02:2630241:2630280 [1] NCCL INFO Connected all trees
hltsz02:2630241:2630280 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
hltsz02:2630241:2630280 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
hltsz02:2630241:2630280 [1] NCCL INFO comm 0x1e9a1780 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 88000 commId 0x887fc2e142c193c7 - Init COMPLETE
hltsz02:2630240:2630279 [0] NCCL INFO comm 0x1e729d40 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3e000 commId 0x887fc2e142c193c7 - Init COMPLETE
2024-12-20 14:24:20,911 (train_accelerate_ddp:1025) INFO: start training from epoch 1
2024-12-20 14:24:20,911 (train_accelerate_ddp:1026) INFO: Train set grouped total_num_itrs = 2103
2024-12-20 14:24:20,911 (train_accelerate_ddp:1025) INFO: start training from epoch 1
2024-12-20 14:24:20,911 (train_accelerate_ddp:1026) INFO: Train set grouped total_num_itrs = 2103
2024-12-20 14:24:27,513 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 0, num_updates: 0, {'loss': 0.8163720369338989, 'DER': 2.505922469490309, 'ACC': 0.3759375, 'MI': 0.003948312993539124, 'FA': 2.14070351758794, 'CF': 0.3612706389088299}, batch size: 64, grad_norm: 4.244385719299316, grad_scale: , lr: 5.00e-09, 
2024-12-20 14:24:27,520 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 0, num_updates: 0, {'loss': 0.8076398372650146, 'DER': 2.585836114200964, 'ACC': 0.38296875, 'MI': 0.004078605858361142, 'FA': 2.239154616240267, 'CF': 0.3426028921023359}, batch size: 64, grad_norm: 4.244385719299316, grad_scale: , lr: 5.00e-09, 
2024-12-20 14:44:39,335 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 500, num_updates: 500, {'loss': 0.49975165724754333, 'DER': 0.9958906557084152, 'ACC': 0.7806640625, 'MI': 0.9883866356976952, 'FA': 0.00017866714311238162, 'CF': 0.007325352867607647}, batch size: 64, grad_norm: 0.7748257517814636, grad_scale: , lr: 2.51e-06, 
2024-12-20 14:44:39,336 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 14:44:39,336 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 500, num_updates: 500, {'loss': 0.4950467050075531, 'DER': 0.9953617810760668, 'ACC': 0.7867578125, 'MI': 0.9771799628942486, 'FA': 0.0007421150278293135, 'CF': 0.017439703153988868}, batch size: 64, grad_norm: 0.7748257517814636, grad_scale: , lr: 2.51e-06, 
2024-12-20 14:44:39,336 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 14:46:18,596 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 500,  validation: loss=0.4307, DER=0.999, ACC=0.7903, MI=0.9985, FA=8.879e-06, CF=0.0004378, over 0.00 frames. 
2024-12-20 14:46:18,597 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 14:46:18,657 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 500,  validation: loss=0.4312, DER=0.9992, ACC=0.7902, MI=0.9986, FA=1.208e-05, CF=0.0005506, over 0.00 frames. 
2024-12-20 14:46:18,657 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 15:06:41,303 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 1000, num_updates: 1000, {'loss': 0.45371899008750916, 'DER': 0.9214528538200035, 'ACC': 0.785625, 'MI': 0.8400429414922168, 'FA': 0.020933977455716585, 'CF': 0.06047593487207014}, batch size: 64, grad_norm: 0.667182445526123, grad_scale: , lr: 5.00e-06, 
2024-12-20 15:06:41,304 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 15:06:41,310 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 1000, num_updates: 1000, {'loss': 0.44335657358169556, 'DER': 0.8737511353315168, 'ACC': 0.791484375, 'MI': 0.7616712079927339, 'FA': 0.016167120799273387, 'CF': 0.09591280653950954}, batch size: 64, grad_norm: 0.667182445526123, grad_scale: , lr: 5.00e-06, 
2024-12-20 15:06:41,310 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 15:08:20,877 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 1000,  validation: loss=0.3308, DER=0.7374, ACC=0.8033, MI=0.4539, FA=0.0872, CF=0.1963, over 0.00 frames. 
2024-12-20 15:08:20,877 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 15:08:20,940 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 1000,  validation: loss=0.3297, DER=0.7295, ACC=0.8049, MI=0.4481, FA=0.08868, CF=0.1927, over 0.00 frames. 
2024-12-20 15:08:20,940 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 15:28:36,098 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-1500.pt
2024-12-20 15:28:43,899 (checkpoint:349) WARNING: No checkpoints found in /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba
2024-12-20 15:28:46,538 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 1500, num_updates: 1500, {'loss': 0.42040857672691345, 'DER': 0.8924302788844621, 'ACC': 0.792734375, 'MI': 0.798804780876494, 'FA': 0.025172039116262222, 'CF': 0.0684534588917059}, batch size: 64, grad_norm: 0.6451578736305237, grad_scale: , lr: 7.51e-06, 
2024-12-20 15:28:46,538 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 15:28:46,545 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 1500, num_updates: 1500, {'loss': 0.4314548969268799, 'DER': 0.9082301529497451, 'ACC': 0.7877979924717692, 'MI': 0.8040786598689003, 'FA': 0.026948288419519302, 'CF': 0.07720320466132556}, batch size: 64, grad_norm: 0.6451578736305237, grad_scale: , lr: 7.51e-06, 
2024-12-20 15:28:46,545 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 15:30:25,886 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 1500,  validation: loss=0.2934, DER=0.6094, ACC=0.8392, MI=0.3495, FA=0.102, CF=0.1578, over 0.00 frames. 
2024-12-20 15:30:25,887 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 15:30:27,069 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 1500,  validation: loss=0.2909, DER=0.6, ACC=0.8431, MI=0.3468, FA=0.1066, CF=0.1466, over 0.00 frames. 
2024-12-20 15:30:27,069 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 15:51:24,918 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 2000, num_updates: 2000, {'loss': 0.3293346166610718, 'DER': 0.6415161176053843, 'ACC': 0.836796875, 'MI': 0.4628055260361318, 'FA': 0.08023379383634431, 'CF': 0.09847679773290825}, batch size: 64, grad_norm: 0.5605747699737549, grad_scale: , lr: 1.00e-05, 
2024-12-20 15:51:24,918 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 15:51:24,920 (train_accelerate_ddp:699) INFO: [Train] - Epoch 1, batch_idx_train: 2000, num_updates: 2000, {'loss': 0.3839096426963806, 'DER': 0.7913824057450628, 'ACC': 0.8054296875, 'MI': 0.5928186714542191, 'FA': 0.09569120287253143, 'CF': 0.10287253141831239}, batch size: 64, grad_norm: 0.5605747699737549, grad_scale: , lr: 1.00e-05, 
2024-12-20 15:51:24,920 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 15:53:04,296 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 2000,  validation: loss=0.2454, DER=0.522, ACC=0.8658, MI=0.1302, FA=0.279, CF=0.1127, over 0.00 frames. 
2024-12-20 15:53:04,297 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 15:53:04,771 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 1, batch_idx_train: 2000,  validation: loss=0.2454, DER=0.5219, ACC=0.8677, MI=0.1312, FA=0.29, CF=0.1006, over 0.00 frames. 
2024-12-20 15:53:04,771 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 15:57:22,918 (train_accelerate_ddp:710) INFO: end of epoch 1, batch_idx: 2102 batch_idx_train: 2102, {'loss': 0.3551785945892334, 'DER': 0.6951509606587374, 'ACC': 0.8253125, 'MI': 0.45032021957914, 'FA': 0.12168344007319305, 'CF': 0.12314730100640439}, batch size: 64, grad_norm: 0.6699311137199402, grad_scale: , lr: 9.94e-06, 
2024-12-20 15:57:22,924 (train_accelerate_ddp:710) INFO: end of epoch 1, batch_idx: 2102 batch_idx_train: 2102, {'loss': 0.34741467237472534, 'DER': 0.693065625580963, 'ACC': 0.8265234375, 'MI': 0.450269566833984, 'FA': 0.11024353969139246, 'CF': 0.13255251905558654}, batch size: 64, grad_norm: 0.6699311137199402, grad_scale: , lr: 9.94e-06, 
2024-12-20 15:57:22,926 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-1.pt
2024-12-20 15:57:28,593 (train_accelerate_ddp:558) INFO:  end of epoch 1, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-1.pt 
2024-12-20 15:58:01,272 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 2103, num_updates: 2000, {'loss': 0.3782263696193695, 'DER': 0.7630824372759857, 'ACC': 0.8041269345005472, 'MI': 0.5252688172043011, 'FA': 0.10268817204301076, 'CF': 0.13512544802867382}, batch size: 64, grad_norm: 0.7164972424507141, grad_scale: , lr: 9.94e-06, 
2024-12-20 15:58:01,273 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 2103, num_updates: 2000, {'loss': 0.4039475917816162, 'DER': 0.7662313432835821, 'ACC': 0.809921875, 'MI': 0.5393656716417911, 'FA': 0.08526119402985075, 'CF': 0.1416044776119403}, batch size: 64, grad_norm: 0.7164972424507141, grad_scale: , lr: 9.94e-06, 
2024-12-20 16:18:40,985 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 2603, num_updates: 2500, {'loss': 0.3683195114135742, 'DER': 0.7192630426851514, 'ACC': 0.82296875, 'MI': 0.517694272163444, 'FA': 0.09412623130244437, 'CF': 0.10744253921926304}, batch size: 64, grad_norm: 0.6823655962944031, grad_scale: , lr: 9.66e-06, 
2024-12-20 16:18:40,985 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 2603, num_updates: 2500, {'loss': 0.35963770747184753, 'DER': 0.7027372262773722, 'ACC': 0.823046875, 'MI': 0.4642335766423358, 'FA': 0.1145985401459854, 'CF': 0.12390510948905109}, batch size: 64, grad_norm: 0.6823655962944031, grad_scale: , lr: 9.66e-06, 
2024-12-20 16:18:40,986 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 16:18:40,986 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 16:20:20,452 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 2603,  validation: loss=0.2266, DER=0.4382, ACC=0.8764, MI=0.09773, FA=0.1925, CF=0.1479, over 0.00 frames. 
2024-12-20 16:20:20,453 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 16:20:21,155 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 2603,  validation: loss=0.2304, DER=0.4468, ACC=0.8742, MI=0.0961, FA=0.2057, CF=0.145, over 0.00 frames. 
2024-12-20 16:20:21,156 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 16:37:06,647 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-3000.pt
2024-12-20 16:37:14,178 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 16:41:37,124 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 3103, num_updates: 3000, {'loss': 0.3441193401813507, 'DER': 0.6920995670995671, 'ACC': 0.8262109375, 'MI': 0.43867243867243866, 'FA': 0.14303751803751805, 'CF': 0.11038961038961038}, batch size: 64, grad_norm: 0.5609630942344666, grad_scale: , lr: 9.39e-06, 
2024-12-20 16:41:37,124 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 16:41:37,124 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 3103, num_updates: 3000, {'loss': 0.30973944067955017, 'DER': 0.6347858197932054, 'ACC': 0.840625, 'MI': 0.4423929098966027, 'FA': 0.07385524372230429, 'CF': 0.11853766617429838}, batch size: 64, grad_norm: 0.5609630942344666, grad_scale: , lr: 9.39e-06, 
2024-12-20 16:41:37,125 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 16:43:16,893 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 3103,  validation: loss=0.2126, DER=0.4758, ACC=0.8844, MI=0.06672, FA=0.3387, CF=0.07036, over 0.00 frames. 
2024-12-20 16:43:16,894 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 16:43:17,763 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 3103,  validation: loss=0.2144, DER=0.4805, ACC=0.8836, MI=0.06638, FA=0.347, CF=0.06715, over 0.00 frames. 
2024-12-20 16:43:17,763 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 17:03:54,975 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 3603, num_updates: 3500, {'loss': 0.27333328127861023, 'DER': 0.5422522016113922, 'ACC': 0.865625, 'MI': 0.33895446880269814, 'FA': 0.10099306726625445, 'CF': 0.10230466554243957}, batch size: 64, grad_norm: 0.7680783271789551, grad_scale: , lr: 9.11e-06, 
2024-12-20 17:03:54,976 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 17:03:54,980 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 3603, num_updates: 3500, {'loss': 0.31126561760902405, 'DER': 0.6093352706138758, 'ACC': 0.8411328125, 'MI': 0.37577188521612787, 'FA': 0.10424990918997458, 'CF': 0.12931347620777334}, batch size: 64, grad_norm: 0.7680783271789551, grad_scale: , lr: 9.11e-06, 
2024-12-20 17:03:54,980 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 17:05:35,472 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 3603,  validation: loss=0.2121, DER=0.3607, ACC=0.8913, MI=0.07458, FA=0.1323, CF=0.1539, over 0.00 frames. 
2024-12-20 17:05:35,473 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 17:05:35,558 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 3603,  validation: loss=0.2056, DER=0.3398, ACC=0.8987, MI=0.07333, FA=0.128, CF=0.1385, over 0.00 frames. 
2024-12-20 17:05:35,558 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 11329MB
2024-12-20 17:26:26,115 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 4103, num_updates: 4000, {'loss': 0.29614686965942383, 'DER': 0.5441415417945478, 'ACC': 0.8623046875, 'MI': 0.3859902509478245, 'FA': 0.06589637118613469, 'CF': 0.09225491966058856}, batch size: 64, grad_norm: 0.8807435631752014, grad_scale: , lr: 8.83e-06, 
2024-12-20 17:26:26,116 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 17:26:26,119 (train_accelerate_ddp:699) INFO: [Train] - Epoch 2, batch_idx_train: 4103, num_updates: 4000, {'loss': 0.3125368654727936, 'DER': 0.5689872279186904, 'ACC': 0.8465625, 'MI': 0.335671883432272, 'FA': 0.09570066558733585, 'CF': 0.13761467889908258}, batch size: 64, grad_norm: 0.8807435631752014, grad_scale: , lr: 8.83e-06, 
2024-12-20 17:26:26,120 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 17:28:05,580 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 4103,  validation: loss=0.1894, DER=0.3543, ACC=0.9039, MI=0.05269, FA=0.2025, CF=0.09907, over 0.00 frames. 
2024-12-20 17:28:05,581 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 17:28:06,040 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 2, batch_idx_train: 4103,  validation: loss=0.1879, DER=0.3518, ACC=0.9052, MI=0.05204, FA=0.2065, CF=0.09317, over 0.00 frames. 
2024-12-20 17:28:06,041 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 17:32:36,040 (train_accelerate_ddp:710) INFO: end of epoch 2, batch_idx: 2102 batch_idx_train: 4205, {'loss': 0.2638901472091675, 'DER': 0.49298711144806673, 'ACC': 0.8776953125, 'MI': 0.28525398028809706, 'FA': 0.10727824109173617, 'CF': 0.10045489006823351}, batch size: 64, grad_norm: 1.068697452545166, grad_scale: , lr: 8.77e-06, 
2024-12-20 17:32:36,043 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-2.pt
2024-12-20 17:32:36,046 (train_accelerate_ddp:710) INFO: end of epoch 2, batch_idx: 2102 batch_idx_train: 4205, {'loss': 0.3325917720794678, 'DER': 0.6137585094948047, 'ACC': 0.8327340941066125, 'MI': 0.3498745969186671, 'FA': 0.11089215335005374, 'CF': 0.15299175922608385}, batch size: 64, grad_norm: 1.068697452545166, grad_scale: , lr: 8.77e-06, 
2024-12-20 17:32:44,622 (train_accelerate_ddp:558) INFO:  end of epoch 2, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-2.pt 
2024-12-20 17:33:08,739 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 4206, num_updates: 4000, {'loss': 0.3233005404472351, 'DER': 0.5732277797768982, 'ACC': 0.8475, 'MI': 0.3907880532565671, 'FA': 0.053256567110471394, 'CF': 0.12918315940985967}, batch size: 64, grad_norm: 1.0117038488388062, grad_scale: , lr: 8.77e-06, 
2024-12-20 17:33:08,745 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 4206, num_updates: 4000, {'loss': 0.2775702476501465, 'DER': 0.5295494441193681, 'ACC': 0.87078125, 'MI': 0.3099278330407646, 'FA': 0.10395943046615955, 'CF': 0.11566218061244392}, batch size: 64, grad_norm: 1.0117038488388062, grad_scale: , lr: 8.77e-06, 
2024-12-20 17:45:47,783 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-4500.pt
2024-12-20 17:45:56,097 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 17:45:56,195 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 17:55:03,879 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 4706, num_updates: 4500, {'loss': 0.3030789792537689, 'DER': 0.5176873335869152, 'ACC': 0.8723828125, 'MI': 0.3202738683910232, 'FA': 0.09376188664891594, 'CF': 0.10365157854697604}, batch size: 64, grad_norm: 1.0338728427886963, grad_scale: , lr: 8.50e-06, 
2024-12-20 17:55:03,880 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 17:55:03,884 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 4706, num_updates: 4500, {'loss': 0.26086679100990295, 'DER': 0.4433979351566745, 'ACC': 0.879453125, 'MI': 0.2599166817605506, 'FA': 0.06792247781199058, 'CF': 0.11555877558413331}, batch size: 64, grad_norm: 1.0338728427886963, grad_scale: , lr: 8.50e-06, 
2024-12-20 17:55:03,884 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 17:56:43,187 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 4706,  validation: loss=0.1835, DER=0.3124, ACC=0.9057, MI=0.04357, FA=0.1366, CF=0.1322, over 0.00 frames. 
2024-12-20 17:56:43,187 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 17:56:43,712 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 4706,  validation: loss=0.1827, DER=0.3109, ACC=0.9065, MI=0.04283, FA=0.1393, CF=0.1287, over 0.00 frames. 
2024-12-20 17:56:43,712 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 18:18:47,712 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 5206, num_updates: 5000, {'loss': 0.2759470045566559, 'DER': 0.4973610133708656, 'ACC': 0.860703125, 'MI': 0.3279380717804363, 'FA': 0.03940886699507389, 'CF': 0.13001407459535538}, batch size: 64, grad_norm: 1.0641014575958252, grad_scale: , lr: 8.22e-06, 
2024-12-20 18:18:47,713 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 18:18:47,714 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 5206, num_updates: 5000, {'loss': 0.2408950924873352, 'DER': 0.4140357207288472, 'ACC': 0.8875, 'MI': 0.24481327800829875, 'FA': 0.06368392567201876, 'CF': 0.10553851704852968}, batch size: 64, grad_norm: 1.0641014575958252, grad_scale: , lr: 8.22e-06, 
2024-12-20 18:18:47,714 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 18:20:26,404 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 5206,  validation: loss=0.1721, DER=0.2975, ACC=0.9141, MI=0.04108, FA=0.1473, CF=0.1091, over 0.00 frames. 
2024-12-20 18:20:26,404 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 18:20:27,535 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 5206,  validation: loss=0.1727, DER=0.2991, ACC=0.9138, MI=0.04114, FA=0.152, CF=0.106, over 0.00 frames. 
2024-12-20 18:20:27,535 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 18:42:33,538 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 5706, num_updates: 5500, {'loss': 0.20943334698677063, 'DER': 0.3642762031576945, 'ACC': 0.901328125, 'MI': 0.16283051169868745, 'FA': 0.085219707057257, 'CF': 0.11622598440175005}, batch size: 64, grad_norm: 1.2391802072525024, grad_scale: , lr: 7.94e-06, 
2024-12-20 18:42:33,539 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 18:42:33,543 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 5706, num_updates: 5500, {'loss': 0.22166326642036438, 'DER': 0.37919463087248323, 'ACC': 0.8971484375, 'MI': 0.19164802386278895, 'FA': 0.07587621178225205, 'CF': 0.11167039522744221}, batch size: 64, grad_norm: 1.2391802072525024, grad_scale: , lr: 7.94e-06, 
2024-12-20 18:42:33,544 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 18:44:13,099 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 5706,  validation: loss=0.1715, DER=0.283, ACC=0.9173, MI=0.03952, FA=0.1363, CF=0.1072, over 0.00 frames. 
2024-12-20 18:44:13,100 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 18:44:13,495 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 5706,  validation: loss=0.1713, DER=0.2856, ACC=0.9167, MI=0.03887, FA=0.1416, CF=0.1051, over 0.00 frames. 
2024-12-20 18:44:13,496 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 18:56:42,641 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-6000.pt
2024-12-20 18:56:50,589 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 19:05:33,394 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 6206, num_updates: 6000, {'loss': 0.2349841594696045, 'DER': 0.37719298245614036, 'ACC': 0.89140625, 'MI': 0.17306286549707603, 'FA': 0.07328216374269006, 'CF': 0.13084795321637427}, batch size: 64, grad_norm: 1.3306225538253784, grad_scale: , lr: 7.66e-06, 
2024-12-20 19:05:33,395 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 19:05:33,402 (train_accelerate_ddp:699) INFO: [Train] - Epoch 3, batch_idx_train: 6206, num_updates: 6000, {'loss': 0.23936991393566132, 'DER': 0.4192598048241576, 'ACC': 0.8849851492887291, 'MI': 0.22500460320382987, 'FA': 0.07162585159270853, 'CF': 0.12262935002761922}, batch size: 64, grad_norm: 1.3306225538253784, grad_scale: , lr: 7.66e-06, 
2024-12-20 19:05:33,403 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 19:07:12,151 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 6206,  validation: loss=0.1566, DER=0.2709, ACC=0.9227, MI=0.03523, FA=0.1422, CF=0.09344, over 0.00 frames. 
2024-12-20 19:07:12,152 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 19:07:13,247 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 3, batch_idx_train: 6206,  validation: loss=0.1561, DER=0.271, ACC=0.9232, MI=0.0346, FA=0.1464, CF=0.09007, over 0.00 frames. 
2024-12-20 19:07:13,247 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 19:11:32,621 (train_accelerate_ddp:710) INFO: end of epoch 3, batch_idx: 2102 batch_idx_train: 6308, {'loss': 0.2434277981519699, 'DER': 0.37070040990910713, 'ACC': 0.8925, 'MI': 0.20531099625735164, 'FA': 0.04562466583496703, 'CF': 0.11976474781678845}, batch size: 64, grad_norm: 1.2330865859985352, grad_scale: , lr: 7.61e-06, 
2024-12-20 19:11:32,625 (train_accelerate_ddp:710) INFO: end of epoch 3, batch_idx: 2102 batch_idx_train: 6308, {'loss': 0.22640065848827362, 'DER': 0.3933191735856343, 'ACC': 0.9013671875, 'MI': 0.21027225333075883, 'FA': 0.08882023556671172, 'CF': 0.09422668468816374}, batch size: 64, grad_norm: 1.2330865859985352, grad_scale: , lr: 7.61e-06, 
2024-12-20 19:11:32,629 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-3.pt
2024-12-20 19:11:49,138 (train_accelerate_ddp:558) INFO:  end of epoch 3, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-3.pt 
2024-12-20 19:12:09,429 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 6309, num_updates: 6000, {'loss': 0.2591753900051117, 'DER': 0.4139200883815135, 'ACC': 0.8818694222639737, 'MI': 0.2040139937396428, 'FA': 0.06812741668201068, 'CF': 0.14177867795986007}, batch size: 64, grad_norm: 1.1835544109344482, grad_scale: , lr: 7.61e-06, 
2024-12-20 19:12:09,432 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 6309, num_updates: 6000, {'loss': 0.20534271001815796, 'DER': 0.34288372093023256, 'ACC': 0.9109765625, 'MI': 0.20446511627906977, 'FA': 0.05730232558139535, 'CF': 0.08111627906976744}, batch size: 64, grad_norm: 1.1835544109344482, grad_scale: , lr: 7.61e-06, 
2024-12-20 19:33:43,399 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 6809, num_updates: 6500, {'loss': 0.2589631676673889, 'DER': 0.457473481195757, 'ACC': 0.8694921875, 'MI': 0.18534233365477337, 'FA': 0.08524590163934426, 'CF': 0.18688524590163935}, batch size: 64, grad_norm: 1.5265624523162842, grad_scale: , lr: 7.33e-06, 
2024-12-20 19:33:43,400 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 19:33:43,401 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 6809, num_updates: 6500, {'loss': 0.23806867003440857, 'DER': 0.40540033752109506, 'ACC': 0.8853125, 'MI': 0.1890118132383274, 'FA': 0.0712544534033377, 'CF': 0.14513407087942998}, batch size: 64, grad_norm: 1.5265624523162842, grad_scale: , lr: 7.33e-06, 
2024-12-20 19:33:43,401 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 19:35:22,812 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 6809,  validation: loss=0.151, DER=0.2506, ACC=0.9254, MI=0.04049, FA=0.109, CF=0.1011, over 0.00 frames. 
2024-12-20 19:35:22,813 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 19:35:23,944 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 6809,  validation: loss=0.1505, DER=0.2498, ACC=0.9256, MI=0.03981, FA=0.1128, CF=0.09716, over 0.00 frames. 
2024-12-20 19:35:23,945 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 19:56:49,923 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 7309, num_updates: 7000, {'loss': 0.2074781358242035, 'DER': 0.34429906542056077, 'ACC': 0.90359375, 'MI': 0.15308411214953271, 'FA': 0.07420560747663552, 'CF': 0.11700934579439252}, batch size: 64, grad_norm: 1.4118034839630127, grad_scale: , lr: 7.05e-06, 
2024-12-20 19:56:49,924 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 19:56:49,927 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 7309, num_updates: 7000, {'loss': 0.2595638036727905, 'DER': 0.4694870834893298, 'ACC': 0.8796875, 'MI': 0.2811681018345189, 'FA': 0.08124298015724447, 'CF': 0.10707600149756645}, batch size: 64, grad_norm: 1.4118034839630127, grad_scale: , lr: 7.05e-06, 
2024-12-20 19:56:49,928 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 19:58:29,238 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 7309,  validation: loss=0.142, DER=0.2283, ACC=0.9328, MI=0.04553, FA=0.09287, CF=0.08992, over 0.00 frames. 
2024-12-20 19:58:29,239 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 19:58:29,695 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 7309,  validation: loss=0.1424, DER=0.2294, ACC=0.9324, MI=0.04506, FA=0.09599, CF=0.08838, over 0.00 frames. 
2024-12-20 19:58:29,696 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 20:07:04,633 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-7500.pt
2024-12-20 20:07:16,851 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 20:07:17,010 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 20:27:04,717 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 7809, num_updates: 7500, {'loss': 0.21643446385860443, 'DER': 0.3505800036825631, 'ACC': 0.9024609375, 'MI': 0.17050266985822132, 'FA': 0.07088933897993004, 'CF': 0.1091879948444117}, batch size: 64, grad_norm: 1.3594090938568115, grad_scale: , lr: 6.77e-06, 
2024-12-20 20:27:04,723 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 7809, num_updates: 7500, {'loss': 0.18478885293006897, 'DER': 0.28293545534924847, 'ACC': 0.918125, 'MI': 0.14801061007957558, 'FA': 0.047214854111405836, 'CF': 0.08770999115826703}, batch size: 64, grad_norm: 1.3594090938568115, grad_scale: , lr: 6.77e-06, 
2024-12-20 20:27:04,726 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 20:27:04,726 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 20:28:47,226 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 7809,  validation: loss=0.1398, DER=0.2292, ACC=0.9331, MI=0.04393, FA=0.1011, CF=0.08409, over 0.00 frames. 
2024-12-20 20:28:47,280 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 20:28:47,328 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 7809,  validation: loss=0.1394, DER=0.2295, ACC=0.9327, MI=0.04448, FA=0.09755, CF=0.08746, over 0.00 frames. 
2024-12-20 20:28:47,328 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 20:55:45,919 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 8309, num_updates: 8000, {'loss': 0.16476275026798248, 'DER': 0.28442437923250563, 'ACC': 0.9300390625, 'MI': 0.13449962377727614, 'FA': 0.09744168547780285, 'CF': 0.05248306997742664}, batch size: 64, grad_norm: 0.9236515760421753, grad_scale: , lr: 6.49e-06, 
2024-12-20 20:55:45,919 (train_accelerate_ddp:699) INFO: [Train] - Epoch 4, batch_idx_train: 8309, num_updates: 8000, {'loss': 0.174038365483284, 'DER': 0.29503443276549474, 'ACC': 0.9188671875, 'MI': 0.1377310619789779, 'FA': 0.07593330916998912, 'CF': 0.08137006161652773}, batch size: 64, grad_norm: 0.9236515760421753, grad_scale: , lr: 6.49e-06, 
2024-12-20 20:55:45,919 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 20:55:45,920 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 20:57:25,268 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 8309,  validation: loss=0.1409, DER=0.2221, ACC=0.93, MI=0.04595, FA=0.07139, CF=0.1047, over 0.00 frames. 
2024-12-20 20:57:25,360 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 4, batch_idx_train: 8309,  validation: loss=0.1413, DER=0.2222, ACC=0.9303, MI=0.04619, FA=0.06987, CF=0.1062, over 0.00 frames. 
2024-12-20 20:57:25,427 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 20:57:25,427 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 21:07:25,382 (train_accelerate_ddp:710) INFO: end of epoch 4, batch_idx: 2102 batch_idx_train: 8411, {'loss': 0.21203845739364624, 'DER': 0.35362153962836557, 'ACC': 0.9009765625, 'MI': 0.1632536973833902, 'FA': 0.06332954114524081, 'CF': 0.12703830109973455}, batch size: 64, grad_norm: 1.2597860097885132, grad_scale: , lr: 6.44e-06, 
2024-12-20 21:07:25,383 (train_accelerate_ddp:710) INFO: end of epoch 4, batch_idx: 2102 batch_idx_train: 8411, {'loss': 0.18536092340946198, 'DER': 0.289726270885176, 'ACC': 0.9141796875, 'MI': 0.14361891219338785, 'FA': 0.045325275506576605, 'CF': 0.10078208318521152}, batch size: 64, grad_norm: 1.2597860097885132, grad_scale: , lr: 6.44e-06, 
2024-12-20 21:07:25,385 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-4.pt
2024-12-20 21:07:34,678 (train_accelerate_ddp:558) INFO:  end of epoch 4, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-4.pt 
2024-12-20 21:08:23,280 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 8412, num_updates: 8000, {'loss': 0.2042204737663269, 'DER': 0.33240792152507864, 'ACC': 0.9032852438450683, 'MI': 0.1569498426799926, 'FA': 0.051267814177308905, 'CF': 0.12419026466777716}, batch size: 64, grad_norm: 1.294417142868042, grad_scale: , lr: 6.44e-06, 
2024-12-20 21:08:23,285 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 8412, num_updates: 8000, {'loss': 0.19439305365085602, 'DER': 0.3039614957423177, 'ACC': 0.9212890625, 'MI': 0.1567937800814513, 'FA': 0.07811921510551648, 'CF': 0.06904850055534988}, batch size: 64, grad_norm: 1.294417142868042, grad_scale: , lr: 6.44e-06, 
2024-12-20 21:38:58,879 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 8912, num_updates: 8500, {'loss': 0.16040660440921783, 'DER': 0.2526581366011894, 'ACC': 0.9338671875, 'MI': 0.14218778158226708, 'FA': 0.058028473598846636, 'CF': 0.05244188142007569}, batch size: 64, grad_norm: 1.0305650234222412, grad_scale: , lr: 6.16e-06, 
2024-12-20 21:38:58,880 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 21:38:58,888 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 8912, num_updates: 8500, {'loss': 0.21258191764354706, 'DER': 0.3421989136542424, 'ACC': 0.903046875, 'MI': 0.16482487357182993, 'FA': 0.05469188986701629, 'CF': 0.12268215021539614}, batch size: 64, grad_norm: 1.0305650234222412, grad_scale: , lr: 6.16e-06, 
2024-12-20 21:38:58,889 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 21:40:42,692 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 8912,  validation: loss=0.1311, DER=0.2035, ACC=0.9401, MI=0.04448, FA=0.08088, CF=0.07819, over 0.00 frames. 
2024-12-20 21:40:42,730 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 21:40:43,183 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 8912,  validation: loss=0.1311, DER=0.2065, ACC=0.9392, MI=0.04489, FA=0.07952, CF=0.08209, over 0.00 frames. 
2024-12-20 21:40:43,183 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 21:50:13,067 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-9000.pt
2024-12-20 21:50:31,009 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 21:50:31,185 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 22:41:28,318 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 9412, num_updates: 9000, {'loss': 0.1773163229227066, 'DER': 0.335759433058801, 'ACC': 0.9176567140847273, 'MI': 0.18329821873204366, 'FA': 0.08465811147289791, 'CF': 0.06780310285385942}, batch size: 64, grad_norm: 1.1323219537734985, grad_scale: , lr: 5.88e-06, 
2024-12-20 22:41:28,491 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 22:41:28,329 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 9412, num_updates: 9000, {'loss': 0.16620424389839172, 'DER': 0.26940026825062274, 'ACC': 0.93015625, 'MI': 0.11419812224564092, 'FA': 0.08200804751868174, 'CF': 0.07319409848630005}, batch size: 64, grad_norm: 1.1323219537734985, grad_scale: , lr: 5.88e-06, 
2024-12-20 22:41:28,491 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-20 22:43:15,056 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 9412,  validation: loss=0.1274, DER=0.1954, ACC=0.9422, MI=0.04649, FA=0.07272, CF=0.07615, over 0.00 frames. 
2024-12-20 22:43:15,058 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-20 22:43:15,340 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 9412,  validation: loss=0.1267, DER=0.1985, ACC=0.9414, MI=0.04669, FA=0.07271, CF=0.07913, over 0.00 frames. 
2024-12-20 22:43:15,341 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 00:14:53,538 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 9912, num_updates: 9500, {'loss': 0.16691942512989044, 'DER': 0.2624609159462939, 'ACC': 0.9300390625, 'MI': 0.1289313959904359, 'FA': 0.06658083501931213, 'CF': 0.06694868493654589}, batch size: 64, grad_norm: 1.0537103414535522, grad_scale: , lr: 5.60e-06, 
2024-12-21 00:14:53,542 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 9912, num_updates: 9500, {'loss': 0.20882216095924377, 'DER': 0.3490494296577947, 'ACC': 0.9052734375, 'MI': 0.15874524714828897, 'FA': 0.07832699619771863, 'CF': 0.11197718631178707}, batch size: 64, grad_norm: 1.0537103414535522, grad_scale: , lr: 5.60e-06, 
2024-12-21 00:14:53,544 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 00:14:53,544 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 00:16:39,115 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 9912,  validation: loss=0.1287, DER=0.1981, ACC=0.9412, MI=0.04293, FA=0.07696, CF=0.07823, over 0.00 frames. 
2024-12-21 00:16:39,126 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 00:16:39,578 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 9912,  validation: loss=0.1282, DER=0.2015, ACC=0.9402, MI=0.04318, FA=0.07701, CF=0.08128, over 0.00 frames. 
2024-12-21 00:16:39,579 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 01:23:01,628 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 10412, num_updates: 10000, {'loss': 0.2280469536781311, 'DER': 0.3466058586212945, 'ACC': 0.8994921875, 'MI': 0.19487809156288372, 'FA': 0.04700929661462901, 'CF': 0.1047184704437818}, batch size: 64, grad_norm: 1.1557857990264893, grad_scale: , lr: 5.33e-06, 
2024-12-21 01:23:01,629 (train_accelerate_ddp:699) INFO: [Train] - Epoch 5, batch_idx_train: 10412, num_updates: 10000, {'loss': 0.19345247745513916, 'DER': 0.3157695939565628, 'ACC': 0.9168359375, 'MI': 0.15864022662889518, 'FA': 0.0708215297450425, 'CF': 0.08630783758262511}, batch size: 64, grad_norm: 1.1557857990264893, grad_scale: , lr: 5.33e-06, 
2024-12-21 01:23:01,653 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 01:23:01,653 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 01:24:42,056 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 10412,  validation: loss=0.125, DER=0.1962, ACC=0.9427, MI=0.04059, FA=0.08199, CF=0.07365, over 0.00 frames. 
2024-12-21 01:24:42,100 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 01:24:42,363 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 5, batch_idx_train: 10412,  validation: loss=0.1245, DER=0.1972, ACC=0.9427, MI=0.04053, FA=0.08119, CF=0.07546, over 0.00 frames. 
2024-12-21 01:24:42,363 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 01:34:16,251 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-10500.pt
2024-12-21 01:34:30,402 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-21 01:36:11,974 (train_accelerate_ddp:710) INFO: end of epoch 5, batch_idx: 2102 batch_idx_train: 10514, {'loss': 0.1909651756286621, 'DER': 0.31323529411764706, 'ACC': 0.9106946840206994, 'MI': 0.16158088235294119, 'FA': 0.046139705882352944, 'CF': 0.10551470588235294}, batch size: 64, grad_norm: 1.4315783977508545, grad_scale: , lr: 5.27e-06, 
2024-12-21 01:36:11,975 (train_accelerate_ddp:710) INFO: end of epoch 5, batch_idx: 2102 batch_idx_train: 10514, {'loss': 0.1838114857673645, 'DER': 0.3213877239801754, 'ACC': 0.9184375, 'MI': 0.1484940907357987, 'FA': 0.096263820053374, 'CF': 0.07662981319100266}, batch size: 64, grad_norm: 1.4315783977508545, grad_scale: , lr: 5.27e-06, 
2024-12-21 01:36:12,022 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-5.pt
2024-12-21 01:36:19,653 (train_accelerate_ddp:558) INFO:  end of epoch 5, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-5.pt 
2024-12-21 01:36:53,159 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 10515, num_updates: 10000, {'loss': 0.1791599988937378, 'DER': 0.2935259692757864, 'ACC': 0.9224609375, 'MI': 0.1389904901243599, 'FA': 0.08504023408924652, 'CF': 0.06949524506217995}, batch size: 64, grad_norm: 1.0799250602722168, grad_scale: , lr: 5.27e-06, 
2024-12-21 01:36:53,163 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 10515, num_updates: 10000, {'loss': 0.2143854945898056, 'DER': 0.35870755750273825, 'ACC': 0.89828125, 'MI': 0.17123037604965316, 'FA': 0.07082876962395035, 'CF': 0.11664841182913473}, batch size: 64, grad_norm: 1.0799250602722168, grad_scale: , lr: 5.27e-06, 
2024-12-21 02:38:15,182 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 11015, num_updates: 10500, {'loss': 0.16933074593544006, 'DER': 0.25452646239554316, 'ACC': 0.92375, 'MI': 0.13196378830083566, 'FA': 0.03725626740947075, 'CF': 0.08530640668523677}, batch size: 64, grad_norm: 1.2196009159088135, grad_scale: , lr: 4.99e-06, 
2024-12-21 02:38:15,186 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 11015, num_updates: 10500, {'loss': 0.1769385188817978, 'DER': 0.25618374558303886, 'ACC': 0.92421875, 'MI': 0.13374558303886927, 'FA': 0.03586572438162544, 'CF': 0.08657243816254417}, batch size: 64, grad_norm: 1.2196009159088135, grad_scale: , lr: 4.99e-06, 
2024-12-21 02:38:15,199 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 02:38:15,199 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 02:39:56,574 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 11015,  validation: loss=0.1231, DER=0.1868, ACC=0.9449, MI=0.04555, FA=0.06734, CF=0.07394, over 0.00 frames. 
2024-12-21 02:39:56,613 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 02:39:56,680 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 11015,  validation: loss=0.1227, DER=0.1907, ACC=0.9437, MI=0.04582, FA=0.06806, CF=0.07682, over 0.00 frames. 
2024-12-21 02:39:56,681 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 03:29:07,161 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 11515, num_updates: 11000, {'loss': 0.190297469496727, 'DER': 0.29730700179533215, 'ACC': 0.9189453125, 'MI': 0.15206463195691203, 'FA': 0.07001795332136446, 'CF': 0.07522441651705565}, batch size: 64, grad_norm: 1.0941270589828491, grad_scale: , lr: 4.71e-06, 
2024-12-21 03:29:07,162 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 03:29:07,166 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 11515, num_updates: 11000, {'loss': 0.1609918624162674, 'DER': 0.268675799086758, 'ACC': 0.93171875, 'MI': 0.15762557077625572, 'FA': 0.06045662100456621, 'CF': 0.050593607305936074}, batch size: 64, grad_norm: 1.0941270589828491, grad_scale: , lr: 4.71e-06, 
2024-12-21 03:29:07,167 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 03:30:49,433 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 11515,  validation: loss=0.1173, DER=0.1794, ACC=0.947, MI=0.03707, FA=0.07157, CF=0.07076, over 0.00 frames. 
2024-12-21 03:30:49,433 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 03:30:50,088 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 11515,  validation: loss=0.1184, DER=0.1851, ACC=0.945, MI=0.0373, FA=0.07077, CF=0.07707, over 0.00 frames. 
2024-12-21 03:30:50,089 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 04:22:48,951 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-12000.pt
2024-12-21 04:23:02,382 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-21 04:23:02,569 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-21 04:24:33,407 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 12015, num_updates: 11500, {'loss': 0.1754312664270401, 'DER': 0.2716337522441652, 'ACC': 0.9208203125, 'MI': 0.11454219030520646, 'FA': 0.06481149012567325, 'CF': 0.09228007181328546}, batch size: 64, grad_norm: 1.1190159320831299, grad_scale: , lr: 4.44e-06, 
2024-12-21 04:24:33,415 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 12015, num_updates: 11500, {'loss': 0.15028659999370575, 'DER': 0.24482316291002382, 'ACC': 0.9331640625, 'MI': 0.11434854315557999, 'FA': 0.06175554333883086, 'CF': 0.06871907641561298}, batch size: 64, grad_norm: 1.1190159320831299, grad_scale: , lr: 4.44e-06, 
2024-12-21 04:24:33,417 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 04:24:33,417 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 04:26:13,671 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 12015,  validation: loss=0.1197, DER=0.18, ACC=0.9458, MI=0.03941, FA=0.06513, CF=0.07549, over 0.00 frames. 
2024-12-21 04:26:13,787 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 12015,  validation: loss=0.1196, DER=0.184, ACC=0.9448, MI=0.03968, FA=0.0656, CF=0.07874, over 0.00 frames. 
2024-12-21 04:26:14,028 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 04:26:14,028 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 05:17:30,552 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 12515, num_updates: 12000, {'loss': 0.17825832962989807, 'DER': 0.27870659562741135, 'ACC': 0.9214453125, 'MI': 0.13632188131545103, 'FA': 0.05162594157633658, 'CF': 0.09075877273562373}, batch size: 64, grad_norm: 1.0555052757263184, grad_scale: , lr: 4.16e-06, 
2024-12-21 05:17:30,553 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 05:17:30,555 (train_accelerate_ddp:699) INFO: [Train] - Epoch 6, batch_idx_train: 12515, num_updates: 12000, {'loss': 0.16636815667152405, 'DER': 0.2791039294895336, 'ACC': 0.926875, 'MI': 0.1667278736687477, 'FA': 0.0477414616232097, 'CF': 0.06463459419757621}, batch size: 64, grad_norm: 1.0555052757263184, grad_scale: , lr: 4.16e-06, 
2024-12-21 05:17:30,555 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 05:19:09,938 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 12515,  validation: loss=0.118, DER=0.1746, ACC=0.947, MI=0.03708, FA=0.06184, CF=0.07566, over 0.00 frames. 
2024-12-21 05:19:09,941 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 05:19:10,168 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 6, batch_idx_train: 12515,  validation: loss=0.1191, DER=0.1805, ACC=0.9453, MI=0.03764, FA=0.06213, CF=0.08075, over 0.00 frames. 
2024-12-21 05:19:10,168 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 05:29:37,917 (train_accelerate_ddp:710) INFO: end of epoch 6, batch_idx: 2102 batch_idx_train: 12617, {'loss': 0.18066897988319397, 'DER': 0.30895056055872083, 'ACC': 0.9174609375, 'MI': 0.15585370336335233, 'FA': 0.07369968755743429, 'CF': 0.07939716963793421}, batch size: 64, grad_norm: 1.3286675214767456, grad_scale: , lr: 4.10e-06, 
2024-12-21 05:29:37,918 (train_accelerate_ddp:710) INFO: end of epoch 6, batch_idx: 2102 batch_idx_train: 12617, {'loss': 0.20976346731185913, 'DER': 0.36707250590586954, 'ACC': 0.9006640625, 'MI': 0.1946211157550427, 'FA': 0.07741232055242595, 'CF': 0.09503906959840087}, batch size: 64, grad_norm: 1.3286675214767456, grad_scale: , lr: 4.10e-06, 
2024-12-21 05:29:38,769 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-6.pt
2024-12-21 05:29:51,744 (train_accelerate_ddp:558) INFO:  end of epoch 6, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-6.pt 
2024-12-21 05:30:32,230 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 12618, num_updates: 12000, {'loss': 0.2112780660390854, 'DER': 0.3381099220025395, 'ACC': 0.905078125, 'MI': 0.18066388536187195, 'FA': 0.05477961182659169, 'CF': 0.10266642481407583}, batch size: 64, grad_norm: 1.4626972675323486, grad_scale: , lr: 4.10e-06, 
2024-12-21 05:30:32,232 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 12618, num_updates: 12000, {'loss': 0.15615515410900116, 'DER': 0.24592009003939225, 'ACC': 0.9324609375, 'MI': 0.09472894391296192, 'FA': 0.07278184205589945, 'CF': 0.07840930407053086}, batch size: 64, grad_norm: 1.4626972675323486, grad_scale: , lr: 4.10e-06, 
2024-12-21 06:16:41,068 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 13118, num_updates: 12500, {'loss': 0.16052715480327606, 'DER': 0.2637045585689556, 'ACC': 0.9307421875, 'MI': 0.10117330255818427, 'FA': 0.08520869397961146, 'CF': 0.07732256203115984}, batch size: 64, grad_norm: 1.1062378883361816, grad_scale: , lr: 3.82e-06, 
2024-12-21 06:16:41,068 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 06:16:41,074 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 13118, num_updates: 12500, {'loss': 0.18506300449371338, 'DER': 0.28851388132825256, 'ACC': 0.91484375, 'MI': 0.1353656323716204, 'FA': 0.04608963890401016, 'CF': 0.10705861005262203}, batch size: 64, grad_norm: 1.1062378883361816, grad_scale: , lr: 3.82e-06, 
2024-12-21 06:16:41,074 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 06:18:20,905 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 13118,  validation: loss=0.1163, DER=0.1687, ACC=0.949, MI=0.03942, FA=0.05787, CF=0.07137, over 0.00 frames. 
2024-12-21 06:18:20,906 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 06:18:21,223 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 13118,  validation: loss=0.1174, DER=0.1736, ACC=0.9479, MI=0.03948, FA=0.05825, CF=0.07591, over 0.00 frames. 
2024-12-21 06:18:21,223 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 06:55:16,354 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-13500.pt
2024-12-21 06:55:27,442 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-21 06:55:27,607 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-21 07:06:33,558 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 13618, num_updates: 13000, {'loss': 0.15567880868911743, 'DER': 0.20351069665386726, 'ACC': 0.9481640625, 'MI': 0.10641799232035107, 'FA': 0.05796306454562077, 'CF': 0.03912963978789541}, batch size: 64, grad_norm: 0.9344942569732666, grad_scale: , lr: 3.54e-06, 
2024-12-21 07:06:33,565 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 13618, num_updates: 13000, {'loss': 0.15854474902153015, 'DER': 0.24114225277630882, 'ACC': 0.9346875, 'MI': 0.14190022915564957, 'FA': 0.04565485633703508, 'CF': 0.053587167283624185}, batch size: 64, grad_norm: 0.9344942569732666, grad_scale: , lr: 3.54e-06, 
2024-12-21 07:06:33,573 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 07:06:33,573 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 07:08:12,597 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 13618,  validation: loss=0.1148, DER=0.1714, ACC=0.9488, MI=0.0412, FA=0.05992, CF=0.0703, over 0.00 frames. 
2024-12-21 07:08:12,616 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 07:08:12,634 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 13618,  validation: loss=0.115, DER=0.1759, ACC=0.9474, MI=0.04125, FA=0.06028, CF=0.07438, over 0.00 frames. 
2024-12-21 07:08:12,635 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 07:52:53,899 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 14118, num_updates: 13500, {'loss': 0.1958625167608261, 'DER': 0.28828992897468586, 'ACC': 0.916015625, 'MI': 0.13385539974503732, 'FA': 0.051174649426334, 'CF': 0.10325987980331451}, batch size: 64, grad_norm: 1.4567344188690186, grad_scale: , lr: 3.27e-06, 
2024-12-21 07:52:53,900 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 14118, num_updates: 13500, {'loss': 0.21777896583080292, 'DER': 0.36673573177622903, 'ACC': 0.89953125, 'MI': 0.15068751177246187, 'FA': 0.09832360143153136, 'CF': 0.11772461857223583}, batch size: 64, grad_norm: 1.4567344188690186, grad_scale: , lr: 3.27e-06, 
2024-12-21 07:52:53,902 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 07:52:53,902 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 07:54:35,719 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 14118,  validation: loss=0.1148, DER=0.1665, ACC=0.9498, MI=0.0396, FA=0.05684, CF=0.07003, over 0.00 frames. 
2024-12-21 07:54:35,730 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 07:54:35,911 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 14118,  validation: loss=0.1156, DER=0.1714, ACC=0.9484, MI=0.0399, FA=0.05604, CF=0.07546, over 0.00 frames. 
2024-12-21 07:54:35,912 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 08:38:50,482 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 14618, num_updates: 14000, {'loss': 0.18727409839630127, 'DER': 0.30470310513891413, 'ACC': 0.912421875, 'MI': 0.14254585073542764, 'FA': 0.059742146359179224, 'CF': 0.10241510804430724}, batch size: 64, grad_norm: 1.1516462564468384, grad_scale: , lr: 2.99e-06, 
2024-12-21 08:38:50,484 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 08:38:50,488 (train_accelerate_ddp:699) INFO: [Train] - Epoch 7, batch_idx_train: 14618, num_updates: 14000, {'loss': 0.15466511249542236, 'DER': 0.25533961901096786, 'ACC': 0.932890625, 'MI': 0.09948046950163555, 'FA': 0.08062343659803733, 'CF': 0.07523571291129498}, batch size: 64, grad_norm: 1.1516462564468384, grad_scale: , lr: 2.99e-06, 
2024-12-21 08:38:50,489 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 08:40:32,608 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 14618,  validation: loss=0.1159, DER=0.1703, ACC=0.9489, MI=0.03454, FA=0.06492, CF=0.07082, over 0.00 frames. 
2024-12-21 08:40:32,609 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 08:40:33,276 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 7, batch_idx_train: 14618,  validation: loss=0.1168, DER=0.1758, ACC=0.9472, MI=0.03526, FA=0.06483, CF=0.07571, over 0.00 frames. 
2024-12-21 08:40:33,276 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 08:49:38,256 (train_accelerate_ddp:710) INFO: end of epoch 7, batch_idx: 2102 batch_idx_train: 14720, {'loss': 0.1589014232158661, 'DER': 0.2581531696592158, 'ACC': 0.9326171875, 'MI': 0.14712348845731038, 'FA': 0.053133015756687434, 'CF': 0.05789666544521803}, batch size: 64, grad_norm: 1.0447587966918945, grad_scale: , lr: 2.93e-06, 
2024-12-21 08:49:38,256 (train_accelerate_ddp:710) INFO: end of epoch 7, batch_idx: 2102 batch_idx_train: 14720, {'loss': 0.16214624047279358, 'DER': 0.2571049640015157, 'ACC': 0.936640625, 'MI': 0.12788935202728305, 'FA': 0.07900719969685487, 'CF': 0.050208412277377795}, batch size: 64, grad_norm: 1.0447587966918945, grad_scale: , lr: 2.93e-06, 
2024-12-21 08:49:38,370 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-7.pt
2024-12-21 08:49:49,618 (train_accelerate_ddp:558) INFO:  end of epoch 7, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-7.pt 
2024-12-21 08:50:31,330 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 14721, num_updates: 14000, {'loss': 0.1793469935655594, 'DER': 0.2986627043090639, 'ACC': 0.91578125, 'MI': 0.13410104011887072, 'FA': 0.06277860326894502, 'CF': 0.10178306092124814}, batch size: 64, grad_norm: 1.135839819908142, grad_scale: , lr: 2.93e-06, 
2024-12-21 08:50:31,330 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 14721, num_updates: 14000, {'loss': 0.17621321976184845, 'DER': 0.2964175304600837, 'ACC': 0.917109375, 'MI': 0.1431169303509729, 'FA': 0.06382978723404255, 'CF': 0.0894708128750682}, batch size: 64, grad_norm: 1.135839819908142, grad_scale: , lr: 2.93e-06, 
2024-12-21 09:14:05,191 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-15000.pt
2024-12-21 09:14:20,789 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-21 09:14:20,969 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-21 09:33:16,447 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 15221, num_updates: 14500, {'loss': 0.16426634788513184, 'DER': 0.22515968772178852, 'ACC': 0.9353515625, 'MI': 0.127750177430802, 'FA': 0.028921220723917673, 'CF': 0.06848828956706884}, batch size: 64, grad_norm: 1.1075447797775269, grad_scale: , lr: 2.65e-06, 
2024-12-21 09:33:16,447 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 09:33:16,451 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 15221, num_updates: 14500, {'loss': 0.16351500153541565, 'DER': 0.2646131007608091, 'ACC': 0.9307421875, 'MI': 0.14158470959361663, 'FA': 0.05863796622750046, 'CF': 0.06439042493969196}, batch size: 64, grad_norm: 1.1075447797775269, grad_scale: , lr: 2.65e-06, 
2024-12-21 09:33:16,451 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 09:34:57,155 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 15221,  validation: loss=0.1116, DER=0.1623, ACC=0.9518, MI=0.04155, FA=0.05566, CF=0.06505, over 0.00 frames. 
2024-12-21 09:34:57,159 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 09:34:57,386 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 15221,  validation: loss=0.1109, DER=0.1638, ACC=0.952, MI=0.04179, FA=0.05599, CF=0.066, over 0.00 frames. 
2024-12-21 09:34:57,387 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 10:38:54,998 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 15721, num_updates: 15000, {'loss': 0.16996362805366516, 'DER': 0.26468992948833847, 'ACC': 0.9228125, 'MI': 0.10739468450551437, 'FA': 0.06472608931477129, 'CF': 0.0925691556680528}, batch size: 64, grad_norm: 1.2975411415100098, grad_scale: , lr: 2.38e-06, 
2024-12-21 10:38:55,000 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 10:38:55,005 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 15721, num_updates: 15000, {'loss': 0.2155769318342209, 'DER': 0.33779201205727205, 'ACC': 0.9073828125, 'MI': 0.15919366993217784, 'FA': 0.06970610399397137, 'CF': 0.10889223813112284}, batch size: 64, grad_norm: 1.2975411415100098, grad_scale: , lr: 2.38e-06, 
2024-12-21 10:38:55,006 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 10:40:35,403 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 15721,  validation: loss=0.1123, DER=0.1624, ACC=0.9514, MI=0.04093, FA=0.05447, CF=0.06699, over 0.00 frames. 
2024-12-21 10:40:35,404 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 10:40:35,894 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 15721,  validation: loss=0.1128, DER=0.1663, ACC=0.9506, MI=0.04137, FA=0.05451, CF=0.07042, over 0.00 frames. 
2024-12-21 10:40:35,894 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 11:53:12,790 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 16221, num_updates: 15500, {'loss': 0.19895444810390472, 'DER': 0.3192436040044494, 'ACC': 0.90859375, 'MI': 0.11364479050797183, 'FA': 0.09102706711160549, 'CF': 0.11457174638487208}, batch size: 64, grad_norm: 1.1611021757125854, grad_scale: , lr: 2.10e-06, 
2024-12-21 11:53:12,791 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 11:53:12,791 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 16221, num_updates: 15500, {'loss': 0.17585372924804688, 'DER': 0.31089395584509594, 'ACC': 0.9190625, 'MI': 0.18802026782482809, 'FA': 0.05881288454578357, 'CF': 0.06406080347448426}, batch size: 64, grad_norm: 1.1611021757125854, grad_scale: , lr: 2.10e-06, 
2024-12-21 11:53:12,791 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 11:54:53,572 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 16221,  validation: loss=0.1112, DER=0.1608, ACC=0.9527, MI=0.0374, FA=0.061, CF=0.06237, over 0.00 frames. 
2024-12-21 11:54:53,618 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 11:54:54,097 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 16221,  validation: loss=0.1108, DER=0.1612, ACC=0.9532, MI=0.03766, FA=0.06027, CF=0.06329, over 0.00 frames. 
2024-12-21 11:54:54,097 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 12:34:23,512 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-16500.pt
2024-12-21 12:34:46,406 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-21 13:09:36,847 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 16721, num_updates: 16000, {'loss': 0.18490782380104065, 'DER': 0.28182144137683757, 'ACC': 0.916353824495542, 'MI': 0.1253137325206167, 'FA': 0.054858372176407315, 'CF': 0.10164933667981356}, batch size: 64, grad_norm: 1.0953106880187988, grad_scale: , lr: 1.82e-06, 
2024-12-21 13:09:36,852 (train_accelerate_ddp:699) INFO: [Train] - Epoch 8, batch_idx_train: 16721, num_updates: 16000, {'loss': 0.1434507668018341, 'DER': 0.21010353445198143, 'ACC': 0.9409765625, 'MI': 0.11567297393787933, 'FA': 0.034808996786861834, 'CF': 0.05962156372724027}, batch size: 64, grad_norm: 1.0953106880187988, grad_scale: , lr: 1.82e-06, 
2024-12-21 13:09:36,874 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 13:09:36,874 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 13:11:23,848 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 16721,  validation: loss=0.1099, DER=0.1579, ACC=0.9529, MI=0.03879, FA=0.0547, CF=0.06443, over 0.00 frames. 
2024-12-21 13:11:23,955 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 8, batch_idx_train: 16721,  validation: loss=0.1104, DER=0.1606, ACC=0.9527, MI=0.03924, FA=0.05452, CF=0.06685, over 0.00 frames. 
2024-12-21 13:11:24,073 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 13:11:24,073 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 13:26:42,944 (train_accelerate_ddp:710) INFO: end of epoch 8, batch_idx: 2102 batch_idx_train: 16823, {'loss': 0.2094781994819641, 'DER': 0.33652802893309225, 'ACC': 0.9069140625, 'MI': 0.17106690777576852, 'FA': 0.07106690777576853, 'CF': 0.09439421338155515}, batch size: 64, grad_norm: 1.3516130447387695, grad_scale: , lr: 1.76e-06, 
2024-12-21 13:26:42,944 (train_accelerate_ddp:710) INFO: end of epoch 8, batch_idx: 2102 batch_idx_train: 16823, {'loss': 0.1872229129076004, 'DER': 0.31339622641509435, 'ACC': 0.9194921875, 'MI': 0.16339622641509435, 'FA': 0.07452830188679245, 'CF': 0.07547169811320754}, batch size: 64, grad_norm: 1.3516130447387695, grad_scale: , lr: 1.76e-06, 
2024-12-21 13:26:43,023 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-8.pt
2024-12-21 13:26:51,400 (train_accelerate_ddp:558) INFO:  end of epoch 8, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-8.pt 
2024-12-21 13:27:54,157 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 16824, num_updates: 16000, {'loss': 0.17280565202236176, 'DER': 0.28012665300800893, 'ACC': 0.9242578125, 'MI': 0.1244179549264295, 'FA': 0.07468802384056622, 'CF': 0.08102067424101322}, batch size: 64, grad_norm: 1.3286164999008179, grad_scale: , lr: 1.76e-06, 
2024-12-21 13:27:54,158 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 16824, num_updates: 16000, {'loss': 0.19129805266857147, 'DER': 0.3087994170158499, 'ACC': 0.9140625, 'MI': 0.1592275460010931, 'FA': 0.05756968482419384, 'CF': 0.09200218619056294}, batch size: 64, grad_norm: 1.3286164999008179, grad_scale: , lr: 1.76e-06, 
2024-12-21 14:54:17,950 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 17324, num_updates: 16500, {'loss': 0.1960126906633377, 'DER': 0.2968548832076513, 'ACC': 0.916328125, 'MI': 0.13408129483170866, 'FA': 0.0656612102262277, 'CF': 0.09711237814971492}, batch size: 64, grad_norm: 1.2994464635849, grad_scale: , lr: 1.49e-06, 
2024-12-21 14:54:17,950 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 17324, num_updates: 16500, {'loss': 0.1931496560573578, 'DER': 0.30054844606946984, 'ACC': 0.9139453125, 'MI': 0.13985374771480805, 'FA': 0.05850091407678245, 'CF': 0.10219378427787934}, batch size: 64, grad_norm: 1.2994464635849, grad_scale: , lr: 1.49e-06, 
2024-12-21 14:54:17,954 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 14:54:17,954 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 14:56:05,564 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 17324,  validation: loss=0.1102, DER=0.1605, ACC=0.9523, MI=0.0349, FA=0.06087, CF=0.06469, over 0.00 frames. 
2024-12-21 14:56:05,624 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 14:56:05,777 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 17324,  validation: loss=0.1107, DER=0.1648, ACC=0.9514, MI=0.03537, FA=0.06101, CF=0.06842, over 0.00 frames. 
2024-12-21 14:56:05,778 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 16:10:53,138 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 17824, num_updates: 17000, {'loss': 0.15212498605251312, 'DER': 0.24484674672880444, 'ACC': 0.93578125, 'MI': 0.15235705323534685, 'FA': 0.04265997490589712, 'CF': 0.049829718587560495}, batch size: 64, grad_norm: 1.0990303754806519, grad_scale: , lr: 1.21e-06, 
2024-12-21 16:10:53,138 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 17824, num_updates: 17000, {'loss': 0.15981169044971466, 'DER': 0.25034891835310535, 'ACC': 0.9291796875, 'MI': 0.14166085136078158, 'FA': 0.04274249825540823, 'CF': 0.06594556873691557}, batch size: 64, grad_norm: 1.0990303754806519, grad_scale: , lr: 1.21e-06, 
2024-12-21 16:10:53,237 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 16:10:53,237 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 16:12:33,294 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 17824,  validation: loss=0.11, DER=0.1591, ACC=0.9529, MI=0.03798, FA=0.05796, CF=0.06319, over 0.00 frames. 
2024-12-21 16:12:33,308 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 16:12:33,680 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 17824,  validation: loss=0.11, DER=0.1623, ACC=0.9525, MI=0.03868, FA=0.05797, CF=0.06562, over 0.00 frames. 
2024-12-21 16:12:33,680 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 16:37:04,606 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-18000.pt
2024-12-21 16:37:24,061 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-21 16:37:24,206 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-21 17:27:01,246 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 18324, num_updates: 17500, {'loss': 0.13308878242969513, 'DER': 0.21069868995633187, 'ACC': 0.9415234375, 'MI': 0.08478893740902474, 'FA': 0.06422852983988356, 'CF': 0.06168122270742358}, batch size: 64, grad_norm: 1.1476669311523438, grad_scale: , lr: 9.31e-07, 
2024-12-21 17:27:01,253 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 18324, num_updates: 17500, {'loss': 0.19156238436698914, 'DER': 0.2894156560088203, 'ACC': 0.9196484375, 'MI': 0.13579566335905918, 'FA': 0.06504961411245866, 'CF': 0.08857037853730246}, batch size: 64, grad_norm: 1.1476669311523438, grad_scale: , lr: 9.31e-07, 
2024-12-21 17:27:01,359 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 17:27:01,360 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 17:28:47,533 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 18324,  validation: loss=0.1102, DER=0.1609, ACC=0.9516, MI=0.0373, FA=0.05599, CF=0.06759, over 0.00 frames. 
2024-12-21 17:28:47,590 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 17:28:47,832 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 18324,  validation: loss=0.1098, DER=0.1631, ACC=0.9516, MI=0.03791, FA=0.05608, CF=0.0691, over 0.00 frames. 
2024-12-21 17:28:47,833 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 18:53:45,407 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 18824, num_updates: 18000, {'loss': 0.1555013209581375, 'DER': 0.2548393159180605, 'ACC': 0.9301953125, 'MI': 0.08983273820710393, 'FA': 0.08400676564555534, 'CF': 0.08099981206540124}, batch size: 64, grad_norm: 1.2917711734771729, grad_scale: , lr: 6.53e-07, 
2024-12-21 18:53:45,407 (train_accelerate_ddp:699) INFO: [Train] - Epoch 9, batch_idx_train: 18824, num_updates: 18000, {'loss': 0.16725166141986847, 'DER': 0.2779128483261642, 'ACC': 0.92796875, 'MI': 0.1335328221432579, 'FA': 0.07742659435197306, 'CF': 0.06695343183093323}, batch size: 64, grad_norm: 1.2917711734771729, grad_scale: , lr: 6.53e-07, 
2024-12-21 18:53:45,408 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 18:53:45,408 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 18:55:26,959 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 18824,  validation: loss=0.1085, DER=0.1575, ACC=0.9535, MI=0.03966, FA=0.05579, CF=0.0621, over 0.00 frames. 
2024-12-21 18:55:26,984 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 18:55:27,439 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 9, batch_idx_train: 18824,  validation: loss=0.108, DER=0.1588, ACC=0.9538, MI=0.04029, FA=0.05561, CF=0.06287, over 0.00 frames. 
2024-12-21 18:55:27,440 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 19:12:11,834 (train_accelerate_ddp:710) INFO: end of epoch 9, batch_idx: 2102 batch_idx_train: 18926, {'loss': 0.17746613919734955, 'DER': 0.28321085358959863, 'ACC': 0.920234375, 'MI': 0.1138119464857735, 'FA': 0.06783493499152063, 'CF': 0.1015639721123045}, batch size: 64, grad_norm: 0.9581810832023621, grad_scale: , lr: 5.96e-07, 
2024-12-21 19:12:11,835 (train_accelerate_ddp:710) INFO: end of epoch 9, batch_idx: 2102 batch_idx_train: 18926, {'loss': 0.1447635293006897, 'DER': 0.21965827021314074, 'ACC': 0.9410546875, 'MI': 0.1130878985379602, 'FA': 0.06041923551171394, 'CF': 0.04615113616346662}, batch size: 64, grad_norm: 0.9581810832023621, grad_scale: , lr: 5.96e-07, 
2024-12-21 19:12:11,996 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-9.pt
2024-12-21 19:12:25,767 (train_accelerate_ddp:558) INFO:  end of epoch 9, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-9.pt 
2024-12-21 19:13:40,034 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 18927, num_updates: 18000, {'loss': 0.15034990012645721, 'DER': 0.2386652158142909, 'ACC': 0.9351953125, 'MI': 0.11389191149800508, 'FA': 0.06256800870511425, 'CF': 0.062205295611171565}, batch size: 64, grad_norm: 0.9372331500053406, grad_scale: , lr: 5.96e-07, 
2024-12-21 19:13:40,037 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 18927, num_updates: 18000, {'loss': 0.15431396663188934, 'DER': 0.25704870010984987, 'ACC': 0.9312109375, 'MI': 0.13950933723910655, 'FA': 0.05217868912486269, 'CF': 0.06536067374588063}, batch size: 64, grad_norm: 0.9372331500053406, grad_scale: , lr: 5.96e-07, 
2024-12-21 20:04:33,888 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 19427, num_updates: 18500, {'loss': 0.15978647768497467, 'DER': 0.2585155969881678, 'ACC': 0.9306207004377736, 'MI': 0.14342058085335246, 'FA': 0.055396199354607385, 'CF': 0.05969881678020796}, batch size: 64, grad_norm: 0.9502784609794617, grad_scale: , lr: 3.18e-07, 
2024-12-21 20:04:33,889 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 19427, num_updates: 18500, {'loss': 0.1442490965127945, 'DER': 0.21696035242290748, 'ACC': 0.9429296875, 'MI': 0.10848017621145374, 'FA': 0.05726872246696035, 'CF': 0.051211453744493395}, batch size: 64, grad_norm: 0.9502784609794617, grad_scale: , lr: 3.18e-07, 
2024-12-21 20:04:33,892 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 20:04:33,892 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 20:06:14,984 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 19427,  validation: loss=0.1097, DER=0.1594, ACC=0.9525, MI=0.03821, FA=0.05618, CF=0.06506, over 0.00 frames. 
2024-12-21 20:06:15,010 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 20:06:15,091 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 19427,  validation: loss=0.1092, DER=0.1624, ACC=0.9522, MI=0.03901, FA=0.05686, CF=0.06655, over 0.00 frames. 
2024-12-21 20:06:15,092 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 20:15:36,196 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-19500.pt
2024-12-21 20:15:57,908 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-21 20:15:58,044 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-21 21:18:20,951 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 19927, num_updates: 19000, {'loss': 0.16517432034015656, 'DER': 0.25931363552945497, 'ACC': 0.9268359375, 'MI': 0.10754266837951917, 'FA': 0.06735180767113232, 'CF': 0.08441915947880345}, batch size: 64, grad_norm: 1.0833650827407837, grad_scale: , lr: 4.00e-08, 
2024-12-21 21:18:20,961 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 21:18:20,951 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 19927, num_updates: 19000, {'loss': 0.19774700701236725, 'DER': 0.30241051862673485, 'ACC': 0.91140625, 'MI': 0.1260043827611395, 'FA': 0.06464572680788896, 'CF': 0.11176040905770636}, batch size: 64, grad_norm: 1.0833650827407837, grad_scale: , lr: 4.00e-08, 
2024-12-21 21:18:20,961 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 21:20:07,285 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 19927,  validation: loss=0.1098, DER=0.1581, ACC=0.953, MI=0.03795, FA=0.05654, CF=0.0636, over 0.00 frames. 
2024-12-21 21:20:07,536 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 19927,  validation: loss=0.1094, DER=0.1611, ACC=0.9527, MI=0.03877, FA=0.05668, CF=0.06559, over 0.00 frames. 
2024-12-21 21:20:07,846 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 21:20:07,846 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 22:48:46,603 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 20427, num_updates: 19500, {'loss': 0.15046709775924683, 'DER': 0.23818724749692605, 'ACC': 0.9280859375, 'MI': 0.11821535218689619, 'FA': 0.03477955383804673, 'CF': 0.08519234147198314}, batch size: 64, grad_norm: 0.9103774428367615, grad_scale: , lr: 0.00e+00, 
2024-12-21 22:48:46,604 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 22:48:46,606 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 20427, num_updates: 19500, {'loss': 0.16080762445926666, 'DER': 0.24212303075768943, 'ACC': 0.93515625, 'MI': 0.11046511627906977, 'FA': 0.06245311327831958, 'CF': 0.06920480120030008}, batch size: 64, grad_norm: 0.9103774428367615, grad_scale: , lr: 0.00e+00, 
2024-12-21 22:48:46,606 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-21 22:50:32,763 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 20427,  validation: loss=0.1106, DER=0.1586, ACC=0.9525, MI=0.0335, FA=0.05916, CF=0.06592, over 0.00 frames. 
2024-12-21 22:50:32,854 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 20427,  validation: loss=0.1101, DER=0.1619, ACC=0.9521, MI=0.03432, FA=0.05967, CF=0.06794, over 0.00 frames. 
2024-12-21 22:50:32,904 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-21 22:50:32,904 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 00:07:16,627 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 20927, num_updates: 20000, {'loss': 0.14652766287326813, 'DER': 0.23564143853998926, 'ACC': 0.935078125, 'MI': 0.12954016818751118, 'FA': 0.04437287529074969, 'CF': 0.06172839506172839}, batch size: 64, grad_norm: 0.9031761884689331, grad_scale: , lr: 0.00e+00, 
2024-12-22 00:07:16,627 (train_accelerate_ddp:699) INFO: [Train] - Epoch 10, batch_idx_train: 20927, num_updates: 20000, {'loss': 0.132283553481102, 'DER': 0.20974808324205915, 'ACC': 0.94546875, 'MI': 0.10021905805038335, 'FA': 0.06443957648776925, 'CF': 0.045089448703906534}, batch size: 64, grad_norm: 0.9031761884689331, grad_scale: , lr: 0.00e+00, 
2024-12-22 00:07:16,636 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 00:07:16,636 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 00:08:59,393 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 20927,  validation: loss=0.1098, DER=0.158, ACC=0.953, MI=0.03667, FA=0.05716, CF=0.06414, over 0.00 frames. 
2024-12-22 00:08:59,398 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 00:08:59,765 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 10, batch_idx_train: 20927,  validation: loss=0.1094, DER=0.1599, ACC=0.9531, MI=0.03693, FA=0.05745, CF=0.06552, over 0.00 frames. 
2024-12-22 00:08:59,766 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 00:19:18,209 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-21000.pt
2024-12-22 00:19:29,639 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 00:23:37,763 (train_accelerate_ddp:710) INFO: end of epoch 10, batch_idx: 2102 batch_idx_train: 21029, {'loss': 0.20717096328735352, 'DER': 0.3388728323699422, 'ACC': 0.9030859375, 'MI': 0.17304913294797689, 'FA': 0.056539017341040464, 'CF': 0.10928468208092486}, batch size: 64, grad_norm: 1.4081165790557861, grad_scale: , lr: 0.00e+00, 
2024-12-22 00:23:37,764 (train_accelerate_ddp:710) INFO: end of epoch 10, batch_idx: 2102 batch_idx_train: 21029, {'loss': 0.16083145141601562, 'DER': 0.22714130827611131, 'ACC': 0.9355078125, 'MI': 0.11366100469822912, 'FA': 0.0422840621611854, 'CF': 0.07119624141669678}, batch size: 64, grad_norm: 1.4081165790557861, grad_scale: , lr: 0.00e+00, 
2024-12-22 00:23:37,775 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-10.pt
2024-12-22 00:23:59,923 (train_accelerate_ddp:558) INFO:  end of epoch 10, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-10.pt 
2024-12-22 00:24:17,024 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 21030, num_updates: 20000, {'loss': 0.1454612910747528, 'DER': 0.23180740470545322, 'ACC': 0.9328515625, 'MI': 0.09246762721138063, 'FA': 0.057632682837862485, 'CF': 0.08170709465621011}, batch size: 64, grad_norm: 1.3659257888793945, grad_scale: , lr: 0.00e+00, 
2024-12-22 00:24:17,026 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 21030, num_updates: 20000, {'loss': 0.18617238104343414, 'DER': 0.29976807112485504, 'ACC': 0.921484375, 'MI': 0.12060301507537688, 'FA': 0.09045226130653267, 'CF': 0.0887127947429455}, batch size: 64, grad_norm: 1.3659257888793945, grad_scale: , lr: 0.00e+00, 
2024-12-22 01:43:44,622 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 21530, num_updates: 20500, {'loss': 0.16915854811668396, 'DER': 0.2850423330197554, 'ACC': 0.92375, 'MI': 0.116274694261524, 'FA': 0.08654750705550329, 'CF': 0.08222013170272813}, batch size: 64, grad_norm: 1.0461530685424805, grad_scale: , lr: 0.00e+00, 
2024-12-22 01:43:44,631 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 21530, num_updates: 20500, {'loss': 0.16788919270038605, 'DER': 0.28739336492890993, 'ACC': 0.9275, 'MI': 0.12682464454976303, 'FA': 0.09611374407582939, 'CF': 0.06445497630331753}, batch size: 64, grad_norm: 1.0461530685424805, grad_scale: , lr: 0.00e+00, 
2024-12-22 01:43:44,632 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 01:43:44,632 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 01:45:29,619 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 21530,  validation: loss=0.1096, DER=0.1579, ACC=0.953, MI=0.03722, FA=0.05694, CF=0.06376, over 0.00 frames. 
2024-12-22 01:45:29,646 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 01:45:29,977 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 21530,  validation: loss=0.1092, DER=0.1596, ACC=0.9532, MI=0.03776, FA=0.05673, CF=0.0651, over 0.00 frames. 
2024-12-22 01:45:29,978 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 03:06:38,979 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 22030, num_updates: 21000, {'loss': 0.15843012928962708, 'DER': 0.2551674982181041, 'ACC': 0.9280078125, 'MI': 0.12544547398431932, 'FA': 0.05648610121168924, 'CF': 0.07323592302209551}, batch size: 64, grad_norm: 1.1141237020492554, grad_scale: , lr: 0.00e+00, 
2024-12-22 03:06:38,980 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 22030, num_updates: 21000, {'loss': 0.18094779551029205, 'DER': 0.2934333958724203, 'ACC': 0.9158203125, 'MI': 0.11031894934333959, 'FA': 0.07223264540337711, 'CF': 0.11088180112570356}, batch size: 64, grad_norm: 1.1141237020492554, grad_scale: , lr: 0.00e+00, 
2024-12-22 03:06:39,110 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 03:06:39,111 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 03:08:20,119 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 22030,  validation: loss=0.1103, DER=0.1592, ACC=0.9526, MI=0.0381, FA=0.05621, CF=0.06493, over 0.00 frames. 
2024-12-22 03:08:20,124 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 03:08:20,569 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 22030,  validation: loss=0.1099, DER=0.1632, ACC=0.9519, MI=0.03895, FA=0.05695, CF=0.06728, over 0.00 frames. 
2024-12-22 03:08:20,569 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 04:17:33,007 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-22500.pt
2024-12-22 04:17:46,310 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 04:17:46,472 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 04:21:29,065 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 22530, num_updates: 21500, {'loss': 0.16230453550815582, 'DER': 0.269449715370019, 'ACC': 0.9272265625, 'MI': 0.10227703984819735, 'FA': 0.08311195445920304, 'CF': 0.0840607210626186}, batch size: 64, grad_norm: 1.0121525526046753, grad_scale: , lr: 0.00e+00, 
2024-12-22 04:21:29,068 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 04:21:29,072 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 22530, num_updates: 21500, {'loss': 0.14525988698005676, 'DER': 0.24267161410018553, 'ACC': 0.9359375, 'MI': 0.10816326530612246, 'FA': 0.07291280148423006, 'CF': 0.06159554730983302}, batch size: 64, grad_norm: 1.0121525526046753, grad_scale: , lr: 0.00e+00, 
2024-12-22 04:21:29,072 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 04:23:11,245 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 22530,  validation: loss=0.1097, DER=0.1575, ACC=0.9532, MI=0.03778, FA=0.05635, CF=0.06335, over 0.00 frames. 
2024-12-22 04:23:11,255 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 04:23:11,519 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 22530,  validation: loss=0.1095, DER=0.1602, ACC=0.9531, MI=0.03839, FA=0.05666, CF=0.06513, over 0.00 frames. 
2024-12-22 04:23:11,519 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 05:16:19,788 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 23030, num_updates: 22000, {'loss': 0.19260859489440918, 'DER': 0.31545338441890164, 'ACC': 0.910859375, 'MI': 0.14431673052362706, 'FA': 0.070242656449553, 'CF': 0.10089399744572158}, batch size: 64, grad_norm: 1.2021167278289795, grad_scale: , lr: 0.00e+00, 
2024-12-22 05:16:19,789 (train_accelerate_ddp:699) INFO: [Train] - Epoch 11, batch_idx_train: 23030, num_updates: 22000, {'loss': 0.17698386311531067, 'DER': 0.3000545553737043, 'ACC': 0.918984375, 'MI': 0.17875977450445535, 'FA': 0.044189852700491, 'CF': 0.07710492816875796}, batch size: 64, grad_norm: 1.2021167278289795, grad_scale: , lr: 0.00e+00, 
2024-12-22 05:16:19,790 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 05:16:19,790 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 05:18:02,486 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 23030,  validation: loss=0.1094, DER=0.1569, ACC=0.9534, MI=0.03704, FA=0.05697, CF=0.06293, over 0.00 frames. 
2024-12-22 05:18:02,645 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 05:18:02,839 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 11, batch_idx_train: 23030,  validation: loss=0.1089, DER=0.1588, ACC=0.9535, MI=0.03764, FA=0.05698, CF=0.06422, over 0.00 frames. 
2024-12-22 05:18:02,839 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 05:29:34,347 (train_accelerate_ddp:710) INFO: end of epoch 11, batch_idx: 2102 batch_idx_train: 23132, {'loss': 0.16934655606746674, 'DER': 0.28270042194092826, 'ACC': 0.92671875, 'MI': 0.11162255466052934, 'FA': 0.09397775220560031, 'CF': 0.07710011507479862}, batch size: 64, grad_norm: 1.0560455322265625, grad_scale: , lr: 0.00e+00, 
2024-12-22 05:29:34,351 (train_accelerate_ddp:710) INFO: end of epoch 11, batch_idx: 2102 batch_idx_train: 23132, {'loss': 0.1708340048789978, 'DER': 0.2808155699721965, 'ACC': 0.9255859375, 'MI': 0.12103799814643189, 'FA': 0.08748841519925857, 'CF': 0.07228915662650602}, batch size: 64, grad_norm: 1.0560455322265625, grad_scale: , lr: 0.00e+00, 
2024-12-22 05:29:34,359 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-11.pt
2024-12-22 05:29:44,036 (train_accelerate_ddp:558) INFO:  end of epoch 11, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-11.pt 
2024-12-22 05:30:31,639 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 23133, num_updates: 22000, {'loss': 0.11068134754896164, 'DER': 0.16263292995966264, 'ACC': 0.956796875, 'MI': 0.08159149248258159, 'FA': 0.040887422075540886, 'CF': 0.040154015401540157}, batch size: 64, grad_norm: 1.0724263191223145, grad_scale: , lr: 0.00e+00, 
2024-12-22 05:30:31,639 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 23133, num_updates: 22000, {'loss': 0.1958342343568802, 'DER': 0.32350312779267204, 'ACC': 0.9046484375, 'MI': 0.1585344057193923, 'FA': 0.05218945487042002, 'CF': 0.11277926720285969}, batch size: 64, grad_norm: 1.0724263191223145, grad_scale: , lr: 0.00e+00, 
2024-12-22 06:35:25,688 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 23633, num_updates: 22500, {'loss': 0.13221760094165802, 'DER': 0.2084253127299485, 'ACC': 0.9414453125, 'MI': 0.09253127299484916, 'FA': 0.04856512141280353, 'CF': 0.0673289183222958}, batch size: 64, grad_norm: 0.9085360765457153, grad_scale: , lr: 0.00e+00, 
2024-12-22 06:35:25,688 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 23633, num_updates: 22500, {'loss': 0.15426065027713776, 'DER': 0.23549295774647888, 'ACC': 0.9348828125, 'MI': 0.0856338028169014, 'FA': 0.07230046948356808, 'CF': 0.07755868544600938}, batch size: 64, grad_norm: 0.9085360765457153, grad_scale: , lr: 0.00e+00, 
2024-12-22 06:35:25,705 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 06:35:25,705 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 06:37:07,121 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 23633,  validation: loss=0.1093, DER=0.1575, ACC=0.9534, MI=0.03979, FA=0.05511, CF=0.06263, over 0.00 frames. 
2024-12-22 06:37:07,178 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 23633,  validation: loss=0.109, DER=0.16, ACC=0.9533, MI=0.04051, FA=0.0554, CF=0.06408, over 0.00 frames. 
2024-12-22 06:37:07,373 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 06:37:07,374 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 07:22:14,137 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-24000.pt
2024-12-22 07:22:23,643 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 07:22:23,850 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 07:38:09,298 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 24133, num_updates: 23000, {'loss': 0.17671102285385132, 'DER': 0.31593090211132435, 'ACC': 0.9132421875, 'MI': 0.12744721689059502, 'FA': 0.0781190019193858, 'CF': 0.11036468330134357}, batch size: 64, grad_norm: 1.2397555112838745, grad_scale: , lr: 0.00e+00, 
2024-12-22 07:38:09,305 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 24133, num_updates: 23000, {'loss': 0.17299482226371765, 'DER': 0.29119555143651527, 'ACC': 0.91921875, 'MI': 0.11899907321594068, 'FA': 0.08007414272474514, 'CF': 0.09212233549582947}, batch size: 64, grad_norm: 1.2397555112838745, grad_scale: , lr: 0.00e+00, 
2024-12-22 07:38:09,309 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 07:38:09,309 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 07:39:50,752 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 24133,  validation: loss=0.1102, DER=0.1586, ACC=0.9528, MI=0.0364, FA=0.05807, CF=0.06416, over 0.00 frames. 
2024-12-22 07:39:50,771 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 07:39:50,931 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 24133,  validation: loss=0.1098, DER=0.1621, ACC=0.9523, MI=0.03713, FA=0.05835, CF=0.06661, over 0.00 frames. 
2024-12-22 07:39:50,931 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 08:37:24,688 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 24633, num_updates: 23500, {'loss': 0.185965433716774, 'DER': 0.3146252285191956, 'ACC': 0.909609375, 'MI': 0.13638025594149908, 'FA': 0.06983546617915905, 'CF': 0.10840950639853748}, batch size: 64, grad_norm: 1.158677101135254, grad_scale: , lr: 0.00e+00, 
2024-12-22 08:37:24,694 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 24633, num_updates: 23500, {'loss': 0.15863284468650818, 'DER': 0.24361095789667217, 'ACC': 0.928125, 'MI': 0.08365508365508366, 'FA': 0.0652693509836367, 'CF': 0.09468652325795184}, batch size: 64, grad_norm: 1.158677101135254, grad_scale: , lr: 0.00e+00, 
2024-12-22 08:37:25,221 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 08:37:25,222 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 08:39:05,727 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 24633,  validation: loss=0.1097, DER=0.1586, ACC=0.9531, MI=0.03832, FA=0.05724, CF=0.06302, over 0.00 frames. 
2024-12-22 08:39:05,747 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 08:39:05,943 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 24633,  validation: loss=0.1093, DER=0.1618, ACC=0.9526, MI=0.03904, FA=0.05719, CF=0.06553, over 0.00 frames. 
2024-12-22 08:39:05,943 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 09:39:50,251 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 25133, num_updates: 24000, {'loss': 0.15724237263202667, 'DER': 0.24752653354919948, 'ACC': 0.9285546875, 'MI': 0.124123043712898, 'FA': 0.04191401331174672, 'CF': 0.08148947652455478}, batch size: 64, grad_norm: 0.9968075156211853, grad_scale: , lr: 0.00e+00, 
2024-12-22 09:39:50,252 (train_accelerate_ddp:699) INFO: [Train] - Epoch 12, batch_idx_train: 25133, num_updates: 24000, {'loss': 0.17640314996242523, 'DER': 0.3030838735022589, 'ACC': 0.9189453125, 'MI': 0.09506973089766255, 'FA': 0.10351600864270281, 'CF': 0.10449813396189354}, batch size: 64, grad_norm: 0.9968075156211853, grad_scale: , lr: 0.00e+00, 
2024-12-22 09:39:50,252 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 09:39:50,252 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 09:41:35,431 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 25133,  validation: loss=0.1093, DER=0.1583, ACC=0.9529, MI=0.03687, FA=0.05735, CF=0.06411, over 0.00 frames. 
2024-12-22 09:41:35,434 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 09:41:36,095 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 12, batch_idx_train: 25133,  validation: loss=0.1093, DER=0.1607, ACC=0.9528, MI=0.03713, FA=0.05737, CF=0.0662, over 0.00 frames. 
2024-12-22 09:41:36,095 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 09:54:34,676 (train_accelerate_ddp:710) INFO: end of epoch 12, batch_idx: 2102 batch_idx_train: 25235, {'loss': 0.185007706284523, 'DER': 0.29231324431613137, 'ACC': 0.918125, 'MI': 0.13911945146156623, 'FA': 0.0673042223024179, 'CF': 0.08588957055214724}, batch size: 64, grad_norm: 1.3524054288864136, grad_scale: , lr: 0.00e+00, 
2024-12-22 09:54:34,683 (train_accelerate_ddp:710) INFO: end of epoch 12, batch_idx: 2102 batch_idx_train: 25235, {'loss': 0.17369388043880463, 'DER': 0.24282274495276904, 'ACC': 0.929375, 'MI': 0.10001852194850898, 'FA': 0.050750138914613815, 'CF': 0.09205408408964623}, batch size: 64, grad_norm: 1.3524054288864136, grad_scale: , lr: 0.00e+00, 
2024-12-22 09:54:34,686 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-12.pt
2024-12-22 09:54:42,868 (train_accelerate_ddp:558) INFO:  end of epoch 12, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-12.pt 
2024-12-22 09:54:54,193 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 25236, num_updates: 24000, {'loss': 0.15947353839874268, 'DER': 0.25076342733968027, 'ACC': 0.930546875, 'MI': 0.13454284174600323, 'FA': 0.0476019400035926, 'CF': 0.06861864559008443}, batch size: 64, grad_norm: 1.1404632329940796, grad_scale: , lr: 0.00e+00, 
2024-12-22 09:54:54,200 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 25236, num_updates: 24000, {'loss': 0.2071186751127243, 'DER': 0.3392791467451269, 'ACC': 0.9055078125, 'MI': 0.17175432144170652, 'FA': 0.06197131298271423, 'CF': 0.10555351232070614}, batch size: 64, grad_norm: 1.1404632329940796, grad_scale: , lr: 0.00e+00, 
2024-12-22 10:30:01,414 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-25500.pt
2024-12-22 10:30:18,332 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 10:30:18,658 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 11:02:03,090 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 25736, num_updates: 24500, {'loss': 0.23731203377246857, 'DER': 0.35821980018165306, 'ACC': 0.8973585201442232, 'MI': 0.1673024523160763, 'FA': 0.07338782924613987, 'CF': 0.11752951861943688}, batch size: 64, grad_norm: 1.297855019569397, grad_scale: , lr: 0.00e+00, 
2024-12-22 11:02:03,100 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 11:02:03,092 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 25736, num_updates: 24500, {'loss': 0.18728488683700562, 'DER': 0.2898550724637681, 'ACC': 0.9176171875, 'MI': 0.14832707103238504, 'FA': 0.05403471103954196, 'CF': 0.08749329039184112}, batch size: 64, grad_norm: 1.297855019569397, grad_scale: , lr: 0.00e+00, 
2024-12-22 11:02:03,100 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 11:03:47,650 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 25736,  validation: loss=0.1101, DER=0.1583, ACC=0.9528, MI=0.03734, FA=0.05654, CF=0.06445, over 0.00 frames. 
2024-12-22 11:03:47,657 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 11:03:47,762 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 25736,  validation: loss=0.1101, DER=0.1606, ACC=0.9528, MI=0.03801, FA=0.05637, CF=0.06619, over 0.00 frames. 
2024-12-22 11:03:47,763 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 12:08:39,698 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 26236, num_updates: 25000, {'loss': 0.16748714447021484, 'DER': 0.27035889961741666, 'ACC': 0.92890625, 'MI': 0.14519948988886866, 'FA': 0.06394607396611404, 'CF': 0.06121333576243396}, batch size: 64, grad_norm: 1.0612868070602417, grad_scale: , lr: 0.00e+00, 
2024-12-22 12:08:39,704 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 12:08:39,704 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 26236, num_updates: 25000, {'loss': 0.13054707646369934, 'DER': 0.19636816806124266, 'ACC': 0.946796875, 'MI': 0.10770874132098986, 'FA': 0.04254940359622574, 'CF': 0.04611002314402706}, batch size: 64, grad_norm: 1.0612868070602417, grad_scale: , lr: 0.00e+00, 
2024-12-22 12:08:39,704 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 12:10:23,349 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 26236,  validation: loss=0.1103, DER=0.1588, ACC=0.9528, MI=0.03818, FA=0.05654, CF=0.06413, over 0.00 frames. 
2024-12-22 12:10:23,350 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 12:10:23,511 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 26236,  validation: loss=0.1101, DER=0.161, ACC=0.9528, MI=0.03861, FA=0.05688, CF=0.06555, over 0.00 frames. 
2024-12-22 12:10:23,511 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 13:14:09,347 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 26736, num_updates: 25500, {'loss': 0.17378784716129303, 'DER': 0.2697228144989339, 'ACC': 0.9250390625, 'MI': 0.1455223880597015, 'FA': 0.05294953802416489, 'CF': 0.07125088841506752}, batch size: 64, grad_norm: 1.3561242818832397, grad_scale: , lr: 0.00e+00, 
2024-12-22 13:14:09,351 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 13:14:09,352 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 26736, num_updates: 25500, {'loss': 0.18871085345745087, 'DER': 0.29442379182156136, 'ACC': 0.914609375, 'MI': 0.12788104089219332, 'FA': 0.05464684014869888, 'CF': 0.11189591078066914}, batch size: 64, grad_norm: 1.3561242818832397, grad_scale: , lr: 0.00e+00, 
2024-12-22 13:14:09,352 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 13:15:50,198 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 26736,  validation: loss=0.1096, DER=0.1581, ACC=0.9529, MI=0.03734, FA=0.05657, CF=0.0642, over 0.00 frames. 
2024-12-22 13:15:50,299 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 26736,  validation: loss=0.1092, DER=0.1612, ACC=0.9526, MI=0.0382, FA=0.05681, CF=0.06622, over 0.00 frames. 
2024-12-22 13:15:50,646 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 13:15:50,647 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 13:47:31,723 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-27000.pt
2024-12-22 13:47:46,668 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 14:16:07,651 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 27236, num_updates: 26000, {'loss': 0.1931312084197998, 'DER': 0.2897354100206418, 'ACC': 0.9191796875, 'MI': 0.12066053668605742, 'FA': 0.07055732782886096, 'CF': 0.0985175455057234}, batch size: 64, grad_norm: 1.244113564491272, grad_scale: , lr: 0.00e+00, 
2024-12-22 14:16:07,657 (train_accelerate_ddp:699) INFO: [Train] - Epoch 13, batch_idx_train: 27236, num_updates: 26000, {'loss': 0.18994708359241486, 'DER': 0.32483959670027496, 'ACC': 0.9103125, 'MI': 0.16571952337305224, 'FA': 0.06306141154903758, 'CF': 0.09605866177818514}, batch size: 64, grad_norm: 1.244113564491272, grad_scale: , lr: 0.00e+00, 
2024-12-22 14:16:07,663 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 14:16:07,663 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 14:17:50,928 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 27236,  validation: loss=0.1102, DER=0.1585, ACC=0.9527, MI=0.03785, FA=0.05577, CF=0.06489, over 0.00 frames. 
2024-12-22 14:17:51,062 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 14:17:51,237 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 13, batch_idx_train: 27236,  validation: loss=0.1098, DER=0.162, ACC=0.9523, MI=0.03873, FA=0.05641, CF=0.06688, over 0.00 frames. 
2024-12-22 14:17:51,237 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 14:30:10,110 (train_accelerate_ddp:710) INFO: end of epoch 13, batch_idx: 2102 batch_idx_train: 27338, {'loss': 0.20530931651592255, 'DER': 0.31651031894934334, 'ACC': 0.91, 'MI': 0.11951219512195121, 'FA': 0.08123827392120075, 'CF': 0.11575984990619137}, batch size: 64, grad_norm: 1.5246212482452393, grad_scale: , lr: 0.00e+00, 
2024-12-22 14:30:10,115 (train_accelerate_ddp:710) INFO: end of epoch 13, batch_idx: 2102 batch_idx_train: 27338, {'loss': 0.18295249342918396, 'DER': 0.3013596572918607, 'ACC': 0.9151953125, 'MI': 0.1294468243620786, 'FA': 0.06891413671074688, 'CF': 0.10299869621903521}, batch size: 64, grad_norm: 1.5246212482452393, grad_scale: , lr: 0.00e+00, 
2024-12-22 14:30:10,139 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-13.pt
2024-12-22 14:30:26,148 (train_accelerate_ddp:558) INFO:  end of epoch 13, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-13.pt 
2024-12-22 14:30:36,298 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 27339, num_updates: 26000, {'loss': 0.18350288271903992, 'DER': 0.29313142239048134, 'ACC': 0.91203125, 'MI': 0.13394627726699115, 'FA': 0.04633135027943032, 'CF': 0.11285379484405986}, batch size: 64, grad_norm: 1.1924322843551636, grad_scale: , lr: 0.00e+00, 
2024-12-22 14:30:36,304 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 27339, num_updates: 26000, {'loss': 0.19927506148815155, 'DER': 0.32387706855791965, 'ACC': 0.91140625, 'MI': 0.1774868157846881, 'FA': 0.05782869612656847, 'CF': 0.08856155664666303}, batch size: 64, grad_norm: 1.1924322843551636, grad_scale: , lr: 0.00e+00, 
2024-12-22 15:29:15,904 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 27839, num_updates: 26500, {'loss': 0.14781974256038666, 'DER': 0.21823056300268096, 'ACC': 0.9388671875, 'MI': 0.10902591599642537, 'FA': 0.04772117962466488, 'CF': 0.0614834673815907}, batch size: 64, grad_norm: 0.965537428855896, grad_scale: , lr: 0.00e+00, 
2024-12-22 15:29:15,905 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 15:29:15,908 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 27839, num_updates: 26500, {'loss': 0.16074424982070923, 'DER': 0.2569099929128278, 'ACC': 0.92984375, 'MI': 0.14546420978029767, 'FA': 0.0501417434443657, 'CF': 0.06130403968816442}, batch size: 64, grad_norm: 0.965537428855896, grad_scale: , lr: 0.00e+00, 
2024-12-22 15:29:15,908 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 15:30:56,439 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 27839,  validation: loss=0.1098, DER=0.1593, ACC=0.9527, MI=0.03868, FA=0.05642, CF=0.06417, over 0.00 frames. 
2024-12-22 15:30:56,440 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 15:30:57,001 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 27839,  validation: loss=0.1097, DER=0.1625, ACC=0.9522, MI=0.03941, FA=0.05646, CF=0.06667, over 0.00 frames. 
2024-12-22 15:30:57,002 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 16:34:32,535 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 28339, num_updates: 27000, {'loss': 0.16504310071468353, 'DER': 0.27762456333884905, 'ACC': 0.9262109375, 'MI': 0.14359257216400073, 'FA': 0.06435006435006435, 'CF': 0.06968192682478397}, batch size: 64, grad_norm: 0.9162771701812744, grad_scale: , lr: 0.00e+00, 
2024-12-22 16:34:32,573 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 16:34:32,885 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 28339, num_updates: 27000, {'loss': 0.1328412890434265, 'DER': 0.2251042989298023, 'ACC': 0.9401171875, 'MI': 0.12552149464901144, 'FA': 0.04661708688554326, 'CF': 0.052965717395247594}, batch size: 64, grad_norm: 0.9162771701812744, grad_scale: , lr: 0.00e+00, 
2024-12-22 16:34:32,885 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 16:36:15,510 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 28339,  validation: loss=0.1098, DER=0.1582, ACC=0.9528, MI=0.03847, FA=0.05548, CF=0.06429, over 0.00 frames. 
2024-12-22 16:36:15,588 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 16:36:16,041 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 28339,  validation: loss=0.1092, DER=0.1608, ACC=0.9528, MI=0.03911, FA=0.05606, CF=0.06564, over 0.00 frames. 
2024-12-22 16:36:16,041 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 17:03:43,233 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-28500.pt
2024-12-22 17:04:05,555 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 17:04:05,690 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 17:55:56,952 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 28839, num_updates: 27500, {'loss': 0.15292653441429138, 'DER': 0.2286780383795309, 'ACC': 0.9347265625, 'MI': 0.11567164179104478, 'FA': 0.04477611940298507, 'CF': 0.06823027718550106}, batch size: 64, grad_norm: 1.3152501583099365, grad_scale: , lr: 0.00e+00, 
2024-12-22 17:55:56,952 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 28839, num_updates: 27500, {'loss': 0.17571701109409332, 'DER': 0.2847562080841037, 'ACC': 0.9228125, 'MI': 0.1595069784303063, 'FA': 0.05183976798984956, 'CF': 0.0734094616639478}, batch size: 64, grad_norm: 1.3152501583099365, grad_scale: , lr: 0.00e+00, 
2024-12-22 17:55:56,953 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 17:55:56,953 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 17:57:34,569 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 28839,  validation: loss=0.1089, DER=0.1604, ACC=0.9528, MI=0.0405, FA=0.05367, CF=0.06622, over 0.00 frames. 
2024-12-22 17:57:34,569 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 17:57:34,599 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 28839,  validation: loss=0.1091, DER=0.1571, ACC=0.9532, MI=0.04013, FA=0.05304, CF=0.06397, over 0.00 frames. 
2024-12-22 17:57:34,599 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 18:19:03,289 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 29339, num_updates: 28000, {'loss': 0.16128051280975342, 'DER': 0.2446693169497651, 'ACC': 0.9321875, 'MI': 0.13028550777014816, 'FA': 0.045355981207083486, 'CF': 0.06902782797253343}, batch size: 64, grad_norm: 1.0182069540023804, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:19:03,289 (train_accelerate_ddp:699) INFO: [Train] - Epoch 14, batch_idx_train: 29339, num_updates: 28000, {'loss': 0.15579621493816376, 'DER': 0.25342706502636203, 'ACC': 0.931015625, 'MI': 0.14815465729349736, 'FA': 0.048330404217926184, 'CF': 0.056942003514938486}, batch size: 64, grad_norm: 1.0182069540023804, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:19:03,289 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 18:19:03,289 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 18:20:40,409 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 29339,  validation: loss=0.1087, DER=0.1584, ACC=0.9536, MI=0.03865, FA=0.05496, CF=0.06479, over 0.00 frames. 
2024-12-22 18:20:40,410 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 18:20:40,698 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 14, batch_idx_train: 29339,  validation: loss=0.1089, DER=0.1563, ACC=0.9535, MI=0.03831, FA=0.05494, CF=0.06307, over 0.00 frames. 
2024-12-22 18:20:40,698 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 18:24:51,167 (train_accelerate_ddp:710) INFO: end of epoch 14, batch_idx: 2102 batch_idx_train: 29441, {'loss': 0.1622694730758667, 'DER': 0.2481081081081081, 'ACC': 0.930234375, 'MI': 0.12594594594594594, 'FA': 0.048468468468468466, 'CF': 0.07369369369369369}, batch size: 64, grad_norm: 1.1009280681610107, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:24:51,167 (train_accelerate_ddp:710) INFO: end of epoch 14, batch_idx: 2102 batch_idx_train: 29441, {'loss': 0.1597052812576294, 'DER': 0.24765112568693495, 'ACC': 0.927890625, 'MI': 0.1326006027300124, 'FA': 0.03545470661230278, 'CF': 0.07959581634461975}, batch size: 64, grad_norm: 1.1009280681610107, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:24:51,170 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-14.pt
2024-12-22 18:25:02,891 (train_accelerate_ddp:558) INFO:  end of epoch 14, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-14.pt 
2024-12-22 18:25:26,457 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 29442, num_updates: 28000, {'loss': 0.16582496464252472, 'DER': 0.2725560369184404, 'ACC': 0.929375, 'MI': 0.12431719721228103, 'FA': 0.08024110001883594, 'CF': 0.06799773968732341}, batch size: 64, grad_norm: 1.2774661779403687, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:25:26,460 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 29442, num_updates: 28000, {'loss': 0.178287073969841, 'DER': 0.27819004524886876, 'ACC': 0.916484375, 'MI': 0.09918552036199095, 'FA': 0.07022624434389141, 'CF': 0.10877828054298642}, batch size: 64, grad_norm: 1.2774661779403687, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:46:31,040 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 29942, num_updates: 28500, {'loss': 0.17093485593795776, 'DER': 0.2563820018365473, 'ACC': 0.9248046875, 'MI': 0.09641873278236915, 'FA': 0.0628099173553719, 'CF': 0.09715335169880625}, batch size: 64, grad_norm: 1.0528113842010498, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:46:31,040 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 18:46:31,045 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 29942, num_updates: 28500, {'loss': 0.14773479104042053, 'DER': 0.2174962852897474, 'ACC': 0.9384765625, 'MI': 0.09268202080237742, 'FA': 0.04977711738484398, 'CF': 0.075037147102526}, batch size: 64, grad_norm: 1.0528113842010498, grad_scale: , lr: 0.00e+00, 
2024-12-22 18:46:31,045 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 18:48:08,266 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 29942,  validation: loss=0.1092, DER=0.1596, ACC=0.9534, MI=0.03748, FA=0.0582, CF=0.06395, over 0.00 frames. 
2024-12-22 18:48:08,267 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 18:48:09,214 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 29942,  validation: loss=0.1101, DER=0.1579, ACC=0.9532, MI=0.03679, FA=0.05791, CF=0.06316, over 0.00 frames. 
2024-12-22 18:48:09,215 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 18:50:31,073 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-30000.pt
2024-12-22 18:50:38,649 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 18:50:38,812 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 19:09:24,920 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 30442, num_updates: 29000, {'loss': 0.15917353332042694, 'DER': 0.2473898425057512, 'ACC': 0.9300390625, 'MI': 0.1410369846044948, 'FA': 0.036807644664661125, 'CF': 0.0695452132365953}, batch size: 64, grad_norm: 1.031333327293396, grad_scale: , lr: 0.00e+00, 
2024-12-22 19:09:24,920 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 19:09:24,921 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 30442, num_updates: 29000, {'loss': 0.12765447795391083, 'DER': 0.20336811275855757, 'ACC': 0.949765625, 'MI': 0.12429068277503204, 'FA': 0.04704374885593996, 'CF': 0.032033681127585575}, batch size: 64, grad_norm: 1.031333327293396, grad_scale: , lr: 0.00e+00, 
2024-12-22 19:09:24,921 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 19:11:02,045 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 30442,  validation: loss=0.1099, DER=0.1613, ACC=0.9525, MI=0.0385, FA=0.05618, CF=0.06663, over 0.00 frames. 
2024-12-22 19:11:02,045 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 19:11:02,733 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 30442,  validation: loss=0.1104, DER=0.1591, ACC=0.9525, MI=0.03797, FA=0.05588, CF=0.06522, over 0.00 frames. 
2024-12-22 19:11:02,734 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 19:31:56,142 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 30942, num_updates: 29500, {'loss': 0.13419416546821594, 'DER': 0.22424024456033087, 'ACC': 0.94203125, 'MI': 0.12641611221003415, 'FA': 0.05520589821974465, 'CF': 0.04261823413055206}, batch size: 64, grad_norm: 0.9096483588218689, grad_scale: , lr: 0.00e+00, 
2024-12-22 19:31:56,142 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 19:31:56,148 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 30942, num_updates: 29500, {'loss': 0.1443995237350464, 'DER': 0.2510741640201756, 'ACC': 0.9355078125, 'MI': 0.13282271623388753, 'FA': 0.06090042966560807, 'CF': 0.05735101812067999}, batch size: 64, grad_norm: 0.9096483588218689, grad_scale: , lr: 0.00e+00, 
2024-12-22 19:31:56,148 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 19:33:33,547 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 30942,  validation: loss=0.1099, DER=0.1617, ACC=0.9524, MI=0.04036, FA=0.05479, CF=0.06659, over 0.00 frames. 
2024-12-22 19:33:33,548 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 19:33:33,793 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 30942,  validation: loss=0.1103, DER=0.1586, ACC=0.9527, MI=0.03976, FA=0.05399, CF=0.06482, over 0.00 frames. 
2024-12-22 19:33:33,794 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 19:54:45,865 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 31442, num_updates: 30000, {'loss': 0.15534265339374542, 'DER': 0.24991100035599859, 'ACC': 0.932265625, 'MI': 0.13616945532217872, 'FA': 0.05500177999288003, 'CF': 0.058739765040939834}, batch size: 64, grad_norm: 1.1718765497207642, grad_scale: , lr: 0.00e+00, 
2024-12-22 19:54:45,866 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 19:54:45,872 (train_accelerate_ddp:699) INFO: [Train] - Epoch 15, batch_idx_train: 31442, num_updates: 30000, {'loss': 0.1547292321920395, 'DER': 0.2677016281711473, 'ACC': 0.9305078125, 'MI': 0.11870503597122302, 'FA': 0.07989397955319955, 'CF': 0.06910261264672472}, batch size: 64, grad_norm: 1.1718765497207642, grad_scale: , lr: 0.00e+00, 
2024-12-22 19:54:45,872 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 19:56:22,819 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 31442,  validation: loss=0.1101, DER=0.1645, ACC=0.9515, MI=0.03841, FA=0.05848, CF=0.06757, over 0.00 frames. 
2024-12-22 19:56:22,820 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 19:56:24,019 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 15, batch_idx_train: 31442,  validation: loss=0.1105, DER=0.1608, ACC=0.9522, MI=0.03766, FA=0.05811, CF=0.06508, over 0.00 frames. 
2024-12-22 19:56:24,019 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 19:58:53,468 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-31500.pt
2024-12-22 19:59:01,063 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 20:00:54,603 (train_accelerate_ddp:710) INFO: end of epoch 15, batch_idx: 2102 batch_idx_train: 31544, {'loss': 0.1500535011291504, 'DER': 0.25735430157261796, 'ACC': 0.9323828125, 'MI': 0.1147086031452359, 'FA': 0.07974098057354302, 'CF': 0.06290471785383904}, batch size: 64, grad_norm: 1.180012583732605, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:00:54,603 (train_accelerate_ddp:710) INFO: end of epoch 15, batch_idx: 2102 batch_idx_train: 31544, {'loss': 0.1732551008462906, 'DER': 0.2755270394133822, 'ACC': 0.9269921875, 'MI': 0.15178735105407884, 'FA': 0.05664527956003666, 'CF': 0.06709440879926673}, batch size: 64, grad_norm: 1.180012583732605, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:00:54,606 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-15.pt
2024-12-22 20:01:00,338 (train_accelerate_ddp:558) INFO:  end of epoch 15, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-15.pt 
2024-12-22 20:01:04,280 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 31545, num_updates: 30000, {'loss': 0.19759242236614227, 'DER': 0.3037118303163284, 'ACC': 0.9147265625, 'MI': 0.14061071493874566, 'FA': 0.06765405010056683, 'CF': 0.09544706527701591}, batch size: 64, grad_norm: 1.3086997270584106, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:01:04,285 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 31545, num_updates: 30000, {'loss': 0.14252817630767822, 'DER': 0.22965379735363423, 'ACC': 0.935546875, 'MI': 0.1179989124524198, 'FA': 0.04223309769802429, 'CF': 0.06942178720319014}, batch size: 64, grad_norm: 1.3086997270584106, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:22:04,591 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 32045, num_updates: 30500, {'loss': 0.17453040182590485, 'DER': 0.25049133464355905, 'ACC': 0.9294921875, 'MI': 0.12721100589601572, 'FA': 0.05127747007325353, 'CF': 0.0720028586742898}, batch size: 64, grad_norm: 1.5884957313537598, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:22:04,592 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 32045, num_updates: 30500, {'loss': 0.22440959513187408, 'DER': 0.3268809349890431, 'ACC': 0.9047021943573668, 'MI': 0.14572680788897005, 'FA': 0.06391526661796933, 'CF': 0.11723886048210373}, batch size: 64, grad_norm: 1.5884957313537598, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:22:04,592 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 20:22:04,592 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 20:23:41,594 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 32045,  validation: loss=0.1085, DER=0.1592, ACC=0.9532, MI=0.03749, FA=0.05662, CF=0.06512, over 0.00 frames. 
2024-12-22 20:23:41,594 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 20:23:41,806 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 32045,  validation: loss=0.1088, DER=0.1566, ACC=0.9534, MI=0.03711, FA=0.05629, CF=0.06322, over 0.00 frames. 
2024-12-22 20:23:41,807 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 20:44:39,463 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 32545, num_updates: 31000, {'loss': 0.12889136373996735, 'DER': 0.2297790585975024, 'ACC': 0.94234375, 'MI': 0.09029779058597502, 'FA': 0.08568683957732949, 'CF': 0.053794428434197884}, batch size: 64, grad_norm: 0.88887619972229, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:44:39,464 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 20:44:39,471 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 32545, num_updates: 31000, {'loss': 0.17795027792453766, 'DER': 0.29660255257954343, 'ACC': 0.915859375, 'MI': 0.13374078734495776, 'FA': 0.0722631673557433, 'CF': 0.09059859787884235}, batch size: 64, grad_norm: 0.88887619972229, grad_scale: , lr: 0.00e+00, 
2024-12-22 20:44:39,471 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 20:46:16,744 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 32545,  validation: loss=0.1089, DER=0.1608, ACC=0.9527, MI=0.03784, FA=0.05686, CF=0.0661, over 0.00 frames. 
2024-12-22 20:46:16,744 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 20:46:17,400 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 32545,  validation: loss=0.1093, DER=0.158, ACC=0.953, MI=0.03727, FA=0.0565, CF=0.06424, over 0.00 frames. 
2024-12-22 20:46:17,400 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 21:05:20,149 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-33000.pt
2024-12-22 21:05:27,787 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 21:05:27,954 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 21:07:28,109 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 33045, num_updates: 31500, {'loss': 0.1911470890045166, 'DER': 0.33008279668813245, 'ACC': 0.9095703125, 'MI': 0.17111315547378106, 'FA': 0.06310947562097516, 'CF': 0.09586016559337626}, batch size: 64, grad_norm: 1.154943823814392, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:07:28,110 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 33045, num_updates: 31500, {'loss': 0.15265685319900513, 'DER': 0.2654504839910648, 'ACC': 0.92828125, 'MI': 0.13067758749069247, 'FA': 0.0584512285927029, 'CF': 0.0763216679076694}, batch size: 64, grad_norm: 1.154943823814392, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:07:28,110 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 21:07:28,110 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 21:09:05,364 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 33045,  validation: loss=0.1094, DER=0.1574, ACC=0.9532, MI=0.03837, FA=0.05542, CF=0.06365, over 0.00 frames. 
2024-12-22 21:09:05,364 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 21:09:05,970 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 33045,  validation: loss=0.109, DER=0.1603, ACC=0.953, MI=0.03905, FA=0.05606, CF=0.06522, over 0.00 frames. 
2024-12-22 21:09:05,971 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 21:30:01,973 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 33545, num_updates: 32000, {'loss': 0.1477615088224411, 'DER': 0.204769589384974, 'ACC': 0.940859375, 'MI': 0.08678500986193294, 'FA': 0.05128205128205128, 'CF': 0.06670252824098978}, batch size: 64, grad_norm: 0.8972824215888977, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:30:01,973 (train_accelerate_ddp:699) INFO: [Train] - Epoch 16, batch_idx_train: 33545, num_updates: 32000, {'loss': 0.1550130993127823, 'DER': 0.26943297033420954, 'ACC': 0.9338671875, 'MI': 0.14119414194517463, 'FA': 0.07979722117912129, 'CF': 0.04844160720991363}, batch size: 64, grad_norm: 0.8972824215888977, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:30:01,974 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 21:30:01,974 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 21:31:39,366 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 33545,  validation: loss=0.11, DER=0.1632, ACC=0.952, MI=0.03937, FA=0.05667, CF=0.06718, over 0.00 frames. 
2024-12-22 21:31:39,367 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 21:31:39,707 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 16, batch_idx_train: 33545,  validation: loss=0.11, DER=0.1601, ACC=0.9524, MI=0.0388, FA=0.05657, CF=0.06471, over 0.00 frames. 
2024-12-22 21:31:39,708 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 21:35:50,437 (train_accelerate_ddp:710) INFO: end of epoch 16, batch_idx: 2102 batch_idx_train: 33647, {'loss': 0.13942261040210724, 'DER': 0.22090437361008153, 'ACC': 0.9425, 'MI': 0.10100074128984433, 'FA': 0.06801334321719793, 'CF': 0.05189028910303929}, batch size: 64, grad_norm: 1.1223760843276978, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:35:50,438 (train_accelerate_ddp:710) INFO: end of epoch 16, batch_idx: 2102 batch_idx_train: 33647, {'loss': 0.17848369479179382, 'DER': 0.27888948981431405, 'ACC': 0.9187109375, 'MI': 0.1233098972417523, 'FA': 0.05931133946277267, 'CF': 0.09626825310978908}, batch size: 64, grad_norm: 1.1223760843276978, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:35:50,440 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-16.pt
2024-12-22 21:35:56,160 (train_accelerate_ddp:558) INFO:  end of epoch 16, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-16.pt 
2024-12-22 21:35:59,564 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 33648, num_updates: 32000, {'loss': 0.17546392977237701, 'DER': 0.3055296965183392, 'ACC': 0.914921875, 'MI': 0.13032954757028486, 'FA': 0.07521876745485012, 'CF': 0.09998138149320425}, batch size: 64, grad_norm: 1.0207425355911255, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:35:59,569 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 33648, num_updates: 32000, {'loss': 0.15822137892246246, 'DER': 0.25973548861131524, 'ACC': 0.929453125, 'MI': 0.11792799412196914, 'FA': 0.06980161645848641, 'CF': 0.07200587803085966}, batch size: 64, grad_norm: 1.0207425355911255, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:56:48,288 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 34148, num_updates: 32500, {'loss': 0.1549127697944641, 'DER': 0.2455012853470437, 'ACC': 0.9337890625, 'MI': 0.11329416085200147, 'FA': 0.06647080426000734, 'CF': 0.06573632023503488}, batch size: 64, grad_norm: 1.1370575428009033, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:56:48,289 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 21:56:48,290 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 34148, num_updates: 32500, {'loss': 0.16794361174106598, 'DER': 0.2616822429906542, 'ACC': 0.92671875, 'MI': 0.10392523364485981, 'FA': 0.06878504672897197, 'CF': 0.08897196261682243}, batch size: 64, grad_norm: 1.1370575428009033, grad_scale: , lr: 0.00e+00, 
2024-12-22 21:56:48,291 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 21:58:25,295 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 34148,  validation: loss=0.1089, DER=0.1602, ACC=0.9529, MI=0.03999, FA=0.05487, CF=0.06534, over 0.00 frames. 
2024-12-22 21:58:25,296 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 21:58:25,568 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 34148,  validation: loss=0.1094, DER=0.1574, ACC=0.9531, MI=0.03944, FA=0.0541, CF=0.06387, over 0.00 frames. 
2024-12-22 21:58:25,568 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 22:13:07,051 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-34500.pt
2024-12-22 22:13:14,134 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 22:13:14,287 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 22:19:32,359 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 34648, num_updates: 33000, {'loss': 0.14809951186180115, 'DER': 0.24696356275303644, 'ACC': 0.93375, 'MI': 0.1291866028708134, 'FA': 0.05263157894736842, 'CF': 0.06514538093485461}, batch size: 64, grad_norm: 1.0954514741897583, grad_scale: , lr: 0.00e+00, 
2024-12-22 22:19:32,360 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 22:19:32,361 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 34648, num_updates: 33000, {'loss': 0.1695910543203354, 'DER': 0.24708667152221414, 'ACC': 0.9289453125, 'MI': 0.1143481427530954, 'FA': 0.04861616897305171, 'CF': 0.08412235979606701}, batch size: 64, grad_norm: 1.0954514741897583, grad_scale: , lr: 0.00e+00, 
2024-12-22 22:19:32,361 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 22:21:09,898 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 34648,  validation: loss=0.1096, DER=0.1621, ACC=0.9524, MI=0.03705, FA=0.05908, CF=0.06593, over 0.00 frames. 
2024-12-22 22:21:09,898 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 22:21:10,137 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 34648,  validation: loss=0.1098, DER=0.1586, ACC=0.953, MI=0.03624, FA=0.05889, CF=0.06347, over 0.00 frames. 
2024-12-22 22:21:10,137 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 22:41:57,564 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 35148, num_updates: 33500, {'loss': 0.16592800617218018, 'DER': 0.2709141274238227, 'ACC': 0.927890625, 'MI': 0.14681440443213298, 'FA': 0.0541089566020314, 'CF': 0.06999076638965836}, batch size: 64, grad_norm: 0.9792994260787964, grad_scale: , lr: 0.00e+00, 
2024-12-22 22:41:57,565 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 22:41:57,566 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 35148, num_updates: 33500, {'loss': 0.11884675920009613, 'DER': 0.19269340974212035, 'ACC': 0.9483984375, 'MI': 0.10601719197707736, 'FA': 0.042800859598853866, 'CF': 0.043875358166189114}, batch size: 64, grad_norm: 0.9792994260787964, grad_scale: , lr: 0.00e+00, 
2024-12-22 22:41:57,567 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 22:43:34,664 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 35148,  validation: loss=0.1086, DER=0.159, ACC=0.9533, MI=0.03877, FA=0.05504, CF=0.06517, over 0.00 frames. 
2024-12-22 22:43:34,665 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 22:43:35,538 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 35148,  validation: loss=0.1089, DER=0.1567, ACC=0.9534, MI=0.03832, FA=0.05477, CF=0.0636, over 0.00 frames. 
2024-12-22 22:43:35,539 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 23:04:37,155 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 35648, num_updates: 34000, {'loss': 0.18431708216667175, 'DER': 0.2854542144547606, 'ACC': 0.9210546875, 'MI': 0.14491170580739124, 'FA': 0.05807391225195704, 'CF': 0.08246859639541235}, batch size: 64, grad_norm: 1.1502282619476318, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:04:37,156 (train_accelerate_ddp:699) INFO: [Train] - Epoch 17, batch_idx_train: 35648, num_updates: 34000, {'loss': 0.15323658287525177, 'DER': 0.2576492537313433, 'ACC': 0.933046875, 'MI': 0.11473880597014925, 'FA': 0.08078358208955223, 'CF': 0.062126865671641794}, batch size: 64, grad_norm: 1.1502282619476318, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:04:37,156 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 23:04:37,156 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 23:06:14,580 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 35648,  validation: loss=0.1096, DER=0.1606, ACC=0.9528, MI=0.03891, FA=0.05611, CF=0.06563, over 0.00 frames. 
2024-12-22 23:06:14,581 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 23:06:14,883 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 17, batch_idx_train: 35648,  validation: loss=0.1103, DER=0.1583, ACC=0.9528, MI=0.03811, FA=0.05569, CF=0.06452, over 0.00 frames. 
2024-12-22 23:06:14,884 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 23:10:30,093 (train_accelerate_ddp:710) INFO: end of epoch 17, batch_idx: 2102 batch_idx_train: 35750, {'loss': 0.1633671522140503, 'DER': 0.27118017018128004, 'ACC': 0.9223046875, 'MI': 0.11154273029966703, 'FA': 0.06289308176100629, 'CF': 0.09674435812060674}, batch size: 64, grad_norm: 1.0513226985931396, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:10:30,093 (train_accelerate_ddp:710) INFO: end of epoch 17, batch_idx: 2102 batch_idx_train: 35750, {'loss': 0.16472278535366058, 'DER': 0.2668256005868329, 'ACC': 0.930078125, 'MI': 0.14799193104713002, 'FA': 0.0573995965523565, 'CF': 0.06143407298734641}, batch size: 64, grad_norm: 1.0513226985931396, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:10:30,096 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-17.pt
2024-12-22 23:10:35,841 (train_accelerate_ddp:558) INFO:  end of epoch 17, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-17.pt 
2024-12-22 23:10:39,981 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 35751, num_updates: 34000, {'loss': 0.16083534061908722, 'DER': 0.2752410405675823, 'ACC': 0.925390625, 'MI': 0.1424413316354375, 'FA': 0.060578497362197566, 'CF': 0.07222121156994725}, batch size: 64, grad_norm: 1.1190717220306396, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:10:39,981 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 35751, num_updates: 34000, {'loss': 0.14263801276683807, 'DER': 0.2441991017964072, 'ACC': 0.9383984375, 'MI': 0.12649700598802396, 'FA': 0.06680389221556886, 'CF': 0.05089820359281437}, batch size: 64, grad_norm: 1.1190717220306396, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:21:12,502 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-36000.pt
2024-12-22 23:21:20,252 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-22 23:21:20,400 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-22 23:32:00,933 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 36251, num_updates: 34500, {'loss': 0.13515490293502808, 'DER': 0.22006292800296132, 'ACC': 0.9418204566781357, 'MI': 0.1169720525633907, 'FA': 0.04775124930594114, 'CF': 0.055339626133629465}, batch size: 64, grad_norm: 0.8982810378074646, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:32:00,934 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 23:32:00,939 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 36251, num_updates: 34500, {'loss': 0.15240830183029175, 'DER': 0.24924698795180722, 'ACC': 0.9313671875, 'MI': 0.1170933734939759, 'FA': 0.050640060240963854, 'CF': 0.08151355421686747}, batch size: 64, grad_norm: 0.8982810378074646, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:32:00,939 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 23:33:37,916 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 36251,  validation: loss=0.1085, DER=0.1588, ACC=0.9537, MI=0.03807, FA=0.0568, CF=0.06388, over 0.00 frames. 
2024-12-22 23:33:37,917 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 23:33:38,622 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 36251,  validation: loss=0.1088, DER=0.1571, ACC=0.9535, MI=0.03781, FA=0.05698, CF=0.06234, over 0.00 frames. 
2024-12-22 23:33:38,622 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 23:54:34,132 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 36751, num_updates: 35000, {'loss': 0.19842125475406647, 'DER': 0.33696264313872726, 'ACC': 0.9077734375, 'MI': 0.14698704711845317, 'FA': 0.08372442275201802, 'CF': 0.10625117326825606}, batch size: 64, grad_norm: 0.9257178902626038, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:54:34,133 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 23:54:34,476 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 36751, num_updates: 35000, {'loss': 0.14675717055797577, 'DER': 0.2586768935762224, 'ACC': 0.93578125, 'MI': 0.11486097794822626, 'FA': 0.087248322147651, 'CF': 0.05656759348034516}, batch size: 64, grad_norm: 0.9257178902626038, grad_scale: , lr: 0.00e+00, 
2024-12-22 23:54:34,477 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-22 23:56:11,370 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 36751,  validation: loss=0.1092, DER=0.1597, ACC=0.9533, MI=0.03803, FA=0.05697, CF=0.06469, over 0.00 frames. 
2024-12-22 23:56:11,371 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-22 23:56:12,110 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 36751,  validation: loss=0.1095, DER=0.1576, ACC=0.9532, MI=0.03756, FA=0.05697, CF=0.06309, over 0.00 frames. 
2024-12-22 23:56:12,110 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 00:17:00,206 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 37251, num_updates: 35500, {'loss': 0.19170965254306793, 'DER': 0.30555056583438117, 'ACC': 0.9151953125, 'MI': 0.15484102748338424, 'FA': 0.06628345608047423, 'CF': 0.08442608227052273}, batch size: 64, grad_norm: 1.3514022827148438, grad_scale: , lr: 0.00e+00, 
2024-12-23 00:17:00,207 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 00:17:00,212 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 37251, num_updates: 35500, {'loss': 0.16659162938594818, 'DER': 0.26119955572010367, 'ACC': 0.924765625, 'MI': 0.11032950758978156, 'FA': 0.055534987041836355, 'CF': 0.09533506108848575}, batch size: 64, grad_norm: 1.3514022827148438, grad_scale: , lr: 0.00e+00, 
2024-12-23 00:17:00,212 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 00:18:36,949 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 37251,  validation: loss=0.1095, DER=0.1595, ACC=0.9532, MI=0.03817, FA=0.05604, CF=0.06528, over 0.00 frames. 
2024-12-23 00:18:36,949 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 00:18:37,747 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 37251,  validation: loss=0.1099, DER=0.1576, ACC=0.9531, MI=0.03785, FA=0.05592, CF=0.06381, over 0.00 frames. 
2024-12-23 00:18:37,747 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 00:28:57,823 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-37500.pt
2024-12-23 00:29:05,570 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-23 00:39:48,443 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 37751, num_updates: 36000, {'loss': 0.18467652797698975, 'DER': 0.2780245158998046, 'ACC': 0.920703125, 'MI': 0.13839047788239475, 'FA': 0.05702611476283532, 'CF': 0.08260792325457453}, batch size: 64, grad_norm: 1.0564533472061157, grad_scale: , lr: 0.00e+00, 
2024-12-23 00:39:48,444 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 00:39:48,445 (train_accelerate_ddp:699) INFO: [Train] - Epoch 18, batch_idx_train: 37751, num_updates: 36000, {'loss': 0.13908982276916504, 'DER': 0.2200467037901922, 'ACC': 0.9447265625, 'MI': 0.12412430393389617, 'FA': 0.061792707023531526, 'CF': 0.034129692832764506}, batch size: 64, grad_norm: 1.0564533472061157, grad_scale: , lr: 0.00e+00, 
2024-12-23 00:39:48,446 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 00:41:25,185 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 37751,  validation: loss=0.1098, DER=0.1592, ACC=0.9526, MI=0.03743, FA=0.05732, CF=0.06446, over 0.00 frames. 
2024-12-23 00:41:25,185 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 00:41:25,689 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 18, batch_idx_train: 37751,  validation: loss=0.1094, DER=0.1627, ACC=0.9521, MI=0.03816, FA=0.05797, CF=0.06661, over 0.00 frames. 
2024-12-23 00:41:25,690 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 00:45:36,779 (train_accelerate_ddp:710) INFO: end of epoch 18, batch_idx: 2102 batch_idx_train: 37853, {'loss': 0.15742644667625427, 'DER': 0.2541879096868172, 'ACC': 0.9294140625, 'MI': 0.12108521485797524, 'FA': 0.05826656955571741, 'CF': 0.07483612527312454}, batch size: 64, grad_norm: 1.054541826248169, grad_scale: , lr: 0.00e+00, 
2024-12-23 00:45:36,786 (train_accelerate_ddp:710) INFO: end of epoch 18, batch_idx: 2102 batch_idx_train: 37853, {'loss': 0.1573157161474228, 'DER': 0.24023581429624172, 'ACC': 0.935, 'MI': 0.1228813559322034, 'FA': 0.05103168754605748, 'CF': 0.06632277081798084}, batch size: 64, grad_norm: 1.054541826248169, grad_scale: , lr: 0.00e+00, 
2024-12-23 00:45:36,789 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-18.pt
2024-12-23 00:45:42,508 (train_accelerate_ddp:558) INFO:  end of epoch 18, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-18.pt 
2024-12-23 00:45:46,606 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 37854, num_updates: 36000, {'loss': 0.18623782694339752, 'DER': 0.30180678466076694, 'ACC': 0.916015625, 'MI': 0.15025811209439527, 'FA': 0.056969026548672565, 'CF': 0.09457964601769911}, batch size: 64, grad_norm: 0.9345365166664124, grad_scale: , lr: 0.00e+00, 
2024-12-23 00:45:46,613 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 37854, num_updates: 36000, {'loss': 0.16195067763328552, 'DER': 0.28033314404694304, 'ACC': 0.928125, 'MI': 0.10883967442740868, 'FA': 0.10353965549876964, 'CF': 0.06795381412076472}, batch size: 64, grad_norm: 0.9345365166664124, grad_scale: , lr: 0.00e+00, 
2024-12-23 01:06:50,391 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 38354, num_updates: 36500, {'loss': 0.2051539421081543, 'DER': 0.34517766497461927, 'ACC': 0.9015234375, 'MI': 0.17331399564902103, 'FA': 0.06000725163161712, 'CF': 0.11185641769398115}, batch size: 64, grad_norm: 1.483957290649414, grad_scale: , lr: 0.00e+00, 
2024-12-23 01:06:50,392 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 01:06:50,396 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 38354, num_updates: 36500, {'loss': 0.21526387333869934, 'DER': 0.33254545454545453, 'ACC': 0.8998046875, 'MI': 0.13927272727272727, 'FA': 0.059454545454545454, 'CF': 0.1338181818181818}, batch size: 64, grad_norm: 1.483957290649414, grad_scale: , lr: 0.00e+00, 
2024-12-23 01:06:50,397 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 01:08:27,458 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 38354,  validation: loss=0.1099, DER=0.1609, ACC=0.9528, MI=0.0394, FA=0.05591, CF=0.06563, over 0.00 frames. 
2024-12-23 01:08:27,459 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 01:08:27,739 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 38354,  validation: loss=0.1102, DER=0.1582, ACC=0.9529, MI=0.03883, FA=0.05534, CF=0.06407, over 0.00 frames. 
2024-12-23 01:08:27,739 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 01:29:28,097 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 38854, num_updates: 37000, {'loss': 0.15828581154346466, 'DER': 0.25337357954545453, 'ACC': 0.9295703125, 'MI': 0.14311079545454544, 'FA': 0.043501420454545456, 'CF': 0.06676136363636363}, batch size: 64, grad_norm: 1.1138622760772705, grad_scale: , lr: 0.00e+00, 
2024-12-23 01:29:28,098 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 01:29:28,098 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 38854, num_updates: 37000, {'loss': 0.16479969024658203, 'DER': 0.28086253369272235, 'ACC': 0.9240625, 'MI': 0.15795148247978436, 'FA': 0.05444743935309973, 'CF': 0.06846361185983828}, batch size: 64, grad_norm: 1.1138622760772705, grad_scale: , lr: 0.00e+00, 
2024-12-23 01:29:28,098 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 01:31:05,230 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 38854,  validation: loss=0.1093, DER=0.1613, ACC=0.9525, MI=0.03972, FA=0.05517, CF=0.06642, over 0.00 frames. 
2024-12-23 01:31:05,231 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 01:31:05,862 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 38854,  validation: loss=0.1098, DER=0.1583, ACC=0.9528, MI=0.03922, FA=0.05464, CF=0.06445, over 0.00 frames. 
2024-12-23 01:31:05,862 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 01:37:12,753 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-39000.pt
2024-12-23 01:37:20,354 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-23 01:37:20,489 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-23 01:52:26,677 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 39354, num_updates: 37500, {'loss': 0.15675288438796997, 'DER': 0.22427581304425093, 'ACC': 0.9332421875, 'MI': 0.11142704816065399, 'FA': 0.03341034298915941, 'CF': 0.07943842189443753}, batch size: 64, grad_norm: 1.108825445175171, grad_scale: , lr: 0.00e+00, 
2024-12-23 01:52:26,678 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 01:52:26,684 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 39354, num_updates: 37500, {'loss': 0.18318192660808563, 'DER': 0.2731856654060868, 'ACC': 0.920078125, 'MI': 0.1280388978930308, 'FA': 0.04988294615523141, 'CF': 0.0952638213578246}, batch size: 64, grad_norm: 1.108825445175171, grad_scale: , lr: 0.00e+00, 
2024-12-23 01:52:26,684 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 01:54:04,088 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 39354,  validation: loss=0.1101, DER=0.1598, ACC=0.9526, MI=0.03566, FA=0.05966, CF=0.06447, over 0.00 frames. 
2024-12-23 01:54:04,088 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 01:54:04,255 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 39354,  validation: loss=0.1096, DER=0.163, ACC=0.952, MI=0.03634, FA=0.0599, CF=0.06674, over 0.00 frames. 
2024-12-23 01:54:04,255 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 02:15:12,794 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 39854, num_updates: 38000, {'loss': 0.21124877035617828, 'DER': 0.34044561933534745, 'ACC': 0.9053515625, 'MI': 0.12651057401812688, 'FA': 0.09686555891238671, 'CF': 0.11706948640483383}, batch size: 64, grad_norm: 1.2240325212478638, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:15:12,794 (train_accelerate_ddp:699) INFO: [Train] - Epoch 19, batch_idx_train: 39854, num_updates: 38000, {'loss': 0.1637578010559082, 'DER': 0.2606104388658118, 'ACC': 0.9263671875, 'MI': 0.12317139245078562, 'FA': 0.057612425501173924, 'CF': 0.07982662091385227}, batch size: 64, grad_norm: 1.2240325212478638, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:15:12,795 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 02:15:12,795 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 02:16:50,205 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 39854,  validation: loss=0.1093, DER=0.16, ACC=0.9531, MI=0.03908, FA=0.0555, CF=0.06539, over 0.00 frames. 
2024-12-23 02:16:50,205 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 02:16:50,507 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 19, batch_idx_train: 39854,  validation: loss=0.1096, DER=0.1583, ACC=0.9529, MI=0.03885, FA=0.0553, CF=0.06414, over 0.00 frames. 
2024-12-23 02:16:50,508 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 02:21:13,407 (train_accelerate_ddp:710) INFO: end of epoch 19, batch_idx: 2102 batch_idx_train: 39956, {'loss': 0.16807608306407928, 'DER': 0.2946411298123428, 'ACC': 0.9304296875, 'MI': 0.147030373379764, 'FA': 0.09769781389050107, 'CF': 0.04991294254207777}, batch size: 64, grad_norm: 1.1692967414855957, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:21:13,407 (train_accelerate_ddp:710) INFO: end of epoch 19, batch_idx: 2102 batch_idx_train: 39956, {'loss': 0.18881510198116302, 'DER': 0.2563325008919015, 'ACC': 0.921015625, 'MI': 0.10524438102033536, 'FA': 0.04673564038530146, 'CF': 0.10435247948626472}, batch size: 64, grad_norm: 1.1692967414855957, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:21:13,410 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-19.pt
2024-12-23 02:21:19,111 (train_accelerate_ddp:558) INFO:  end of epoch 19, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-19.pt 
2024-12-23 02:21:24,120 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 39957, num_updates: 38000, {'loss': 0.17470300197601318, 'DER': 0.30997056659308314, 'ACC': 0.9138671875, 'MI': 0.1547093451066961, 'FA': 0.059602649006622516, 'CF': 0.09565857247976453}, batch size: 64, grad_norm: 0.9578981399536133, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:21:24,121 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 39957, num_updates: 38000, {'loss': 0.147813081741333, 'DER': 0.22915592229671652, 'ACC': 0.9334765625, 'MI': 0.1430290527763452, 'FA': 0.022520199415506276, 'CF': 0.06360667010486505}, batch size: 64, grad_norm: 0.9578981399536133, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:42:34,824 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 40457, num_updates: 38500, {'loss': 0.16853348910808563, 'DER': 0.2540814411709514, 'ACC': 0.934296875, 'MI': 0.12535184837680616, 'FA': 0.06717958341152186, 'CF': 0.06155000938262338}, batch size: 64, grad_norm: 1.118040919303894, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:42:34,825 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 02:42:34,828 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 40457, num_updates: 38500, {'loss': 0.1628960818052292, 'DER': 0.27512653651482283, 'ACC': 0.919296875, 'MI': 0.1346710050614606, 'FA': 0.04211858279103398, 'CF': 0.09833694866232827}, batch size: 64, grad_norm: 1.118040919303894, grad_scale: , lr: 0.00e+00, 
2024-12-23 02:42:34,828 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 02:44:11,903 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 40457,  validation: loss=0.1099, DER=0.1624, ACC=0.9521, MI=0.0366, FA=0.05849, CF=0.0673, over 0.00 frames. 
2024-12-23 02:44:11,904 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 02:44:13,173 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 40457,  validation: loss=0.1104, DER=0.1593, ACC=0.9524, MI=0.03599, FA=0.05798, CF=0.06533, over 0.00 frames. 
2024-12-23 02:44:13,173 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 02:46:00,127 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-40500.pt
2024-12-23 02:46:07,927 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-23 02:46:08,061 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-23 03:05:34,478 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 40957, num_updates: 39000, {'loss': 0.1957039088010788, 'DER': 0.28084714548802947, 'ACC': 0.9194140625, 'MI': 0.11878453038674033, 'FA': 0.06298342541436464, 'CF': 0.09907918968692449}, batch size: 64, grad_norm: 1.63341224193573, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:05:34,478 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 03:05:34,484 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 40957, num_updates: 39000, {'loss': 0.202816441655159, 'DER': 0.3302752293577982, 'ACC': 0.911171875, 'MI': 0.12258442318953738, 'FA': 0.09408549677923092, 'CF': 0.11360530938902987}, batch size: 64, grad_norm: 1.63341224193573, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:05:34,484 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 03:07:11,466 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 40957,  validation: loss=0.1102, DER=0.1618, ACC=0.9521, MI=0.03775, FA=0.05615, CF=0.06785, over 0.00 frames. 
2024-12-23 03:07:11,467 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 03:07:12,101 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 40957,  validation: loss=0.1106, DER=0.1589, ACC=0.9524, MI=0.03717, FA=0.05581, CF=0.06597, over 0.00 frames. 
2024-12-23 03:07:12,102 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 03:28:17,627 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 41457, num_updates: 39500, {'loss': 0.14303044974803925, 'DER': 0.23696857670979668, 'ACC': 0.94, 'MI': 0.12532347504621072, 'FA': 0.06469500924214418, 'CF': 0.046950092421441776}, batch size: 64, grad_norm: 0.9710608720779419, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:28:17,627 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 03:28:17,633 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 41457, num_updates: 39500, {'loss': 0.18506087362766266, 'DER': 0.2893242995788317, 'ACC': 0.9173046875, 'MI': 0.1340413843618385, 'FA': 0.05694927668925105, 'CF': 0.09833363852774217}, batch size: 64, grad_norm: 0.9710608720779419, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:28:17,633 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 03:29:55,180 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 41457,  validation: loss=0.1096, DER=0.1616, ACC=0.9525, MI=0.03744, FA=0.05801, CF=0.06612, over 0.00 frames. 
2024-12-23 03:29:55,181 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 03:29:55,403 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 41457,  validation: loss=0.1101, DER=0.1593, ACC=0.9526, MI=0.03691, FA=0.0578, CF=0.06459, over 0.00 frames. 
2024-12-23 03:29:55,403 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 03:51:08,138 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 41957, num_updates: 40000, {'loss': 0.19611041247844696, 'DER': 0.3041306436119116, 'ACC': 0.9155078125, 'MI': 0.11392891450528338, 'FA': 0.0787704130643612, 'CF': 0.11143131604226705}, batch size: 64, grad_norm: 1.0926247835159302, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:51:08,138 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 03:51:08,142 (train_accelerate_ddp:699) INFO: [Train] - Epoch 20, batch_idx_train: 41957, num_updates: 40000, {'loss': 0.11839982122182846, 'DER': 0.18170197224251278, 'ACC': 0.951015625, 'MI': 0.08016800584368151, 'FA': 0.05423666910153396, 'CF': 0.0472972972972973}, batch size: 64, grad_norm: 1.0926247835159302, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:51:08,143 (train_accelerate_ddp:717) INFO: Computing validation loss
2024-12-23 03:52:45,210 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 41957,  validation: loss=0.1092, DER=0.1594, ACC=0.9531, MI=0.03724, FA=0.05646, CF=0.06573, over 0.00 frames. 
2024-12-23 03:52:45,211 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 03:52:45,625 (train_accelerate_ddp:725) INFO: [Eval] - Epoch 20, batch_idx_train: 41957,  validation: loss=0.1098, DER=0.1576, ACC=0.953, MI=0.03674, FA=0.05621, CF=0.06463, over 0.00 frames. 
2024-12-23 03:52:45,626 (train_accelerate_ddp:729) INFO: Maximum memory allocated so far is 14198MB
2024-12-23 03:54:39,124 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/checkpoint-42000.pt
2024-12-23 03:54:46,692 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-23 03:57:14,652 (train_accelerate_ddp:710) INFO: end of epoch 20, batch_idx: 2102 batch_idx_train: 42059, {'loss': 0.17716297507286072, 'DER': 0.2766034327009937, 'ACC': 0.9209765625, 'MI': 0.12339656729900632, 'FA': 0.06431797651309847, 'CF': 0.08888888888888889}, batch size: 64, grad_norm: 1.0539990663528442, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:57:14,653 (train_accelerate_ddp:710) INFO: end of epoch 20, batch_idx: 2102 batch_idx_train: 42059, {'loss': 0.14913661777973175, 'DER': 0.2348432055749129, 'ACC': 0.9290234375, 'MI': 0.12073170731707317, 'FA': 0.03240418118466899, 'CF': 0.08170731707317073}, batch size: 64, grad_norm: 1.0539990663528442, grad_scale: , lr: 0.00e+00, 
2024-12-23 03:57:14,655 (train_accelerate_ddp:1059) INFO: Done!
2024-12-23 03:57:14,656 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-20.pt
hltsz02:2630241:2630281 [1] NCCL INFO [Service thread] Connection closed by localRank 1
hltsz02:2630241:2630241 [1] NCCL INFO comm 0x1e9a1780 rank 1 nranks 2 cudaDev 1 busId 88000 - Abort COMPLETE
2024-12-23 03:57:20,412 (train_accelerate_ddp:558) INFO:  end of epoch 20, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/epoch-20.pt 
2024-12-23 03:57:20,414 (train_accelerate_ddp:1059) INFO: Done!
hltsz02:2630240:2630282 [0] NCCL INFO [Service thread] Connection closed by localRank 0
hltsz02:2630240:2630240 [0] NCCL INFO comm 0x1e729d40 rank 0 nranks 2 cudaDev 0 busId 3e000 - Abort COMPLETE
2024-12-23 03:57:55,699 (infer:252) INFO: infer data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=1, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/mntcephfs/lee_dataset/asr/musan', rir_path='/mntcephfs/lee_dataset/asr/RIRS_NOISES')
2024-12-23 03:57:55,699 (infer:253) INFO: currently, it will infer dev set.
  0%|          | 0/38 [00:00<?, ?it/s] 61%|██████    | 23/38 [00:00<00:00, 228.75it/s]100%|██████████| 38/38 [00:00<00:00, 234.87it/s]
2024-12-23 03:57:55,887 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-23 03:57:55,888 (ts_vad_dataset:160) INFO: loaded sentence=35613, shortest sent=3200.0, longest sent=64000.0, rs_len=4, segment_shift=1,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
2024-12-23 03:57:56,228 (infer:275) INFO: Device: cuda:0
2024-12-23 03:57:56,228 (infer:290) INFO: infer model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=4, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
2024-12-23 03:58:00,768 (infer:201) INFO: params.model_file: /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/best-valid-der.pt
2024-12-23 04:06:30,842 (infer:89) INFO: frame_len: 0.04!!
self.wavlm_fuse_feat_post_norm: False
Model DER:  0.16016853167687578
Model ACC:  0.9529909188308845
  0%|          | 0/38 [00:00<?, ?it/s]  3%|▎         | 1/38 [00:01<00:38,  1.03s/it]  5%|▌         | 2/38 [00:02<00:37,  1.03s/it]  8%|▊         | 3/38 [00:03<00:36,  1.05s/it] 11%|█         | 4/38 [00:04<00:35,  1.04s/it] 13%|█▎        | 5/38 [00:05<00:34,  1.04s/it] 16%|█▌        | 6/38 [00:06<00:40,  1.27s/it] 18%|█▊        | 7/38 [00:07<00:37,  1.20s/it] 21%|██        | 8/38 [00:09<00:34,  1.16s/it] 24%|██▎       | 9/38 [00:10<00:32,  1.11s/it] 26%|██▋       | 10/38 [00:11<00:30,  1.08s/it] 29%|██▉       | 11/38 [00:12<00:28,  1.06s/it] 32%|███▏      | 12/38 [00:13<00:27,  1.06s/it] 34%|███▍      | 13/38 [00:14<00:26,  1.07s/it] 37%|███▋      | 14/38 [00:15<00:25,  1.07s/it] 39%|███▉      | 15/38 [00:16<00:24,  1.06s/it] 42%|████▏     | 16/38 [00:17<00:23,  1.06s/it] 45%|████▍     | 17/38 [00:18<00:22,  1.07s/it] 47%|████▋     | 18/38 [00:20<00:25,  1.28s/it] 50%|█████     | 19/38 [00:21<00:22,  1.20s/it] 53%|█████▎    | 20/38 [00:22<00:20,  1.15s/it] 55%|█████▌    | 21/38 [00:23<00:18,  1.11s/it] 58%|█████▊    | 22/38 [00:24<00:17,  1.09s/it] 61%|██████    | 23/38 [00:25<00:16,  1.08s/it] 63%|██████▎   | 24/38 [00:26<00:14,  1.06s/it] 66%|██████▌   | 25/38 [00:27<00:13,  1.06s/it] 68%|██████▊   | 26/38 [00:28<00:12,  1.06s/it] 71%|███████   | 27/38 [00:29<00:11,  1.05s/it] 74%|███████▎  | 28/38 [00:30<00:10,  1.03s/it] 76%|███████▋  | 29/38 [00:31<00:09,  1.03s/it] 79%|███████▉  | 30/38 [00:32<00:08,  1.03s/it] 82%|████████▏ | 31/38 [00:33<00:07,  1.03s/it] 84%|████████▍ | 32/38 [00:35<00:07,  1.21s/it] 87%|████████▋ | 33/38 [00:36<00:05,  1.17s/it] 89%|████████▉ | 34/38 [00:37<00:04,  1.13s/it] 92%|█████████▏| 35/38 [00:38<00:03,  1.10s/it] 95%|█████████▍| 36/38 [00:39<00:02,  1.08s/it] 97%|█████████▋| 37/38 [00:40<00:01,  1.07s/it]100%|██████████| 38/38 [00:41<00:00,  1.07s/it]100%|██████████| 38/38 [00:41<00:00,  1.09s/it]
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
Eval for threshold 0.2 DER=28.52, miss=0.40, falarm=26.64, confusion=1.48


Eval for threshold 0.3 DER=23.60, miss=0.74, falarm=20.66, confusion=2.20


Eval for threshold 0.35 DER=21.53, miss=0.98, falarm=17.62, confusion=2.93


Eval for threshold 0.4 DER=19.39, miss=1.34, falarm=13.88, confusion=4.17


Eval for threshold 0.45 DER=17.34, miss=1.90, falarm=10.07, confusion=5.37


Eval for threshold 0.5 DER=16.06, miss=2.91, falarm=7.17, confusion=5.98


Eval for threshold 0.55 DER=16.13, miss=4.78, falarm=5.62, confusion=5.73


Eval for threshold 0.6 DER=17.04, miss=7.30, falarm=4.94, confusion=4.80


Eval for threshold 0.7 DER=20.09, miss=13.25, falarm=4.08, confusion=2.75


Eval for threshold 0.8 DER=23.93, miss=19.52, falarm=3.18, confusion=1.24


2024-12-23 04:07:30,116 (infer:252) INFO: infer data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=1, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/mntcephfs/lee_dataset/asr/musan', rir_path='/mntcephfs/lee_dataset/asr/RIRS_NOISES')
2024-12-23 04:07:30,117 (infer:253) INFO: currently, it will infer test set.
  0%|          | 0/86 [00:00<?, ?it/s] 31%|███▏      | 27/86 [00:00<00:00, 266.84it/s] 63%|██████▎   | 54/86 [00:00<00:00, 261.09it/s] 94%|█████████▍| 81/86 [00:00<00:00, 163.71it/s]100%|██████████| 86/86 [00:00<00:00, 186.11it/s]
2024-12-23 04:07:30,627 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-23 04:07:30,630 (ts_vad_dataset:160) INFO: loaded sentence=74337, shortest sent=640.0, longest sent=64000.0, rs_len=4, segment_shift=1,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
2024-12-23 04:07:30,925 (infer:275) INFO: Device: cuda:0
2024-12-23 04:07:30,925 (infer:290) INFO: infer model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=4, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
2024-12-23 04:07:32,480 (infer:201) INFO: params.model_file: /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr1e5_both_mamba_4layers_mamba/best-valid-der.pt
2024-12-23 04:23:34,574 (infer:89) INFO: frame_len: 0.04!!
self.wavlm_fuse_feat_post_norm: False
Model DER:  0.17179977488770812
Model ACC:  0.9520501967884138
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:01<01:25,  1.01s/it]  2%|▏         | 2/86 [00:02<01:24,  1.01s/it]  3%|▎         | 3/86 [00:02<01:03,  1.30it/s]  5%|▍         | 4/86 [00:02<00:54,  1.51it/s]  6%|▌         | 5/86 [00:03<00:47,  1.70it/s]  7%|▋         | 6/86 [00:03<00:43,  1.84it/s]  8%|▊         | 7/86 [00:04<00:41,  1.89it/s]  9%|▉         | 8/86 [00:04<00:40,  1.92it/s] 10%|█         | 9/86 [00:05<00:39,  1.97it/s] 12%|█▏        | 10/86 [00:05<00:37,  2.03it/s] 13%|█▎        | 11/86 [00:06<00:36,  2.03it/s] 14%|█▍        | 12/86 [00:06<00:36,  2.04it/s] 15%|█▌        | 13/86 [00:08<01:03,  1.16it/s] 16%|█▋        | 14/86 [00:09<00:54,  1.33it/s] 17%|█▋        | 15/86 [00:10<01:00,  1.18it/s] 19%|█▊        | 16/86 [00:11<01:03,  1.11it/s] 20%|█▉        | 17/86 [00:12<01:04,  1.07it/s] 21%|██        | 18/86 [00:13<01:05,  1.04it/s] 22%|██▏       | 19/86 [00:14<01:04,  1.04it/s] 23%|██▎       | 20/86 [00:15<01:02,  1.05it/s] 24%|██▍       | 21/86 [00:16<01:02,  1.04it/s] 26%|██▌       | 22/86 [00:17<01:02,  1.03it/s] 27%|██▋       | 23/86 [00:18<01:02,  1.01it/s] 28%|██▊       | 24/86 [00:19<01:01,  1.00it/s] 29%|██▉       | 25/86 [00:20<01:00,  1.01it/s] 30%|███       | 26/86 [00:21<00:59,  1.01it/s] 31%|███▏      | 27/86 [00:22<00:59,  1.00s/it] 33%|███▎      | 28/86 [00:23<00:58,  1.01s/it] 34%|███▎      | 29/86 [00:24<00:57,  1.01s/it] 35%|███▍      | 30/86 [00:25<00:56,  1.01s/it] 36%|███▌      | 31/86 [00:26<00:55,  1.01s/it] 37%|███▋      | 32/86 [00:27<00:54,  1.01s/it] 38%|███▊      | 33/86 [00:28<00:53,  1.01s/it] 40%|███▉      | 34/86 [00:29<00:52,  1.01s/it] 41%|████      | 35/86 [00:30<00:51,  1.02s/it] 42%|████▏     | 36/86 [00:31<00:50,  1.02s/it] 43%|████▎     | 37/86 [00:32<00:49,  1.02s/it] 44%|████▍     | 38/86 [00:33<00:48,  1.02s/it] 45%|████▌     | 39/86 [00:35<01:04,  1.38s/it] 47%|████▋     | 40/86 [00:36<00:58,  1.27s/it] 48%|████▊     | 41/86 [00:37<00:53,  1.18s/it] 49%|████▉     | 42/86 [00:38<00:49,  1.12s/it] 50%|█████     | 43/86 [00:39<00:46,  1.08s/it] 51%|█████     | 44/86 [00:40<00:44,  1.05s/it] 52%|█████▏    | 45/86 [00:41<00:42,  1.04s/it] 53%|█████▎    | 46/86 [00:42<00:41,  1.03s/it] 55%|█████▍    | 47/86 [00:43<00:39,  1.02s/it] 56%|█████▌    | 48/86 [00:44<00:38,  1.01s/it] 57%|█████▋    | 49/86 [00:45<00:37,  1.02s/it] 58%|█████▊    | 50/86 [00:46<00:37,  1.03s/it] 59%|█████▉    | 51/86 [00:47<00:35,  1.03s/it] 60%|██████    | 52/86 [00:48<00:34,  1.02s/it] 62%|██████▏   | 53/86 [00:49<00:33,  1.01s/it] 63%|██████▎   | 54/86 [00:50<00:32,  1.01s/it] 64%|██████▍   | 55/86 [00:51<00:31,  1.01s/it] 65%|██████▌   | 56/86 [00:52<00:30,  1.01s/it] 66%|██████▋   | 57/86 [00:53<00:29,  1.02s/it] 67%|██████▋   | 58/86 [00:54<00:28,  1.02s/it] 69%|██████▊   | 59/86 [00:55<00:27,  1.02s/it] 70%|██████▉   | 60/86 [00:56<00:26,  1.02s/it] 71%|███████   | 61/86 [00:57<00:25,  1.02s/it] 72%|███████▏  | 62/86 [00:58<00:24,  1.03s/it] 73%|███████▎  | 63/86 [01:00<00:31,  1.36s/it] 74%|███████▍  | 64/86 [01:01<00:27,  1.25s/it] 76%|███████▌  | 65/86 [01:02<00:24,  1.17s/it] 77%|███████▋  | 66/86 [01:03<00:22,  1.11s/it] 78%|███████▊  | 67/86 [01:04<00:20,  1.08s/it] 79%|███████▉  | 68/86 [01:05<00:19,  1.06s/it] 80%|████████  | 69/86 [01:06<00:17,  1.05s/it] 81%|████████▏ | 70/86 [01:07<00:16,  1.04s/it] 83%|████████▎ | 71/86 [01:08<00:15,  1.03s/it] 84%|████████▎ | 72/86 [01:09<00:14,  1.03s/it] 85%|████████▍ | 73/86 [01:11<00:13,  1.04s/it] 86%|████████▌ | 74/86 [01:12<00:12,  1.05s/it] 87%|████████▋ | 75/86 [01:13<00:11,  1.04s/it] 88%|████████▊ | 76/86 [01:14<00:10,  1.04s/it] 90%|████████▉ | 77/86 [01:15<00:09,  1.03s/it] 91%|█████████ | 78/86 [01:16<00:08,  1.02s/it] 92%|█████████▏| 79/86 [01:17<00:07,  1.02s/it] 93%|█████████▎| 80/86 [01:18<00:06,  1.02s/it] 94%|█████████▍| 81/86 [01:19<00:05,  1.01s/it] 95%|█████████▌| 82/86 [01:20<00:04,  1.01s/it] 97%|█████████▋| 83/86 [01:21<00:03,  1.02s/it] 98%|█████████▊| 84/86 [01:22<00:02,  1.03s/it] 99%|█████████▉| 85/86 [01:23<00:01,  1.02s/it]100%|██████████| 86/86 [01:24<00:00,  1.02s/it]100%|██████████| 86/86 [01:24<00:00,  1.02it/s]
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
Eval for threshold 0.2 DER=26.22, miss=0.52, falarm=23.17, confusion=2.53


Eval for threshold 0.3 DER=21.62, miss=0.98, falarm=17.18, confusion=3.46


Eval for threshold 0.35 DER=19.84, miss=1.30, falarm=14.58, confusion=3.96


Eval for threshold 0.4 DER=18.29, miss=1.74, falarm=11.99, confusion=4.56


Eval for threshold 0.45 DER=16.98, miss=2.39, falarm=9.49, confusion=5.10


Eval for threshold 0.5 DER=16.09, miss=3.52, falarm=7.18, confusion=5.40


Eval for threshold 0.55 DER=16.21, miss=5.41, falarm=5.65, confusion=5.15


Eval for threshold 0.6 DER=17.10, miss=7.86, falarm=4.96, confusion=4.29


Eval for threshold 0.7 DER=19.52, miss=12.48, falarm=4.11, confusion=2.93


Eval for threshold 0.8 DER=22.95, miss=17.76, falarm=3.26, confusion=1.92


