The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-12-19 09:09:30,130 (train_accelerate_ddp:853) INFO: params: {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_der': inf, 'best_valid_der': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 500, 'reset_interval': 200, 'valid_interval': 500, 'batch_size': 64, 'verbose': 1, 'world_size': 2, 'tensorboard': True, 'num_epochs': 20, 'max_updates': 40000, 'warmup_updates': 4000, 'freeze_updates': 4000, 'start_batch': 0, 'start_epoch': 1, 'seed': 1337, 'exp_dir': PosixPath('/data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba'), 'save_every_n': 1500, 'keep_last_k': 1, 'keep_last_epoch': 1, 'grad_clip': True, 'feature_grad_mult': 0.1, 'lr': 1e-05, 'average_period': 200, 'train_on_average': False, 'musan_path': '/data/maduo/datasets/musan', 'rir_path': '/data/maduo/datasets/RIRS_NOISES', 'spk_path': '/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', 'speaker_embedding_name_dir': 'cam++_zh-cn_200k_feature_dir', 'data_dir': '/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', 'dataset_name': 'magicdata-ramc', 'max_num_speaker': 4, 'speech_encoder_path': '/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', 'select_encoder_layer_nums': 6, 'wavlm_fuse_feat_post_norm': False, 'speech_encoder_config': '/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', 'speech_encoder_type': 'CAM++', 'speaker_embed_dim': 192, 'rs_len': 4, 'segment_shift': 2, 'single_backend_type': 'mamba', 'multi_backend_type': 'mamba', 'do_finetune': False, 'init_modules': None, 'finetune_ckpt': None}
2024-12-19 09:09:30,130 (train_accelerate_ddp:853) INFO: params: {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_der': inf, 'best_valid_der': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 500, 'reset_interval': 200, 'valid_interval': 500, 'batch_size': 64, 'verbose': 1, 'world_size': 2, 'tensorboard': True, 'num_epochs': 20, 'max_updates': 40000, 'warmup_updates': 4000, 'freeze_updates': 4000, 'start_batch': 0, 'start_epoch': 1, 'seed': 1337, 'exp_dir': PosixPath('/data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba'), 'save_every_n': 1500, 'keep_last_k': 1, 'keep_last_epoch': 1, 'grad_clip': True, 'feature_grad_mult': 0.1, 'lr': 1e-05, 'average_period': 200, 'train_on_average': False, 'musan_path': '/data/maduo/datasets/musan', 'rir_path': '/data/maduo/datasets/RIRS_NOISES', 'spk_path': '/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', 'speaker_embedding_name_dir': 'cam++_zh-cn_200k_feature_dir', 'data_dir': '/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', 'dataset_name': 'magicdata-ramc', 'max_num_speaker': 4, 'speech_encoder_path': '/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', 'select_encoder_layer_nums': 6, 'wavlm_fuse_feat_post_norm': False, 'speech_encoder_config': '/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', 'speech_encoder_type': 'CAM++', 'speaker_embed_dim': 192, 'rs_len': 4, 'segment_shift': 2, 'single_backend_type': 'mamba', 'multi_backend_type': 'mamba', 'do_finetune': False, 'init_modules': None, 'finetune_ckpt': None}
2024-12-19 09:09:30,132 (train_accelerate_ddp:869) INFO: data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=2, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/data/maduo/datasets/musan', rir_path='/data/maduo/datasets/RIRS_NOISES')
2024-12-19 09:09:30,132 (train_accelerate_ddp:869) INFO: data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=2, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/data/maduo/datasets/musan', rir_path='/data/maduo/datasets/RIRS_NOISES')
  0%|          | 0/38 [00:00<?, ?it/s]  0%|          | 0/38 [00:00<?, ?it/s] 63%|██████▎   | 24/38 [00:00<00:00, 238.70it/s] 71%|███████   | 27/38 [00:00<00:00, 260.94it/s]100%|██████████| 38/38 [00:00<00:00, 265.70it/s]
2024-12-19 09:09:30,408 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-19 09:09:30,409 (ts_vad_dataset:160) INFO: loaded sentence=17812, shortest sent=3840.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
100%|██████████| 38/38 [00:00<00:00, 230.45it/s]
2024-12-19 09:09:30,431 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-19 09:09:30,432 (ts_vad_dataset:160) INFO: loaded sentence=17812, shortest sent=3840.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
  0%|          | 0/578 [00:00<?, ?it/s]  0%|          | 0/578 [00:00<?, ?it/s]  4%|▍         | 26/578 [00:00<00:02, 255.21it/s]  4%|▍         | 26/578 [00:00<00:02, 256.99it/s]  9%|▉         | 54/578 [00:00<00:01, 265.67it/s]  9%|▉         | 54/578 [00:00<00:01, 267.11it/s] 14%|█▍        | 81/578 [00:00<00:03, 164.02it/s] 14%|█▍        | 81/578 [00:00<00:03, 165.30it/s] 19%|█▊        | 107/578 [00:00<00:02, 190.74it/s] 19%|█▊        | 108/578 [00:00<00:02, 194.22it/s] 23%|██▎       | 133/578 [00:00<00:02, 208.34it/s] 23%|██▎       | 134/578 [00:00<00:02, 211.73it/s] 28%|██▊       | 159/578 [00:00<00:01, 221.63it/s] 28%|██▊       | 160/578 [00:00<00:01, 225.73it/s] 32%|███▏      | 184/578 [00:00<00:01, 228.65it/s] 32%|███▏      | 185/578 [00:00<00:01, 232.66it/s] 36%|███▌      | 209/578 [00:00<00:01, 231.95it/s] 37%|███▋      | 211/578 [00:00<00:01, 239.67it/s] 41%|████      | 235/578 [00:01<00:01, 239.89it/s] 41%|████      | 237/578 [00:01<00:01, 241.68it/s] 45%|████▍     | 260/578 [00:01<00:01, 240.60it/s] 46%|████▌     | 263/578 [00:01<00:01, 246.07it/s] 49%|████▉     | 285/578 [00:01<00:01, 153.98it/s] 50%|█████     | 289/578 [00:01<00:01, 157.62it/s] 54%|█████▍    | 311/578 [00:01<00:01, 173.91it/s] 54%|█████▍    | 314/578 [00:01<00:01, 176.51it/s] 58%|█████▊    | 337/578 [00:01<00:01, 192.64it/s] 59%|█████▉    | 340/578 [00:01<00:01, 195.31it/s] 63%|██████▎   | 363/578 [00:01<00:01, 207.28it/s] 63%|██████▎   | 365/578 [00:01<00:01, 207.86it/s] 67%|██████▋   | 389/578 [00:01<00:00, 218.30it/s] 68%|██████▊   | 391/578 [00:01<00:00, 221.26it/s] 72%|███████▏  | 415/578 [00:01<00:00, 228.50it/s] 72%|███████▏  | 417/578 [00:01<00:00, 231.45it/s] 76%|███████▌  | 440/578 [00:02<00:00, 229.75it/s] 76%|███████▋  | 442/578 [00:02<00:00, 236.12it/s] 81%|████████  | 466/578 [00:02<00:00, 237.35it/s] 81%|████████  | 467/578 [00:02<00:00, 239.41it/s] 85%|████████▌ | 492/578 [00:02<00:00, 241.52it/s] 85%|████████▍ | 491/578 [00:02<00:00, 238.69it/s] 89%|████████▉ | 517/578 [00:02<00:00, 137.28it/s] 89%|████████▉ | 516/578 [00:02<00:00, 136.89it/s] 94%|█████████▍| 543/578 [00:02<00:00, 159.95it/s] 94%|█████████▎| 541/578 [00:02<00:00, 158.01it/s] 98%|█████████▊| 567/578 [00:02<00:00, 176.94it/s] 98%|█████████▊| 566/578 [00:02<00:00, 176.26it/s]100%|██████████| 578/578 [00:02<00:00, 201.26it/s]
100%|██████████| 578/578 [00:02<00:00, 198.85it/s]
2024-12-19 09:09:33,534 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-19 09:09:33,541 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-19 09:09:33,545 (ts_vad_dataset:160) INFO: loaded sentence=269074, shortest sent=48640.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=True, musan=True, noise_ratio=0.8, zero_ratio=0.3 
2024-12-19 09:09:33,545 (train_accelerate_ddp:904) INFO: The scale window is set to 8192.
2024-12-19 09:09:33,551 (ts_vad_dataset:160) INFO: loaded sentence=269074, shortest sent=48640.0, longest sent=64000.0, rs_len=4, segment_shift=2,  rir=True, musan=True, noise_ratio=0.8, zero_ratio=0.3 
2024-12-19 09:09:33,552 (train_accelerate_ddp:904) INFO: The scale window is set to 8192.
2024-12-19 09:09:34,008 (train_accelerate_ddp:940) INFO: model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
self.wavlm_fuse_feat_post_norm: False
2024-12-19 09:09:34,031 (train_accelerate_ddp:940) INFO: model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
self.wavlm_fuse_feat_post_norm: False
2024-12-19 09:09:34,928 (train_accelerate_ddp:943) INFO: model: TSVADModel(
  (rs_dropout): Dropout(p=0.1, inplace=False)
  (speech_encoder): CAMPPlus(
    (head): FCM(
      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layer1): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (layer2): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (xvector): Sequential(
      (tdnn): TDNNLayer(
        (linear): Conv1d(320, 128, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (block1): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(160, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(192, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(224, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit1): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block2): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (16): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (17): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (18): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (19): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (20): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (21): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (22): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (23): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit2): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block3): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit3): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (out_nonlinear): Sequential(
        (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (stats): StatsPool()
      (dense): DenseLayer(
        (linear): Conv1d(1024, 192, kernel_size=(1,), stride=(1,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
  (speech_down_or_up): Sequential(
    (0): Conv1d(512, 192, kernel_size=(5,), stride=(2,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (single_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (backend_down): Sequential(
    (0): Conv1d(3072, 384, kernel_size=(5,), stride=(1,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (multi_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
)
2024-12-19 09:09:34,930 (train_accelerate_ddp:945) INFO: Number of model parameters: 30437092
2024-12-19 09:09:34,981 (train_accelerate_ddp:943) INFO: model: TSVADModel(
  (rs_dropout): Dropout(p=0.1, inplace=False)
  (speech_encoder): CAMPPlus(
    (head): FCM(
      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layer1): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (layer2): Sequential(
        (0): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicResBlock(
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (shortcut): Sequential()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (xvector): Sequential(
      (tdnn): TDNNLayer(
        (linear): Conv1d(320, 128, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (block1): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(160, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(192, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(224, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit1): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block2): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(288, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(320, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(352, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(416, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(448, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(480, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (16): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (17): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (18): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (19): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (20): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (21): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (22): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (23): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit2): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (block3): CAMDenseTDNNBlock(
        (0): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (1): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(544, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (2): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(576, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (3): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(608, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (4): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(640, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (5): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(672, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (6): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(704, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (7): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(736, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (8): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(768, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (9): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(800, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (10): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(832, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (11): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(864, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (12): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(896, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (13): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(928, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (14): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(960, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
        (15): CAMDenseTDNNLayer(
          (nonlinear1): Sequential(
            (batchnorm): BatchNorm1d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (linear1): Conv1d(992, 128, kernel_size=(1,), stride=(1,), bias=False)
          (nonlinear2): Sequential(
            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (cam_layer): CAMLayer(
            (linear_local): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)
            (linear1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (relu): ReLU(inplace=True)
            (linear2): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
            (sigmoid): Sigmoid()
          )
        )
      )
      (transit3): TransitLayer(
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (linear): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)
      )
      (out_nonlinear): Sequential(
        (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (stats): StatsPool()
      (dense): DenseLayer(
        (linear): Conv1d(1024, 192, kernel_size=(1,), stride=(1,), bias=False)
        (nonlinear): Sequential(
          (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
  (speech_down_or_up): Sequential(
    (0): Conv1d(512, 192, kernel_size=(5,), stride=(2,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (single_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (backend_down): Sequential(
    (0): Conv1d(3072, 384, kernel_size=(5,), stride=(1,), padding=(2,))
    (1): BatchNorm1D(
      (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ReLU()
  )
  (multi_backend): MambaBlockV2(
    (forward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
    (backward_blocks): ModuleList(
      (0-1): 2 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=384, out_features=3072, bias=False)
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          (act): SiLU()
          (x_proj): Linear(in_features=1536, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=384, bias=False)
        )
      )
    )
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
)
2024-12-19 09:09:34,984 (train_accelerate_ddp:945) INFO: Number of model parameters: 30437092
hltsz02:2559199:2559199 [0] NCCL INFO Bootstrap : Using eno1:10.26.1.136<0>
hltsz02:2559199:2559199 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
hltsz02:2559199:2559199 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
hltsz02:2559199:2559199 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.6+cuda11.8
hltsz02:2559200:2559200 [1] NCCL INFO cudaDriverVersion 12050
hltsz02:2559200:2559200 [1] NCCL INFO Bootstrap : Using eno1:10.26.1.136<0>
hltsz02:2559200:2559200 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
hltsz02:2559200:2559200 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
hltsz02:2559200:2559233 [1] NCCL INFO Failed to open libibverbs.so[.1]
hltsz02:2559200:2559233 [1] NCCL INFO NET/Socket : Using [0]eno1:10.26.1.136<0> [1]eno2:192.168.1.102<0>
hltsz02:2559200:2559233 [1] NCCL INFO Using network Socket
hltsz02:2559199:2559232 [0] NCCL INFO Failed to open libibverbs.so[.1]
hltsz02:2559199:2559232 [0] NCCL INFO NET/Socket : Using [0]eno1:10.26.1.136<0> [1]eno2:192.168.1.102<0>
hltsz02:2559199:2559232 [0] NCCL INFO Using network Socket
hltsz02:2559199:2559232 [0] NCCL INFO comm 0xc139340 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3e000 commId 0xbe16e5806d5ebcaf - Init START
hltsz02:2559200:2559233 [1] NCCL INFO comm 0xbdf2f00 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 88000 commId 0xbe16e5806d5ebcaf - Init START
hltsz02:2559199:2559232 [0] NCCL INFO Setting affinity for GPU 0 to 78,00000000,00078000
hltsz02:2559200:2559233 [1] NCCL INFO Setting affinity for GPU 1 to 3c0000,00000003,c0000000
hltsz02:2559200:2559233 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
hltsz02:2559200:2559233 [1] NCCL INFO P2P Chunksize set to 131072
hltsz02:2559199:2559232 [0] NCCL INFO Channel 00/02 :    0   1
hltsz02:2559199:2559232 [0] NCCL INFO Channel 01/02 :    0   1
hltsz02:2559199:2559232 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
hltsz02:2559199:2559232 [0] NCCL INFO P2P Chunksize set to 131072
hltsz02:2559200:2559233 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
hltsz02:2559200:2559233 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
hltsz02:2559199:2559232 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
hltsz02:2559199:2559232 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
hltsz02:2559199:2559232 [0] NCCL INFO Connected all rings
hltsz02:2559199:2559232 [0] NCCL INFO Connected all trees
hltsz02:2559200:2559233 [1] NCCL INFO Connected all rings
hltsz02:2559200:2559233 [1] NCCL INFO Connected all trees
hltsz02:2559200:2559233 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
hltsz02:2559200:2559233 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
hltsz02:2559199:2559232 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
hltsz02:2559199:2559232 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
hltsz02:2559200:2559233 [1] NCCL INFO comm 0xbdf2f00 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 88000 commId 0xbe16e5806d5ebcaf - Init COMPLETE
hltsz02:2559199:2559232 [0] NCCL INFO comm 0xc139340 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3e000 commId 0xbe16e5806d5ebcaf - Init COMPLETE
2024-12-19 09:09:35,890 (train_accelerate_ddp:1024) INFO: start training from epoch 1
2024-12-19 09:09:35,890 (train_accelerate_ddp:1025) INFO: Train set grouped total_num_itrs = 2103
2024-12-19 09:09:35,890 (train_accelerate_ddp:1024) INFO: start training from epoch 1
2024-12-19 09:09:35,890 (train_accelerate_ddp:1025) INFO: Train set grouped total_num_itrs = 2103
2024-12-19 09:09:42,406 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 0, num_updates: 0, {'loss': 0.741353452205658, 'DER': 1.7336683417085428, 'ACC': 0.505, 'MI': 0.020638908829863602, 'FA': 1.1724694903086863, 'CF': 0.5405599425699928}, batch size: 64, grad_norm: 3.7207539081573486, grad_scale: , lr: 5.00e-09, 
2024-12-19 09:09:42,421 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 0, num_updates: 0, {'loss': 0.7341268658638, 'DER': 1.7916203188728217, 'ACC': 0.5128515625, 'MI': 0.020949202817945867, 'FA': 1.25027808676307, 'CF': 0.5203930292918058}, batch size: 64, grad_norm: 3.7207539081573486, grad_scale: , lr: 5.00e-09, 
2024-12-19 09:26:24,917 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 500, num_updates: 500, {'loss': 0.5061533451080322, 'DER': 0.9862708719851577, 'ACC': 0.7819921875, 'MI': 0.9326530612244898, 'FA': 0.0044526901669758815, 'CF': 0.04916512059369202}, batch size: 64, grad_norm: 1.0599397420883179, grad_scale: , lr: 2.51e-06, 
2024-12-19 09:26:24,917 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 500, num_updates: 500, {'loss': 0.5060632824897766, 'DER': 0.976594604252278, 'ACC': 0.779296875, 'MI': 0.941933178488476, 'FA': 0.0017866714311238164, 'CF': 0.03287475433267822}, batch size: 64, grad_norm: 1.0599397420883179, grad_scale: , lr: 2.51e-06, 
2024-12-19 09:26:24,918 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 09:26:24,918 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 09:27:45,451 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 500,  validation: loss=0.4361, DER=0.9978, ACC=0.7904, MI=0.9964, FA=5.175e-05, CF=0.001363, over 0.00 frames. 
2024-12-19 09:27:45,452 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 09:27:45,606 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 500,  validation: loss=0.4374, DER=0.9979, ACC=0.7903, MI=0.9965, FA=3.531e-05, CF=0.001376, over 0.00 frames. 
2024-12-19 09:27:45,606 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 09:44:07,868 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1000, num_updates: 1000, {'loss': 0.4630167782306671, 'DER': 0.9348720701377706, 'ACC': 0.782734375, 'MI': 0.8609769189479335, 'FA': 0.013598139202003937, 'CF': 0.06029701198783324}, batch size: 64, grad_norm: 0.692303478717804, grad_scale: , lr: 5.00e-06, 
2024-12-19 09:44:07,869 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1000, num_updates: 1000, {'loss': 0.4467390477657318, 'DER': 0.8948228882833787, 'ACC': 0.7898046875, 'MI': 0.8039963669391462, 'FA': 0.008174386920980926, 'CF': 0.08265213442325159}, batch size: 64, grad_norm: 0.692303478717804, grad_scale: , lr: 5.00e-06, 
2024-12-19 09:44:07,869 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 09:44:07,869 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 09:45:26,836 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1000,  validation: loss=0.3366, DER=0.7744, ACC=0.8009, MI=0.4705, FA=0.1352, CF=0.1686, over 0.00 frames. 
2024-12-19 09:45:26,837 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 09:45:27,641 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1000,  validation: loss=0.3382, DER=0.783, ACC=0.7994, MI=0.4816, FA=0.1311, CF=0.1702, over 0.00 frames. 
2024-12-19 09:45:27,641 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 10:01:47,392 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-1500.pt
2024-12-19 10:01:52,633 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 10:01:54,780 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1500, num_updates: 1500, {'loss': 0.4425767660140991, 'DER': 0.9344501092498179, 'ACC': 0.782857590966123, 'MI': 0.831755280407866, 'FA': 0.02876911871813547, 'CF': 0.07392571012381646}, batch size: 64, grad_norm: 0.6658484935760498, grad_scale: , lr: 7.51e-06, 
2024-12-19 10:01:54,780 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 1500, num_updates: 1500, {'loss': 0.4315064549446106, 'DER': 0.9168779427743571, 'ACC': 0.789375, 'MI': 0.8328504165157552, 'FA': 0.024447663889894965, 'CF': 0.05957986236870699}, batch size: 64, grad_norm: 0.6658484935760498, grad_scale: , lr: 7.51e-06, 
2024-12-19 10:01:54,780 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 10:01:54,780 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 10:03:14,244 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1500,  validation: loss=0.3014, DER=0.594, ACC=0.837, MI=0.2317, FA=0.1794, CF=0.1829, over 0.00 frames. 
2024-12-19 10:03:14,244 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 10:03:15,312 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 1500,  validation: loss=0.299, DER=0.5863, ACC=0.8417, MI=0.2288, FA=0.1902, CF=0.1673, over 0.00 frames. 
2024-12-19 10:03:15,312 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 10:20:07,068 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 2000, num_updates: 2000, {'loss': 0.39019298553466797, 'DER': 0.7820466786355476, 'ACC': 0.806328125, 'MI': 0.5956912028725314, 'FA': 0.07827648114901256, 'CF': 0.1080789946140036}, batch size: 64, grad_norm: 0.5598448514938354, grad_scale: , lr: 1.00e-05, 
2024-12-19 10:20:07,069 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 10:20:07,075 (train_accelerate_ddp:698) INFO: [Train] - Epoch 1, batch_idx_train: 2000, num_updates: 2000, {'loss': 0.3402203321456909, 'DER': 0.6611760538434289, 'ACC': 0.8311328125, 'MI': 0.47555791710945805, 'FA': 0.08111937654976975, 'CF': 0.1044987601842012}, batch size: 64, grad_norm: 0.5598448514938354, grad_scale: , lr: 1.00e-05, 
2024-12-19 10:20:07,075 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 10:21:27,927 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 2000,  validation: loss=0.2475, DER=0.5273, ACC=0.8654, MI=0.1333, FA=0.284, CF=0.11, over 0.00 frames. 
2024-12-19 10:21:27,928 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 10:21:28,471 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 1, batch_idx_train: 2000,  validation: loss=0.2483, DER=0.5287, ACC=0.8664, MI=0.1352, FA=0.2927, CF=0.1008, over 0.00 frames. 
2024-12-19 10:21:28,472 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 10:24:50,813 (train_accelerate_ddp:709) INFO: end of epoch 1, batch_idx: 2102 batch_idx_train: 2102, {'loss': 0.35333025455474854, 'DER': 0.7005019520356943, 'ACC': 0.8248828125, 'MI': 0.45231455660903513, 'FA': 0.11526306004833613, 'CF': 0.1329243353783231}, batch size: 64, grad_norm: 0.6528777480125427, grad_scale: , lr: 9.94e-06, 
2024-12-19 10:24:50,814 (train_accelerate_ddp:709) INFO: end of epoch 1, batch_idx: 2102 batch_idx_train: 2102, {'loss': 0.36013513803482056, 'DER': 0.7004574565416285, 'ACC': 0.82203125, 'MI': 0.45032021957914, 'FA': 0.11692589204025618, 'CF': 0.13321134492223238}, batch size: 64, grad_norm: 0.6528777480125427, grad_scale: , lr: 9.94e-06, 
2024-12-19 10:24:50,815 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-1.pt
2024-12-19 10:24:54,389 (train_accelerate_ddp:557) INFO:  end of epoch 1, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-1.pt 
2024-12-19 10:25:17,501 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2103, num_updates: 2000, {'loss': 0.4011947214603424, 'DER': 0.7774253731343284, 'ACC': 0.80984375, 'MI': 0.5628731343283582, 'FA': 0.08376865671641791, 'CF': 0.13078358208955224}, batch size: 64, grad_norm: 0.6701385974884033, grad_scale: , lr: 9.94e-06, 
2024-12-19 10:25:17,505 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2103, num_updates: 2000, {'loss': 0.3819065988063812, 'DER': 0.7469534050179212, 'ACC': 0.8065890261059872, 'MI': 0.524910394265233, 'FA': 0.08207885304659499, 'CF': 0.1399641577060932}, batch size: 64, grad_norm: 0.6701385974884033, grad_scale: , lr: 9.94e-06, 
2024-12-19 10:42:03,631 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2603, num_updates: 2500, {'loss': 0.3590470254421234, 'DER': 0.7072992700729926, 'ACC': 0.824453125, 'MI': 0.48923357664233574, 'FA': 0.1052919708029197, 'CF': 0.11277372262773723}, batch size: 64, grad_norm: 0.6380946040153503, grad_scale: , lr: 9.66e-06, 
2024-12-19 10:42:03,631 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 2603, num_updates: 2500, {'loss': 0.37227460741996765, 'DER': 0.726742064939803, 'ACC': 0.821015625, 'MI': 0.5171470266326158, 'FA': 0.10051076249543962, 'CF': 0.10908427581174754}, batch size: 64, grad_norm: 0.6380946040153503, grad_scale: , lr: 9.66e-06, 
2024-12-19 10:42:03,632 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 10:42:03,632 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 10:43:24,064 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 2603,  validation: loss=0.2402, DER=0.4868, ACC=0.865, MI=0.1156, FA=0.2224, CF=0.1488, over 0.00 frames. 
2024-12-19 10:43:24,065 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 10:43:24,379 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 2603,  validation: loss=0.2368, DER=0.4748, ACC=0.8689, MI=0.1166, FA=0.2113, CF=0.1469, over 0.00 frames. 
2024-12-19 10:43:24,380 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 10:56:41,350 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-3000.pt
2024-12-19 10:56:46,150 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 10:56:46,267 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 11:00:18,744 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3103, num_updates: 3000, {'loss': 0.32011353969573975, 'DER': 0.6562038404726735, 'ACC': 0.8403515625, 'MI': 0.4787666174298375, 'FA': 0.07902511078286559, 'CF': 0.09841211225997046}, batch size: 64, grad_norm: 0.5352737307548523, grad_scale: , lr: 9.39e-06, 
2024-12-19 11:00:18,745 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3103, num_updates: 3000, {'loss': 0.3691433072090149, 'DER': 0.751984126984127, 'ACC': 0.809453125, 'MI': 0.47799422799422797, 'FA': 0.1461038961038961, 'CF': 0.12788600288600288}, batch size: 64, grad_norm: 0.5352737307548523, grad_scale: , lr: 9.39e-06, 
2024-12-19 11:00:18,745 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 11:00:18,745 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 11:01:40,307 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3103,  validation: loss=0.2231, DER=0.4749, ACC=0.8788, MI=0.09559, FA=0.2812, CF=0.09812, over 0.00 frames. 
2024-12-19 11:01:40,308 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 11:01:40,574 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3103,  validation: loss=0.2254, DER=0.4815, ACC=0.878, MI=0.09374, FA=0.2958, CF=0.09191, over 0.00 frames. 
2024-12-19 11:01:40,575 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 11:18:24,479 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3603, num_updates: 3500, {'loss': 0.30109384655952454, 'DER': 0.6046468053213416, 'ACC': 0.8504296875, 'MI': 0.37343076634813566, 'FA': 0.11841858722128537, 'CF': 0.11279745175192056}, batch size: 64, grad_norm: 0.6523640751838684, grad_scale: , lr: 9.11e-06, 
2024-12-19 11:18:24,480 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 11:18:24,481 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 3603, num_updates: 3500, {'loss': 0.33659425377845764, 'DER': 0.6876135125317835, 'ACC': 0.8269921875, 'MI': 0.4386124228114784, 'FA': 0.13221939702143118, 'CF': 0.11678169269887395}, batch size: 64, grad_norm: 0.6523640751838684, grad_scale: , lr: 9.11e-06, 
2024-12-19 11:18:24,481 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 11:19:45,832 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3603,  validation: loss=0.2189, DER=0.368, ACC=0.8943, MI=0.1017, FA=0.1328, CF=0.1335, over 0.00 frames. 
2024-12-19 11:19:45,833 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 11:19:46,479 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 3603,  validation: loss=0.2163, DER=0.3597, ACC=0.898, MI=0.1001, FA=0.1379, CF=0.1217, over 0.00 frames. 
2024-12-19 11:19:46,480 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 5858MB
2024-12-19 11:37:07,377 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 4103, num_updates: 4000, {'loss': 0.3287387788295746, 'DER': 0.617737003058104, 'ACC': 0.8424609375, 'MI': 0.4079870480302213, 'FA': 0.10199676200755532, 'CF': 0.1077531930203274}, batch size: 64, grad_norm: 0.6582719683647156, grad_scale: , lr: 8.83e-06, 
2024-12-19 11:37:07,377 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 11:37:07,379 (train_accelerate_ddp:698) INFO: [Train] - Epoch 2, batch_idx_train: 4103, num_updates: 4000, {'loss': 0.3086746037006378, 'DER': 0.6008304748149486, 'ACC': 0.8535546875, 'MI': 0.4417764939519769, 'FA': 0.08304748149485466, 'CF': 0.07600649936811699}, batch size: 64, grad_norm: 0.6582719683647156, grad_scale: , lr: 8.83e-06, 
2024-12-19 11:37:07,379 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 11:38:27,668 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 4103,  validation: loss=0.2037, DER=0.3636, ACC=0.8989, MI=0.0653, FA=0.1831, CF=0.1152, over 0.00 frames. 
2024-12-19 11:38:27,669 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 11:38:28,823 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 2, batch_idx_train: 4103,  validation: loss=0.203, DER=0.3639, ACC=0.8997, MI=0.06426, FA=0.1923, CF=0.1073, over 0.00 frames. 
2024-12-19 11:38:28,823 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 11:42:16,039 (train_accelerate_ddp:709) INFO: end of epoch 2, batch_idx: 2102 batch_idx_train: 4205, {'loss': 0.2817360460758209, 'DER': 0.5686125852918877, 'ACC': 0.86328125, 'MI': 0.3459059893858984, 'FA': 0.12793783169067474, 'CF': 0.09476876421531463}, batch size: 64, grad_norm: 0.8708711862564087, grad_scale: , lr: 8.77e-06, 
2024-12-19 11:42:16,042 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-2.pt
2024-12-19 11:42:16,045 (train_accelerate_ddp:709) INFO: end of epoch 2, batch_idx: 2102 batch_idx_train: 4205, {'loss': 0.3529965877532959, 'DER': 0.6854174131135794, 'ACC': 0.8234328591527278, 'MI': 0.41795055535650305, 'FA': 0.1434969544965962, 'CF': 0.12396990326048012}, batch size: 64, grad_norm: 0.8708711862564087, grad_scale: , lr: 8.77e-06, 
2024-12-19 11:42:20,254 (train_accelerate_ddp:557) INFO:  end of epoch 2, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-2.pt 
2024-12-19 11:42:37,800 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4206, num_updates: 4000, {'loss': 0.3360898792743683, 'DER': 0.6513134220942786, 'ACC': 0.8343359375, 'MI': 0.47858942065491183, 'FA': 0.06099316300827636, 'CF': 0.11173083843109032}, batch size: 64, grad_norm: 0.7491105794906616, grad_scale: , lr: 8.77e-06, 
2024-12-19 11:42:37,802 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4206, num_updates: 4000, {'loss': 0.29283055663108826, 'DER': 0.590988882387361, 'ACC': 0.861171875, 'MI': 0.38014433391847086, 'FA': 0.1086405305246733, 'CF': 0.1022040179442169}, batch size: 64, grad_norm: 0.7491105794906616, grad_scale: , lr: 8.77e-06, 
2024-12-19 11:53:37,409 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-4500.pt
2024-12-19 11:53:42,922 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 11:53:43,030 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 12:01:30,207 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4706, num_updates: 4500, {'loss': 0.34400448203086853, 'DER': 0.6103081019399011, 'ACC': 0.85109375, 'MI': 0.39463674400912896, 'FA': 0.10098896918980602, 'CF': 0.11468238874096615}, batch size: 64, grad_norm: 0.7989495396614075, grad_scale: , lr: 8.50e-06, 
2024-12-19 12:01:30,207 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 4706, num_updates: 4500, {'loss': 0.2818632423877716, 'DER': 0.5093280202861801, 'ACC': 0.8660546875, 'MI': 0.3211374750950915, 'FA': 0.07643542836442674, 'CF': 0.11175511682666184}, batch size: 64, grad_norm: 0.7989495396614075, grad_scale: , lr: 8.50e-06, 
2024-12-19 12:01:30,208 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 12:01:30,208 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 12:02:50,778 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 4706,  validation: loss=0.1948, DER=0.3359, ACC=0.9026, MI=0.05371, FA=0.1574, CF=0.1248, over 0.00 frames. 
2024-12-19 12:02:50,779 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 12:02:51,210 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 4706,  validation: loss=0.1943, DER=0.3346, ACC=0.9034, MI=0.05291, FA=0.1617, CF=0.12, over 0.00 frames. 
2024-12-19 12:02:51,211 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 12:21:16,086 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5206, num_updates: 5000, {'loss': 0.274228572845459, 'DER': 0.4872812556377413, 'ACC': 0.8673828125, 'MI': 0.2771062601479343, 'FA': 0.08497203680317518, 'CF': 0.12520295868663178}, batch size: 64, grad_norm: 0.9667876958847046, grad_scale: , lr: 8.22e-06, 
2024-12-19 12:21:16,087 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 12:21:16,090 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5206, num_updates: 5000, {'loss': 0.2999701499938965, 'DER': 0.5459183673469388, 'ACC': 0.851328125, 'MI': 0.35942997888810696, 'FA': 0.06280788177339902, 'CF': 0.12368050668543279}, batch size: 64, grad_norm: 0.9667876958847046, grad_scale: , lr: 8.22e-06, 
2024-12-19 12:21:16,090 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 12:22:36,330 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5206,  validation: loss=0.1777, DER=0.3176, ACC=0.9119, MI=0.0408, FA=0.1777, CF=0.0991, over 0.00 frames. 
2024-12-19 12:22:36,331 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 12:22:37,288 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5206,  validation: loss=0.1779, DER=0.3188, ACC=0.9117, MI=0.04017, FA=0.1824, CF=0.09617, over 0.00 frames. 
2024-12-19 12:22:37,289 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 12:40:47,784 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5706, num_updates: 5500, {'loss': 0.21673648059368134, 'DER': 0.4017500475556401, 'ACC': 0.8996875, 'MI': 0.1877496671105193, 'FA': 0.1272588929046985, 'CF': 0.0867414875404223}, batch size: 64, grad_norm: 0.8977150321006775, grad_scale: , lr: 7.94e-06, 
2024-12-19 12:40:47,785 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 5706, num_updates: 5500, {'loss': 0.2402045726776123, 'DER': 0.43027591349739003, 'ACC': 0.8876171875, 'MI': 0.2313571961222968, 'FA': 0.09284116331096197, 'CF': 0.10607755406413125}, batch size: 64, grad_norm: 0.8977150321006775, grad_scale: , lr: 7.94e-06, 
2024-12-19 12:40:47,785 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 12:40:47,785 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 12:42:08,703 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5706,  validation: loss=0.1684, DER=0.2933, ACC=0.9197, MI=0.03955, FA=0.1666, CF=0.08717, over 0.00 frames. 
2024-12-19 12:42:08,704 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 12:42:08,968 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 5706,  validation: loss=0.1689, DER=0.2951, ACC=0.9195, MI=0.0383, FA=0.1713, CF=0.08546, over 0.00 frames. 
2024-12-19 12:42:08,968 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 12:53:16,836 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-6000.pt
2024-12-19 12:53:22,473 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 13:00:50,985 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 6206, num_updates: 6000, {'loss': 0.26088228821754456, 'DER': 0.46658074019517587, 'ACC': 0.8738471158355479, 'MI': 0.24396980298287607, 'FA': 0.09482599889523108, 'CF': 0.1277849383170687}, batch size: 64, grad_norm: 0.9751840233802795, grad_scale: , lr: 7.66e-06, 
2024-12-19 13:00:50,985 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 13:00:50,985 (train_accelerate_ddp:698) INFO: [Train] - Epoch 3, batch_idx_train: 6206, num_updates: 6000, {'loss': 0.24246293306350708, 'DER': 0.3956505847953216, 'ACC': 0.89296875, 'MI': 0.19883040935672514, 'FA': 0.09173976608187134, 'CF': 0.10508040935672515}, batch size: 64, grad_norm: 0.9751840233802795, grad_scale: , lr: 7.66e-06, 
2024-12-19 13:00:50,986 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 13:02:12,026 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 6206,  validation: loss=0.1668, DER=0.2868, ACC=0.9189, MI=0.03819, FA=0.1527, CF=0.09596, over 0.00 frames. 
2024-12-19 13:02:12,027 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 13:02:12,588 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 3, batch_idx_train: 6206,  validation: loss=0.1654, DER=0.2849, ACC=0.9199, MI=0.037, FA=0.1559, CF=0.09197, over 0.00 frames. 
2024-12-19 13:02:12,588 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 13:05:54,787 (train_accelerate_ddp:709) INFO: end of epoch 3, batch_idx: 2102 batch_idx_train: 6308, {'loss': 0.2598109245300293, 'DER': 0.42291926572803423, 'ACC': 0.8848046875, 'MI': 0.25182676884690786, 'FA': 0.06843699875245055, 'CF': 0.10265549812867582}, batch size: 64, grad_norm: 1.021188497543335, grad_scale: , lr: 7.61e-06, 
2024-12-19 13:05:54,794 (train_accelerate_ddp:709) INFO: end of epoch 3, batch_idx: 2102 batch_idx_train: 6308, {'loss': 0.25469404458999634, 'DER': 0.4778914848426337, 'ACC': 0.88, 'MI': 0.2560339833944777, 'FA': 0.10658428268005407, 'CF': 0.11527321876810195}, batch size: 64, grad_norm: 1.021188497543335, grad_scale: , lr: 7.61e-06, 
2024-12-19 13:05:54,796 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-3.pt
2024-12-19 13:05:59,203 (train_accelerate_ddp:557) INFO:  end of epoch 3, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-3.pt 
2024-12-19 13:06:15,870 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6309, num_updates: 6000, {'loss': 0.27796775102615356, 'DER': 0.464555330510035, 'ACC': 0.868365429779239, 'MI': 0.23697293316148038, 'FA': 0.07291474866507089, 'CF': 0.1546676486834837}, batch size: 64, grad_norm: 1.0163719654083252, grad_scale: , lr: 7.61e-06, 
2024-12-19 13:06:15,878 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6309, num_updates: 6000, {'loss': 0.2316676825284958, 'DER': 0.3970232558139535, 'ACC': 0.896796875, 'MI': 0.2243720930232558, 'FA': 0.07813953488372093, 'CF': 0.09451162790697674}, batch size: 64, grad_norm: 1.0163719654083252, grad_scale: , lr: 7.61e-06, 
2024-12-19 13:24:11,034 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6809, num_updates: 6500, {'loss': 0.2439679354429245, 'DER': 0.4294018376148509, 'ACC': 0.8859765625, 'MI': 0.22520157509844366, 'FA': 0.08625539096193512, 'CF': 0.11794487155447216}, batch size: 64, grad_norm: 1.1731098890304565, grad_scale: , lr: 7.33e-06, 
2024-12-19 13:24:11,034 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 6809, num_updates: 6500, {'loss': 0.26275861263275146, 'DER': 0.485824493731919, 'ACC': 0.8683984375, 'MI': 0.2403085824493732, 'FA': 0.08158148505303761, 'CF': 0.16393442622950818}, batch size: 64, grad_norm: 1.1731098890304565, grad_scale: , lr: 7.33e-06, 
2024-12-19 13:24:11,034 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 13:24:11,034 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 13:25:32,919 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 6809,  validation: loss=0.1555, DER=0.2762, ACC=0.9248, MI=0.04311, FA=0.1536, CF=0.07949, over 0.00 frames. 
2024-12-19 13:25:32,920 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 13:25:33,049 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 6809,  validation: loss=0.1559, DER=0.2804, ACC=0.924, MI=0.04239, FA=0.1605, CF=0.07749, over 0.00 frames. 
2024-12-19 13:25:33,049 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 13:43:38,547 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7309, num_updates: 7000, {'loss': 0.2348584532737732, 'DER': 0.4160747663551402, 'ACC': 0.8840625, 'MI': 0.1942056074766355, 'FA': 0.08317757009345794, 'CF': 0.13869158878504673}, batch size: 64, grad_norm: 1.2829679250717163, grad_scale: , lr: 7.05e-06, 
2024-12-19 13:43:38,547 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7309, num_updates: 7000, {'loss': 0.29243767261505127, 'DER': 0.5190939722950206, 'ACC': 0.8663671875, 'MI': 0.3199176338450019, 'FA': 0.07787345563459379, 'CF': 0.12130288281542494}, batch size: 64, grad_norm: 1.2829679250717163, grad_scale: , lr: 7.05e-06, 
2024-12-19 13:43:38,548 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 13:43:38,548 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 13:44:59,158 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7309,  validation: loss=0.1559, DER=0.2554, ACC=0.9258, MI=0.04402, FA=0.1147, CF=0.09665, over 0.00 frames. 
2024-12-19 13:44:59,159 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 13:44:59,825 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7309,  validation: loss=0.1571, DER=0.2605, ACC=0.9246, MI=0.04321, FA=0.1214, CF=0.09589, over 0.00 frames. 
2024-12-19 13:44:59,825 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 13:51:56,954 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-7500.pt
2024-12-19 13:52:02,840 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 13:52:02,969 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 14:03:04,503 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7809, num_updates: 7500, {'loss': 0.19295552372932434, 'DER': 0.307869142351901, 'ACC': 0.9120703125, 'MI': 0.167816091954023, 'FA': 0.04986737400530504, 'CF': 0.09018567639257294}, batch size: 64, grad_norm: 1.1946797370910645, grad_scale: , lr: 6.77e-06, 
2024-12-19 14:03:04,503 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 14:03:04,503 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 7809, num_updates: 7500, {'loss': 0.24236717820167542, 'DER': 0.4172343951390168, 'ACC': 0.884140625, 'MI': 0.21450929847173633, 'FA': 0.07383538943104401, 'CF': 0.1288897072362364}, batch size: 64, grad_norm: 1.1946797370910645, grad_scale: , lr: 6.77e-06, 
2024-12-19 14:03:04,504 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 14:04:25,956 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7809,  validation: loss=0.1496, DER=0.2271, ACC=0.9306, MI=0.04713, FA=0.07758, CF=0.1024, over 0.00 frames. 
2024-12-19 14:04:25,957 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 14:04:26,569 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 7809,  validation: loss=0.1501, DER=0.2301, ACC=0.9294, MI=0.04694, FA=0.08089, CF=0.1023, over 0.00 frames. 
2024-12-19 14:04:26,570 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 14:22:00,724 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 8309, num_updates: 8000, {'loss': 0.18672163784503937, 'DER': 0.319862268938021, 'ACC': 0.912890625, 'MI': 0.14516129032258066, 'FA': 0.09043131569409206, 'CF': 0.08426966292134831}, batch size: 64, grad_norm: 0.9135942459106445, grad_scale: , lr: 6.49e-06, 
2024-12-19 14:22:00,725 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 14:22:00,734 (train_accelerate_ddp:698) INFO: [Train] - Epoch 4, batch_idx_train: 8309, num_updates: 8000, {'loss': 0.1890294849872589, 'DER': 0.34462001504890893, 'ACC': 0.9151953125, 'MI': 0.17795334838224228, 'FA': 0.1028969149736644, 'CF': 0.06376975169300225}, batch size: 64, grad_norm: 0.9135942459106445, grad_scale: , lr: 6.49e-06, 
2024-12-19 14:22:00,735 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 14:23:17,537 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 8309,  validation: loss=0.1507, DER=0.2268, ACC=0.9297, MI=0.04813, FA=0.07204, CF=0.1066, over 0.00 frames. 
2024-12-19 14:23:17,537 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 14:23:17,671 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 4, batch_idx_train: 8309,  validation: loss=0.1511, DER=0.2286, ACC=0.929, MI=0.04776, FA=0.07521, CF=0.1056, over 0.00 frames. 
2024-12-19 14:23:17,671 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 14:26:41,121 (train_accelerate_ddp:709) INFO: end of epoch 4, batch_idx: 2102 batch_idx_train: 8411, {'loss': 0.22377733886241913, 'DER': 0.36992794842624194, 'ACC': 0.8994921875, 'MI': 0.17159651118695488, 'FA': 0.08039438756162305, 'CF': 0.11793704967766401}, batch size: 64, grad_norm: 1.0247970819473267, grad_scale: , lr: 6.44e-06, 
2024-12-19 14:26:41,128 (train_accelerate_ddp:709) INFO: end of epoch 4, batch_idx: 2102 batch_idx_train: 8411, {'loss': 0.19811996817588806, 'DER': 0.31834340561677926, 'ACC': 0.9052734375, 'MI': 0.15215072875933167, 'FA': 0.05350159971560611, 'CF': 0.11269107714184146}, batch size: 64, grad_norm: 1.0247970819473267, grad_scale: , lr: 6.44e-06, 
2024-12-19 14:26:41,130 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-4.pt
2024-12-19 14:26:45,295 (train_accelerate_ddp:557) INFO:  end of epoch 4, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-4.pt 
2024-12-19 14:27:01,645 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8412, num_updates: 8000, {'loss': 0.23018011450767517, 'DER': 0.3792337590227651, 'ACC': 0.8885055668809785, 'MI': 0.17952989080140663, 'FA': 0.05256339070886545, 'CF': 0.14714047751249307}, batch size: 64, grad_norm: 1.1324480772018433, grad_scale: , lr: 6.44e-06, 
2024-12-19 14:27:01,645 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8412, num_updates: 8000, {'loss': 0.21607297658920288, 'DER': 0.3450573861532766, 'ACC': 0.910859375, 'MI': 0.18807848944835245, 'FA': 0.07960014809329878, 'CF': 0.07737874861162532}, batch size: 64, grad_norm: 1.1324480772018433, grad_scale: , lr: 6.44e-06, 
2024-12-19 14:44:02,156 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8912, num_updates: 8500, {'loss': 0.23495925962924957, 'DER': 0.364862333770369, 'ACC': 0.9000390625, 'MI': 0.19123431354186177, 'FA': 0.05918711369170256, 'CF': 0.11444090653680465}, batch size: 64, grad_norm: 0.8605315685272217, grad_scale: , lr: 6.16e-06, 
2024-12-19 14:44:02,156 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 14:44:02,160 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 8912, num_updates: 8500, {'loss': 0.1743830144405365, 'DER': 0.27031897639214275, 'ACC': 0.930703125, 'MI': 0.16453415029735088, 'FA': 0.05640655974049378, 'CF': 0.049378266354298074}, batch size: 64, grad_norm: 0.8605315685272217, grad_scale: , lr: 6.16e-06, 
2024-12-19 14:44:02,160 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 14:45:18,294 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 8912,  validation: loss=0.1464, DER=0.2365, ACC=0.931, MI=0.0444, FA=0.1019, CF=0.09021, over 0.00 frames. 
2024-12-19 14:45:18,294 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 14:45:18,599 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 8912,  validation: loss=0.1481, DER=0.2443, ACC=0.9292, MI=0.0441, FA=0.1084, CF=0.09176, over 0.00 frames. 
2024-12-19 14:45:18,599 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 14:48:15,654 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-9000.pt
2024-12-19 14:48:21,578 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 14:48:21,622 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 15:02:46,066 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9412, num_updates: 9000, {'loss': 0.20860280096530914, 'DER': 0.3399118605096762, 'ACC': 0.9047265625, 'MI': 0.13489174171297183, 'FA': 0.0776010730024909, 'CF': 0.12741904579421345}, batch size: 64, grad_norm: 1.1191073656082153, grad_scale: , lr: 5.88e-06, 
2024-12-19 15:02:46,067 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:02:46,067 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9412, num_updates: 9000, {'loss': 0.1896272748708725, 'DER': 0.3673625742194982, 'ACC': 0.9116773487572299, 'MI': 0.2043669795058418, 'FA': 0.0974909021260295, 'CF': 0.0655046925876269}, batch size: 64, grad_norm: 1.1191073656082153, grad_scale: , lr: 5.88e-06, 
2024-12-19 15:02:46,067 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:04:10,088 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9412,  validation: loss=0.1435, DER=0.2083, ACC=0.9359, MI=0.04673, FA=0.0669, CF=0.09468, over 0.00 frames. 
2024-12-19 15:04:10,088 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 15:04:10,161 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9412,  validation: loss=0.1438, DER=0.2102, ACC=0.9354, MI=0.04696, FA=0.06605, CF=0.09716, over 0.00 frames. 
2024-12-19 15:04:10,161 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 15:22:19,429 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9912, num_updates: 9500, {'loss': 0.22382846474647522, 'DER': 0.38745247148288975, 'ACC': 0.8951953125, 'MI': 0.1779467680608365, 'FA': 0.08688212927756654, 'CF': 0.12262357414448669}, batch size: 64, grad_norm: 1.0219810009002686, grad_scale: , lr: 5.60e-06, 
2024-12-19 15:22:19,430 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:22:19,435 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 9912, num_updates: 9500, {'loss': 0.184061661362648, 'DER': 0.2997976825455214, 'ACC': 0.9216015625, 'MI': 0.14916314143829318, 'FA': 0.08129483170866286, 'CF': 0.06933970939856539}, batch size: 64, grad_norm: 1.0219810009002686, grad_scale: , lr: 5.60e-06, 
2024-12-19 15:22:19,435 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:23:43,548 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9912,  validation: loss=0.1437, DER=0.2146, ACC=0.9332, MI=0.04964, FA=0.0644, CF=0.1006, over 0.00 frames. 
2024-12-19 15:23:43,548 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 15:23:43,832 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 9912,  validation: loss=0.1449, DER=0.2181, ACC=0.9323, MI=0.04967, FA=0.06659, CF=0.1018, over 0.00 frames. 
2024-12-19 15:23:43,832 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 15:41:44,849 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 10412, num_updates: 10000, {'loss': 0.23630240559577942, 'DER': 0.36449745658656374, 'ACC': 0.8941796875, 'MI': 0.20136818102087353, 'FA': 0.05244693913348535, 'CF': 0.11068233643220488}, batch size: 64, grad_norm: 1.141778826713562, grad_scale: , lr: 5.33e-06, 
2024-12-19 15:41:44,849 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:41:44,850 (train_accelerate_ddp:698) INFO: [Train] - Epoch 5, batch_idx_train: 10412, num_updates: 10000, {'loss': 0.21574485301971436, 'DER': 0.37016052880075545, 'ACC': 0.902890625, 'MI': 0.20321057601510858, 'FA': 0.0676109537299339, 'CF': 0.09933899905571293}, batch size: 64, grad_norm: 1.141778826713562, grad_scale: , lr: 5.33e-06, 
2024-12-19 15:41:44,850 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 15:43:04,972 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 10412,  validation: loss=0.1367, DER=0.1981, ACC=0.9382, MI=0.04405, FA=0.05757, CF=0.09649, over 0.00 frames. 
2024-12-19 15:43:04,972 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 15:43:05,385 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 5, batch_idx_train: 10412,  validation: loss=0.137, DER=0.199, ACC=0.9375, MI=0.04423, FA=0.05903, CF=0.09575, over 0.00 frames. 
2024-12-19 15:43:05,386 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 15:46:16,568 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-10500.pt
2024-12-19 15:46:22,368 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 15:46:55,642 (train_accelerate_ddp:709) INFO: end of epoch 5, batch_idx: 2102 batch_idx_train: 10514, {'loss': 0.21737876534461975, 'DER': 0.36378676470588234, 'ACC': 0.9007370236788459, 'MI': 0.21433823529411763, 'FA': 0.04779411764705882, 'CF': 0.10165441176470588}, batch size: 64, grad_norm: 1.2963898181915283, grad_scale: , lr: 5.27e-06, 
2024-12-19 15:46:55,648 (train_accelerate_ddp:709) INFO: end of epoch 5, batch_idx: 2102 batch_idx_train: 10514, {'loss': 0.1969679296016693, 'DER': 0.32443766679374764, 'ACC': 0.9168359375, 'MI': 0.14944719786504004, 'FA': 0.09359512009149829, 'CF': 0.08139534883720931}, batch size: 64, grad_norm: 1.2963898181915283, grad_scale: , lr: 5.27e-06, 
2024-12-19 15:46:55,651 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-5.pt
2024-12-19 15:46:59,649 (train_accelerate_ddp:557) INFO:  end of epoch 5, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-5.pt 
2024-12-19 15:47:18,514 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 10515, num_updates: 10000, {'loss': 0.20806926488876343, 'DER': 0.3321141185076811, 'ACC': 0.9073828125, 'MI': 0.14959765910753475, 'FA': 0.081016825164594, 'CF': 0.1014996342355523}, batch size: 64, grad_norm: 0.9862717986106873, grad_scale: , lr: 5.27e-06, 
2024-12-19 15:47:18,515 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 10515, num_updates: 10000, {'loss': 0.22409260272979736, 'DER': 0.39065352318364366, 'ACC': 0.8875390625, 'MI': 0.18893756845564075, 'FA': 0.06681270536692223, 'CF': 0.1349032493610807}, batch size: 64, grad_norm: 0.9862717986106873, grad_scale: , lr: 5.27e-06, 
2024-12-19 16:05:15,650 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11015, num_updates: 10500, {'loss': 0.19705726206302643, 'DER': 0.31841922005571033, 'ACC': 0.9053515625, 'MI': 0.18036211699164345, 'FA': 0.03464484679665738, 'CF': 0.10341225626740948}, batch size: 64, grad_norm: 1.107710599899292, grad_scale: , lr: 4.99e-06, 
2024-12-19 16:05:15,650 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:05:15,650 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11015, num_updates: 10500, {'loss': 0.18120703101158142, 'DER': 0.27508833922261483, 'ACC': 0.921484375, 'MI': 0.15671378091872792, 'FA': 0.03833922261484099, 'CF': 0.08003533568904593}, batch size: 64, grad_norm: 1.107710599899292, grad_scale: , lr: 4.99e-06, 
2024-12-19 16:05:15,650 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:06:35,889 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11015,  validation: loss=0.1358, DER=0.2022, ACC=0.9375, MI=0.05169, FA=0.05547, CF=0.09502, over 0.00 frames. 
2024-12-19 16:06:35,890 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 16:06:36,541 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11015,  validation: loss=0.137, DER=0.2054, ACC=0.9368, MI=0.05227, FA=0.0576, CF=0.09551, over 0.00 frames. 
2024-12-19 16:06:36,542 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 16:24:18,309 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11515, num_updates: 11000, {'loss': 0.17695024609565735, 'DER': 0.30484018264840185, 'ACC': 0.9205078125, 'MI': 0.17589041095890412, 'FA': 0.062100456621004566, 'CF': 0.06684931506849315}, batch size: 64, grad_norm: 0.9839348793029785, grad_scale: , lr: 4.71e-06, 
2024-12-19 16:24:18,309 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:24:18,316 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 11515, num_updates: 11000, {'loss': 0.21021397411823273, 'DER': 0.326032315978456, 'ACC': 0.9116796875, 'MI': 0.1763016157989228, 'FA': 0.06983842010771993, 'CF': 0.07989228007181329}, batch size: 64, grad_norm: 0.9839348793029785, grad_scale: , lr: 4.71e-06, 
2024-12-19 16:24:18,316 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:25:39,218 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11515,  validation: loss=0.1342, DER=0.1943, ACC=0.9397, MI=0.04835, FA=0.05362, CF=0.09234, over 0.00 frames. 
2024-12-19 16:25:39,219 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 16:25:39,804 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 11515,  validation: loss=0.1341, DER=0.1928, ACC=0.9403, MI=0.04881, FA=0.05456, CF=0.08945, over 0.00 frames. 
2024-12-19 16:25:39,804 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 16:42:32,071 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-12000.pt
2024-12-19 16:42:38,140 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 16:42:38,188 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 16:43:12,657 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12015, num_updates: 11500, {'loss': 0.20766395330429077, 'DER': 0.3324955116696589, 'ACC': 0.902265625, 'MI': 0.16014362657091563, 'FA': 0.05565529622980251, 'CF': 0.11669658886894076}, batch size: 64, grad_norm: 1.03917396068573, grad_scale: , lr: 4.44e-06, 
2024-12-19 16:43:12,657 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:43:12,658 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12015, num_updates: 11500, {'loss': 0.15913371741771698, 'DER': 0.24482316291002382, 'ACC': 0.9321484375, 'MI': 0.11324903793293019, 'FA': 0.058090525929998166, 'CF': 0.07348359904709548}, batch size: 64, grad_norm: 1.03917396068573, grad_scale: , lr: 4.44e-06, 
2024-12-19 16:43:12,658 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 16:44:34,093 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12015,  validation: loss=0.1328, DER=0.1929, ACC=0.9413, MI=0.04774, FA=0.05917, CF=0.08595, over 0.00 frames. 
2024-12-19 16:44:34,094 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 16:44:34,121 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12015,  validation: loss=0.1315, DER=0.1896, ACC=0.9425, MI=0.0471, FA=0.05771, CF=0.08479, over 0.00 frames. 
2024-12-19 16:44:34,121 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 17:02:05,981 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12515, num_updates: 12000, {'loss': 0.1819644570350647, 'DER': 0.30683070143224384, 'ACC': 0.919140625, 'MI': 0.18508997429305912, 'FA': 0.04847594564818215, 'CF': 0.07326478149100257}, batch size: 64, grad_norm: 1.098297357559204, grad_scale: , lr: 4.16e-06, 
2024-12-19 17:02:05,981 (train_accelerate_ddp:698) INFO: [Train] - Epoch 6, batch_idx_train: 12515, num_updates: 12000, {'loss': 0.19704662263393402, 'DER': 0.3147161491824362, 'ACC': 0.913203125, 'MI': 0.16167554657358074, 'FA': 0.05952599669300018, 'CF': 0.09351460591585523}, batch size: 64, grad_norm: 1.098297357559204, grad_scale: , lr: 4.16e-06, 
2024-12-19 17:02:05,982 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:02:05,982 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:03:25,956 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12515,  validation: loss=0.136, DER=0.1824, ACC=0.943, MI=0.04303, FA=0.04972, CF=0.08964, over 0.00 frames. 
2024-12-19 17:03:25,957 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 17:03:27,104 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 6, batch_idx_train: 12515,  validation: loss=0.1357, DER=0.1822, ACC=0.9429, MI=0.0431, FA=0.05024, CF=0.08885, over 0.00 frames. 
2024-12-19 17:03:27,104 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 17:06:51,486 (train_accelerate_ddp:709) INFO: end of epoch 6, batch_idx: 2102 batch_idx_train: 12617, {'loss': 0.21231216192245483, 'DER': 0.3727057968380883, 'ACC': 0.9001953125, 'MI': 0.20897692167908413, 'FA': 0.07214246774486643, 'CF': 0.09158640741413775}, batch size: 64, grad_norm: 1.1612186431884766, grad_scale: , lr: 4.10e-06, 
2024-12-19 17:06:51,486 (train_accelerate_ddp:709) INFO: end of epoch 6, batch_idx: 2102 batch_idx_train: 12617, {'loss': 0.19579602777957916, 'DER': 0.32879985296820435, 'ACC': 0.91375, 'MI': 0.18764932916743246, 'FA': 0.06414262084175704, 'CF': 0.07700790295901488}, batch size: 64, grad_norm: 1.1612186431884766, grad_scale: , lr: 4.10e-06, 
2024-12-19 17:06:51,488 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-6.pt
2024-12-19 17:06:55,932 (train_accelerate_ddp:557) INFO:  end of epoch 6, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-6.pt 
2024-12-19 17:07:12,517 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 12618, num_updates: 12000, {'loss': 0.20728453993797302, 'DER': 0.3315799020497007, 'ACC': 0.9084375, 'MI': 0.18483584255396335, 'FA': 0.05314710683838201, 'CF': 0.09359695265735535}, batch size: 64, grad_norm: 1.0566655397415161, grad_scale: , lr: 4.10e-06, 
2024-12-19 17:07:12,522 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 12618, num_updates: 12000, {'loss': 0.1697128713130951, 'DER': 0.27930969799287186, 'ACC': 0.9258984375, 'MI': 0.1286812980679047, 'FA': 0.07409491652598012, 'CF': 0.07653348339898705}, batch size: 64, grad_norm: 1.0566655397415161, grad_scale: , lr: 4.10e-06, 
2024-12-19 17:24:47,811 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13118, num_updates: 12500, {'loss': 0.17187702655792236, 'DER': 0.2898634352760146, 'ACC': 0.924453125, 'MI': 0.11944604731679169, 'FA': 0.0882862088863243, 'CF': 0.08213117907289863}, batch size: 64, grad_norm: 0.8295860886573792, grad_scale: , lr: 3.82e-06, 
2024-12-19 17:24:47,812 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:24:47,822 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13118, num_updates: 12500, {'loss': 0.20499050617218018, 'DER': 0.32335329341317365, 'ACC': 0.9043359375, 'MI': 0.1538740700417347, 'FA': 0.048448557430593356, 'CF': 0.12103066594084558}, batch size: 64, grad_norm: 0.8295860886573792, grad_scale: , lr: 3.82e-06, 
2024-12-19 17:24:47,823 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:26:08,197 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13118,  validation: loss=0.1304, DER=0.181, ACC=0.943, MI=0.04283, FA=0.04645, CF=0.0917, over 0.00 frames. 
2024-12-19 17:26:08,198 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 17:26:08,653 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13118,  validation: loss=0.1301, DER=0.1796, ACC=0.9431, MI=0.04293, FA=0.04737, CF=0.08934, over 0.00 frames. 
2024-12-19 17:26:08,653 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 17:39:23,360 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-13500.pt
2024-12-19 17:39:29,229 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 17:39:29,275 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 17:43:39,344 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13618, num_updates: 13000, {'loss': 0.16868442296981812, 'DER': 0.25365767671426054, 'ACC': 0.93, 'MI': 0.14313414419178566, 'FA': 0.048298959985898116, 'CF': 0.06222457253657677}, batch size: 64, grad_norm: 0.8335705399513245, grad_scale: , lr: 3.54e-06, 
2024-12-19 17:43:39,344 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:43:39,346 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 13618, num_updates: 13000, {'loss': 0.1762257069349289, 'DER': 0.26001097092704334, 'ACC': 0.93109375, 'MI': 0.13439385628085573, 'FA': 0.06308283049917718, 'CF': 0.06253428414701043}, batch size: 64, grad_norm: 0.8335705399513245, grad_scale: , lr: 3.54e-06, 
2024-12-19 17:43:39,346 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 17:44:59,075 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13618,  validation: loss=0.1288, DER=0.1824, ACC=0.9428, MI=0.04598, FA=0.04628, CF=0.0901, over 0.00 frames. 
2024-12-19 17:44:59,076 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 17:44:59,823 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 13618,  validation: loss=0.129, DER=0.1805, ACC=0.9432, MI=0.0459, FA=0.04655, CF=0.08807, over 0.00 frames. 
2024-12-19 17:44:59,823 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 18:02:33,897 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14118, num_updates: 13500, {'loss': 0.22405260801315308, 'DER': 0.3693727632322471, 'ACC': 0.89765625, 'MI': 0.15822188736108495, 'FA': 0.08702203804859672, 'CF': 0.12412883782256545}, batch size: 64, grad_norm: 1.0926319360733032, grad_scale: , lr: 3.27e-06, 
2024-12-19 18:02:33,898 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:02:33,901 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14118, num_updates: 13500, {'loss': 0.20070408284664154, 'DER': 0.3125113822618831, 'ACC': 0.909296875, 'MI': 0.15115643780732108, 'FA': 0.0509925332362047, 'CF': 0.11036241121835731}, batch size: 64, grad_norm: 1.0926319360733032, grad_scale: , lr: 3.27e-06, 
2024-12-19 18:02:33,901 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:03:54,972 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14118,  validation: loss=0.1299, DER=0.1806, ACC=0.943, MI=0.04685, FA=0.04205, CF=0.0917, over 0.00 frames. 
2024-12-19 18:03:54,973 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 18:03:55,721 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14118,  validation: loss=0.1299, DER=0.1781, ACC=0.9435, MI=0.0467, FA=0.04217, CF=0.08923, over 0.00 frames. 
2024-12-19 18:03:55,721 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 18:21:58,110 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14618, num_updates: 14000, {'loss': 0.19411978125572205, 'DER': 0.3086980207009261, 'ACC': 0.9128125, 'MI': 0.1452696568004358, 'FA': 0.06682404212820048, 'CF': 0.09660432177228981}, batch size: 64, grad_norm: 0.9414753317832947, grad_scale: , lr: 2.99e-06, 
2024-12-19 18:21:58,110 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:21:58,110 (train_accelerate_ddp:698) INFO: [Train] - Epoch 7, batch_idx_train: 14618, num_updates: 14000, {'loss': 0.17587946355342865, 'DER': 0.2882432172407158, 'ACC': 0.9231640625, 'MI': 0.12218587646719262, 'FA': 0.0758129690205888, 'CF': 0.09024437175293439}, batch size: 64, grad_norm: 0.9414753317832947, grad_scale: , lr: 2.99e-06, 
2024-12-19 18:21:58,110 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:23:18,590 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14618,  validation: loss=0.129, DER=0.1775, ACC=0.9449, MI=0.04226, FA=0.05091, CF=0.08434, over 0.00 frames. 
2024-12-19 18:23:18,591 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 18:23:18,744 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 7, batch_idx_train: 14618,  validation: loss=0.1297, DER=0.1782, ACC=0.9444, MI=0.04228, FA=0.05103, CF=0.08493, over 0.00 frames. 
2024-12-19 18:23:18,744 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 18:26:57,815 (train_accelerate_ddp:709) INFO: end of epoch 7, batch_idx: 2102 batch_idx_train: 14720, {'loss': 0.17856448888778687, 'DER': 0.2845364602418468, 'ACC': 0.9256640625, 'MI': 0.17222425796995236, 'FA': 0.04818614877244412, 'CF': 0.06412605349945034}, batch size: 64, grad_norm: 1.0300626754760742, grad_scale: , lr: 2.93e-06, 
2024-12-19 18:26:57,816 (train_accelerate_ddp:709) INFO: end of epoch 7, batch_idx: 2102 batch_idx_train: 14720, {'loss': 0.17314812541007996, 'DER': 0.28268283440697234, 'ACC': 0.927421875, 'MI': 0.1398256915498295, 'FA': 0.07351269420234938, 'CF': 0.06934444865479349}, batch size: 64, grad_norm: 1.0300626754760742, grad_scale: , lr: 2.93e-06, 
2024-12-19 18:26:57,819 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-7.pt
2024-12-19 18:27:02,037 (train_accelerate_ddp:557) INFO:  end of epoch 7, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-7.pt 
2024-12-19 18:27:18,810 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 14721, num_updates: 14000, {'loss': 0.17817984521389008, 'DER': 0.3046008365157301, 'ACC': 0.9151953125, 'MI': 0.15184579014366248, 'FA': 0.06255682851427533, 'CF': 0.09019821785779232}, batch size: 64, grad_norm: 1.0990288257598877, grad_scale: , lr: 2.93e-06, 
2024-12-19 18:27:18,813 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 14721, num_updates: 14000, {'loss': 0.21433965861797333, 'DER': 0.3465824665676077, 'ACC': 0.9001171875, 'MI': 0.15416047548291234, 'FA': 0.06407875185735512, 'CF': 0.12834323922734026}, batch size: 64, grad_norm: 1.0990288257598877, grad_scale: , lr: 2.93e-06, 
2024-12-19 18:36:56,003 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-15000.pt
2024-12-19 18:37:01,554 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 18:37:01,684 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 18:44:40,564 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15221, num_updates: 14500, {'loss': 0.1937844157218933, 'DER': 0.27164655784244146, 'ACC': 0.9200390625, 'MI': 0.14815471965933286, 'FA': 0.0319375443577005, 'CF': 0.09155429382540808}, batch size: 64, grad_norm: 1.0842260122299194, grad_scale: , lr: 2.65e-06, 
2024-12-19 18:44:40,565 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:44:40,565 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15221, num_updates: 14500, {'loss': 0.1886335164308548, 'DER': 0.3054370012989423, 'ACC': 0.918359375, 'MI': 0.16069771757283355, 'FA': 0.062349229912785305, 'CF': 0.08239005381332344}, batch size: 64, grad_norm: 1.0842260122299194, grad_scale: , lr: 2.65e-06, 
2024-12-19 18:44:40,566 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 18:46:00,645 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15221,  validation: loss=0.1276, DER=0.1794, ACC=0.9439, MI=0.04839, FA=0.04342, CF=0.08758, over 0.00 frames. 
2024-12-19 18:46:00,646 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 18:46:01,203 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15221,  validation: loss=0.1286, DER=0.1794, ACC=0.9436, MI=0.04882, FA=0.04354, CF=0.08707, over 0.00 frames. 
2024-12-19 18:46:01,203 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 19:03:24,413 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15721, num_updates: 15000, {'loss': 0.2268652766942978, 'DER': 0.3636021100226074, 'ACC': 0.9025390625, 'MI': 0.1900904295403165, 'FA': 0.06706857573474001, 'CF': 0.10644310474755087}, batch size: 64, grad_norm: 1.0020105838775635, grad_scale: , lr: 2.38e-06, 
2024-12-19 19:03:24,413 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 15721, num_updates: 15000, {'loss': 0.18279661238193512, 'DER': 0.2675827156029651, 'ACC': 0.9256640625, 'MI': 0.13975772916290002, 'FA': 0.051346953534623034, 'CF': 0.07647803290544206}, batch size: 64, grad_norm: 1.0020105838775635, grad_scale: , lr: 2.38e-06, 
2024-12-19 19:03:24,414 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:03:24,414 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:04:44,743 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15721,  validation: loss=0.1257, DER=0.1792, ACC=0.9445, MI=0.05096, FA=0.04414, CF=0.08414, over 0.00 frames. 
2024-12-19 19:04:44,744 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 19:04:45,595 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 15721,  validation: loss=0.1274, DER=0.1824, ACC=0.9435, MI=0.05128, FA=0.04508, CF=0.08599, over 0.00 frames. 
2024-12-19 19:04:45,596 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 19:22:09,162 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16221, num_updates: 15500, {'loss': 0.19979707896709442, 'DER': 0.3359288097886541, 'ACC': 0.9033984375, 'MI': 0.12087504634779385, 'FA': 0.0925101965146459, 'CF': 0.1225435669262143}, batch size: 64, grad_norm: 0.9487423896789551, grad_scale: , lr: 2.10e-06, 
2024-12-19 19:22:09,162 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16221, num_updates: 15500, {'loss': 0.1881958395242691, 'DER': 0.32500904813608394, 'ACC': 0.91421875, 'MI': 0.19507781397032212, 'FA': 0.05754614549402823, 'CF': 0.07238508867173363}, batch size: 64, grad_norm: 0.9487423896789551, grad_scale: , lr: 2.10e-06, 
2024-12-19 19:22:09,163 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:22:09,163 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:23:30,191 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16221,  validation: loss=0.1238, DER=0.1754, ACC=0.9453, MI=0.04687, FA=0.04334, CF=0.08524, over 0.00 frames. 
2024-12-19 19:23:30,192 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 19:23:30,821 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16221,  validation: loss=0.1255, DER=0.1763, ACC=0.9445, MI=0.04679, FA=0.04368, CF=0.08582, over 0.00 frames. 
2024-12-19 19:23:30,822 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 19:33:00,975 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-16500.pt
2024-12-19 19:33:06,512 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 19:40:53,241 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16721, num_updates: 16000, {'loss': 0.20398622751235962, 'DER': 0.3081749731086411, 'ACC': 0.908650086031597, 'MI': 0.14861957690928648, 'FA': 0.048942273216206526, 'CF': 0.11061312298314808}, batch size: 64, grad_norm: 1.09916090965271, grad_scale: , lr: 1.82e-06, 
2024-12-19 19:40:53,241 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:40:53,241 (train_accelerate_ddp:698) INFO: [Train] - Epoch 8, batch_idx_train: 16721, num_updates: 16000, {'loss': 0.16828227043151855, 'DER': 0.2595501606569082, 'ACC': 0.9252734375, 'MI': 0.1403070332024277, 'FA': 0.03730810424848269, 'CF': 0.08193502320599785}, batch size: 64, grad_norm: 1.09916090965271, grad_scale: , lr: 1.82e-06, 
2024-12-19 19:40:53,242 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 19:42:14,480 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16721,  validation: loss=0.1216, DER=0.1722, ACC=0.9468, MI=0.04825, FA=0.04254, CF=0.08144, over 0.00 frames. 
2024-12-19 19:42:14,480 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 19:42:14,752 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 8, batch_idx_train: 16721,  validation: loss=0.123, DER=0.1727, ACC=0.9462, MI=0.04845, FA=0.04233, CF=0.08197, over 0.00 frames. 
2024-12-19 19:42:14,752 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 19:45:49,683 (train_accelerate_ddp:709) INFO: end of epoch 8, batch_idx: 2102 batch_idx_train: 16823, {'loss': 0.21175822615623474, 'DER': 0.35714285714285715, 'ACC': 0.9012109375, 'MI': 0.18426763110307415, 'FA': 0.07269439421338156, 'CF': 0.10018083182640145}, batch size: 64, grad_norm: 1.1984801292419434, grad_scale: , lr: 1.76e-06, 
2024-12-19 19:45:49,686 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-8.pt
2024-12-19 19:45:49,688 (train_accelerate_ddp:709) INFO: end of epoch 8, batch_idx: 2102 batch_idx_train: 16823, {'loss': 0.19287385046482086, 'DER': 0.31886792452830187, 'ACC': 0.9173046875, 'MI': 0.1641509433962264, 'FA': 0.07415094339622641, 'CF': 0.08056603773584906}, batch size: 64, grad_norm: 1.1984801292419434, grad_scale: , lr: 1.76e-06, 
2024-12-19 19:45:53,846 (train_accelerate_ddp:557) INFO:  end of epoch 8, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-8.pt 
2024-12-19 19:46:10,286 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 16824, num_updates: 16000, {'loss': 0.1818736344575882, 'DER': 0.29837958651517976, 'ACC': 0.9219921875, 'MI': 0.15030731979884523, 'FA': 0.07450176941702366, 'CF': 0.07357049729931085}, batch size: 64, grad_norm: 1.1496248245239258, grad_scale: , lr: 1.76e-06, 
2024-12-19 19:46:10,287 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 16824, num_updates: 16000, {'loss': 0.21413931250572205, 'DER': 0.34487156130442703, 'ACC': 0.9014453125, 'MI': 0.1721625068318455, 'FA': 0.05793404991801785, 'CF': 0.11477500455456367}, batch size: 64, grad_norm: 1.1496248245239258, grad_scale: , lr: 1.76e-06, 
2024-12-19 20:03:54,390 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17324, num_updates: 16500, {'loss': 0.19428949058055878, 'DER': 0.2978062157221207, 'ACC': 0.9162109375, 'MI': 0.14716636197440586, 'FA': 0.05630712979890311, 'CF': 0.0943327239488117}, batch size: 64, grad_norm: 1.1401501893997192, grad_scale: , lr: 1.49e-06, 
2024-12-19 20:03:54,391 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17324, num_updates: 16500, {'loss': 0.22364939749240875, 'DER': 0.34430752253080743, 'ACC': 0.9017578125, 'MI': 0.16019863895530623, 'FA': 0.06584513518484458, 'CF': 0.1182637483906566}, batch size: 64, grad_norm: 1.1401501893997192, grad_scale: , lr: 1.49e-06, 
2024-12-19 20:03:54,391 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:03:54,391 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:05:15,352 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17324,  validation: loss=0.1223, DER=0.1724, ACC=0.9464, MI=0.04325, FA=0.0465, CF=0.08265, over 0.00 frames. 
2024-12-19 20:05:15,353 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 20:05:15,633 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17324,  validation: loss=0.1235, DER=0.1733, ACC=0.9459, MI=0.04399, FA=0.04664, CF=0.08262, over 0.00 frames. 
2024-12-19 20:05:15,633 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 20:22:30,961 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17824, num_updates: 17000, {'loss': 0.1764366626739502, 'DER': 0.28410109338591144, 'ACC': 0.9234375, 'MI': 0.17619645097687758, 'FA': 0.04068829539343968, 'CF': 0.06721634701559419}, batch size: 64, grad_norm: 1.0750399827957153, grad_scale: , lr: 1.21e-06, 
2024-12-19 20:22:30,962 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:22:30,962 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 17824, num_updates: 17000, {'loss': 0.17477422952651978, 'DER': 0.2686671318911375, 'ACC': 0.9249609375, 'MI': 0.1612002791346825, 'FA': 0.040997906489881365, 'CF': 0.06646894626657363}, batch size: 64, grad_norm: 1.0750399827957153, grad_scale: , lr: 1.21e-06, 
2024-12-19 20:22:30,962 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:23:51,380 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17824,  validation: loss=0.123, DER=0.1728, ACC=0.9463, MI=0.04654, FA=0.04361, CF=0.08264, over 0.00 frames. 
2024-12-19 20:23:51,381 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 20:23:51,818 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 17824,  validation: loss=0.1246, DER=0.1741, ACC=0.9456, MI=0.04656, FA=0.04387, CF=0.08369, over 0.00 frames. 
2024-12-19 20:23:51,818 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 20:29:58,573 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-18000.pt
2024-12-19 20:30:03,963 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 20:30:04,102 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 20:41:23,776 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18324, num_updates: 17500, {'loss': 0.14718979597091675, 'DER': 0.2292576419213974, 'ACC': 0.936328125, 'MI': 0.10662299854439593, 'FA': 0.055312954876273655, 'CF': 0.0673216885007278}, batch size: 64, grad_norm: 0.9929048418998718, grad_scale: , lr: 9.31e-07, 
2024-12-19 20:41:23,777 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:41:23,777 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18324, num_updates: 17500, {'loss': 0.2031172662973404, 'DER': 0.3195516354281514, 'ACC': 0.9114453125, 'MI': 0.1517824329290702, 'FA': 0.07074604924660051, 'CF': 0.0970231532524807}, batch size: 64, grad_norm: 0.9929048418998718, grad_scale: , lr: 9.31e-07, 
2024-12-19 20:41:23,778 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 20:42:44,880 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18324,  validation: loss=0.1257, DER=0.1719, ACC=0.9461, MI=0.04336, FA=0.04381, CF=0.0847, over 0.00 frames. 
2024-12-19 20:42:44,881 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 20:42:45,067 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18324,  validation: loss=0.1271, DER=0.1728, ACC=0.9455, MI=0.04339, FA=0.04405, CF=0.08539, over 0.00 frames. 
2024-12-19 20:42:45,068 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 21:00:07,404 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18824, num_updates: 18000, {'loss': 0.1794622391462326, 'DER': 0.30316065083224236, 'ACC': 0.9219921875, 'MI': 0.15578829250046755, 'FA': 0.0770525528333645, 'CF': 0.07031980549841033}, batch size: 64, grad_norm: 1.0698286294937134, grad_scale: , lr: 6.53e-07, 
2024-12-19 21:00:07,405 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:00:07,409 (train_accelerate_ddp:698) INFO: [Train] - Epoch 9, batch_idx_train: 18824, num_updates: 18000, {'loss': 0.17685259878635406, 'DER': 0.2845329825220823, 'ACC': 0.921015625, 'MI': 0.10561924450291299, 'FA': 0.08344296184927645, 'CF': 0.09547077616989287}, batch size: 64, grad_norm: 1.0698286294937134, grad_scale: , lr: 6.53e-07, 
2024-12-19 21:00:07,410 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:01:28,639 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18824,  validation: loss=0.1224, DER=0.171, ACC=0.947, MI=0.0481, FA=0.04168, CF=0.08127, over 0.00 frames. 
2024-12-19 21:01:28,640 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 21:01:29,265 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 9, batch_idx_train: 18824,  validation: loss=0.1241, DER=0.1731, ACC=0.946, MI=0.04842, FA=0.04182, CF=0.0829, over 0.00 frames. 
2024-12-19 21:01:29,265 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 21:05:04,363 (train_accelerate_ddp:709) INFO: end of epoch 9, batch_idx: 2102 batch_idx_train: 18926, {'loss': 0.1822585016489029, 'DER': 0.30488034671188996, 'ACC': 0.915078125, 'MI': 0.12832108535895986, 'FA': 0.071791972866026, 'CF': 0.10476728848690409}, batch size: 64, grad_norm: 0.808941662311554, grad_scale: , lr: 5.96e-07, 
2024-12-19 21:05:04,363 (train_accelerate_ddp:709) INFO: end of epoch 9, batch_idx: 2102 batch_idx_train: 18926, {'loss': 0.15240448713302612, 'DER': 0.2369209089307733, 'ACC': 0.93703125, 'MI': 0.13035053725559273, 'FA': 0.05953848863836533, 'CF': 0.04703188303681522}, batch size: 64, grad_norm: 0.808941662311554, grad_scale: , lr: 5.96e-07, 
2024-12-19 21:05:04,366 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-9.pt
2024-12-19 21:05:08,580 (train_accelerate_ddp:557) INFO:  end of epoch 9, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-9.pt 
2024-12-19 21:05:11,627 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 18927, num_updates: 18000, {'loss': 0.160265251994133, 'DER': 0.254443235400798, 'ACC': 0.932265625, 'MI': 0.1302140007254262, 'FA': 0.06420021762785637, 'CF': 0.06002901704751541}, batch size: 64, grad_norm: 0.9829428195953369, grad_scale: , lr: 5.96e-07, 
2024-12-19 21:05:11,633 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 18927, num_updates: 18000, {'loss': 0.17404820024967194, 'DER': 0.273892347125595, 'ACC': 0.9259375, 'MI': 0.1484804101061882, 'FA': 0.05217868912486269, 'CF': 0.07323324789454412}, batch size: 64, grad_norm: 0.9829428195953369, grad_scale: , lr: 5.96e-07, 
2024-12-19 21:22:54,304 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19427, num_updates: 18500, {'loss': 0.15455783903598785, 'DER': 0.25844346549192365, 'ACC': 0.9296484375, 'MI': 0.131791483113069, 'FA': 0.054515418502202644, 'CF': 0.07213656387665199}, batch size: 64, grad_norm: 0.9535613059997559, grad_scale: , lr: 3.18e-07, 
2024-12-19 21:22:54,305 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:22:54,309 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19427, num_updates: 18500, {'loss': 0.17509731650352478, 'DER': 0.27429186088203655, 'ACC': 0.9246404002501564, 'MI': 0.15310147006095376, 'FA': 0.04983865184653998, 'CF': 0.07135173897454285}, batch size: 64, grad_norm: 0.9535613059997559, grad_scale: , lr: 3.18e-07, 
2024-12-19 21:22:54,310 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:24:14,608 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19427,  validation: loss=0.1244, DER=0.1732, ACC=0.9463, MI=0.04638, FA=0.04448, CF=0.08229, over 0.00 frames. 
2024-12-19 21:24:14,609 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 21:24:16,253 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19427,  validation: loss=0.1261, DER=0.1769, ACC=0.9449, MI=0.04677, FA=0.04532, CF=0.08478, over 0.00 frames. 
2024-12-19 21:24:16,254 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 21:26:46,534 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-19500.pt
2024-12-19 21:26:52,038 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 21:26:52,163 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 21:41:44,223 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19927, num_updates: 19000, {'loss': 0.20774248242378235, 'DER': 0.3164718772826881, 'ACC': 0.9087109375, 'MI': 0.14079620160701242, 'FA': 0.06537618699780862, 'CF': 0.11029948867786706}, batch size: 64, grad_norm: 1.1065611839294434, grad_scale: , lr: 4.00e-08, 
2024-12-19 21:41:44,224 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:41:44,233 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 19927, num_updates: 19000, {'loss': 0.19876132905483246, 'DER': 0.3176729675169756, 'ACC': 0.90984375, 'MI': 0.13819049366856304, 'FA': 0.07359148467608735, 'CF': 0.1058909891723252}, batch size: 64, grad_norm: 1.1065611839294434, grad_scale: , lr: 4.00e-08, 
2024-12-19 21:41:44,234 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 21:43:04,934 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19927,  validation: loss=0.1244, DER=0.1709, ACC=0.9467, MI=0.04493, FA=0.04364, CF=0.08234, over 0.00 frames. 
2024-12-19 21:43:04,935 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 21:43:05,802 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 19927,  validation: loss=0.1263, DER=0.1739, ACC=0.9455, MI=0.045, FA=0.04442, CF=0.08446, over 0.00 frames. 
2024-12-19 21:43:05,802 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 22:00:37,585 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20427, num_updates: 19500, {'loss': 0.17425687611103058, 'DER': 0.2782364307043738, 'ACC': 0.9139453125, 'MI': 0.13226769717196557, 'FA': 0.037238714210433864, 'CF': 0.10873001932197436}, batch size: 64, grad_norm: 1.0339514017105103, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:00:37,585 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20427, num_updates: 19500, {'loss': 0.1834864616394043, 'DER': 0.2794448612153038, 'ACC': 0.9240625, 'MI': 0.13559639909977494, 'FA': 0.05870217554388597, 'CF': 0.08514628657164292}, batch size: 64, grad_norm: 1.0339514017105103, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:00:37,586 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:00:37,586 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:01:58,579 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20427,  validation: loss=0.126, DER=0.1705, ACC=0.9465, MI=0.04051, FA=0.04622, CF=0.08376, over 0.00 frames. 
2024-12-19 22:01:58,580 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 22:01:58,889 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20427,  validation: loss=0.1278, DER=0.1741, ACC=0.9452, MI=0.04055, FA=0.04719, CF=0.08632, over 0.00 frames. 
2024-12-19 22:01:58,890 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 22:19:23,455 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20927, num_updates: 20000, {'loss': 0.1643345057964325, 'DER': 0.26999463231347287, 'ACC': 0.925546875, 'MI': 0.1528001431383074, 'FA': 0.04616210413311862, 'CF': 0.07103238504204688}, batch size: 64, grad_norm: 0.8858771920204163, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:19:23,455 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:19:23,460 (train_accelerate_ddp:698) INFO: [Train] - Epoch 10, batch_idx_train: 20927, num_updates: 20000, {'loss': 0.16250604391098022, 'DER': 0.2628696604600219, 'ACC': 0.9265625, 'MI': 0.11372763782402337, 'FA': 0.0688207374954363, 'CF': 0.08032128514056225}, batch size: 64, grad_norm: 0.8858771920204163, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:19:23,460 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:20:43,836 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20927,  validation: loss=0.1224, DER=0.1705, ACC=0.947, MI=0.04389, FA=0.04505, CF=0.08154, over 0.00 frames. 
2024-12-19 22:20:43,836 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 22:20:44,984 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 10, batch_idx_train: 20927,  validation: loss=0.1242, DER=0.173, ACC=0.9459, MI=0.04436, FA=0.04532, CF=0.08332, over 0.00 frames. 
2024-12-19 22:20:44,984 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 22:23:18,493 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-21000.pt
2024-12-19 22:23:24,543 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 22:24:26,592 (train_accelerate_ddp:709) INFO: end of epoch 10, batch_idx: 2102 batch_idx_train: 21029, {'loss': 0.18174432218074799, 'DER': 0.254969280809541, 'ACC': 0.9263671875, 'MI': 0.1272135887242501, 'FA': 0.042103361040838456, 'CF': 0.08565233104445248}, batch size: 64, grad_norm: 1.299103021621704, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:24:26,593 (train_accelerate_ddp:709) INFO: end of epoch 10, batch_idx: 2102 batch_idx_train: 21029, {'loss': 0.2326592355966568, 'DER': 0.37427745664739887, 'ACC': 0.891328125, 'MI': 0.17991329479768786, 'FA': 0.06611271676300579, 'CF': 0.1282514450867052}, batch size: 64, grad_norm: 1.299103021621704, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:24:26,594 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-10.pt
2024-12-19 22:24:30,754 (train_accelerate_ddp:557) INFO:  end of epoch 10, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-10.pt 
2024-12-19 22:24:34,077 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21030, num_updates: 20000, {'loss': 0.2059326171875, 'DER': 0.36238886741399307, 'ACC': 0.9037109375, 'MI': 0.15152686509470428, 'FA': 0.09683030537301894, 'CF': 0.11403169694626981}, batch size: 64, grad_norm: 1.1917471885681152, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:24:34,077 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21030, num_updates: 20000, {'loss': 0.16982978582382202, 'DER': 0.27795002735728613, 'ACC': 0.918984375, 'MI': 0.12146635053802662, 'FA': 0.05617362757614445, 'CF': 0.10031004924311508}, batch size: 64, grad_norm: 1.1917471885681152, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:41:59,739 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21530, num_updates: 20500, {'loss': 0.18191900849342346, 'DER': 0.2980244590780809, 'ACC': 0.919765625, 'MI': 0.12549388523047977, 'FA': 0.08410159924741298, 'CF': 0.08842897460018814}, batch size: 64, grad_norm: 1.0701074600219727, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:41:59,739 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:41:59,745 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 21530, num_updates: 20500, {'loss': 0.1880301982164383, 'DER': 0.3080568720379147, 'ACC': 0.9196484375, 'MI': 0.14123222748815165, 'FA': 0.08492890995260663, 'CF': 0.08189573459715639}, batch size: 64, grad_norm: 1.0701074600219727, grad_scale: , lr: 0.00e+00, 
2024-12-19 22:41:59,745 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 22:43:20,172 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 21530,  validation: loss=0.1233, DER=0.1706, ACC=0.9469, MI=0.04398, FA=0.04474, CF=0.08191, over 0.00 frames. 
2024-12-19 22:43:20,173 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 22:43:20,708 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 21530,  validation: loss=0.1251, DER=0.1726, ACC=0.9459, MI=0.04397, FA=0.04497, CF=0.08369, over 0.00 frames. 
2024-12-19 22:43:20,709 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 23:00:48,480 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22030, num_updates: 21000, {'loss': 0.17323854565620422, 'DER': 0.27726300784034213, 'ACC': 0.922265625, 'MI': 0.14130434782608695, 'FA': 0.05862437633642195, 'CF': 0.07733428367783321}, batch size: 64, grad_norm: 1.0260558128356934, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:00:48,480 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:00:48,482 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22030, num_updates: 21000, {'loss': 0.20311613380908966, 'DER': 0.3273921200750469, 'ACC': 0.907734375, 'MI': 0.13452157598499062, 'FA': 0.07711069418386492, 'CF': 0.11575984990619137}, batch size: 64, grad_norm: 1.0260558128356934, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:00:48,483 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:02:09,980 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22030,  validation: loss=0.1246, DER=0.171, ACC=0.9467, MI=0.04537, FA=0.04333, CF=0.08227, over 0.00 frames. 
2024-12-19 23:02:09,981 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 23:02:10,387 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22030,  validation: loss=0.1264, DER=0.1743, ACC=0.9455, MI=0.04573, FA=0.04404, CF=0.08452, over 0.00 frames. 
2024-12-19 23:02:10,387 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 23:18:29,886 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-22500.pt
2024-12-19 23:18:37,763 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-19 23:18:37,904 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-19 23:19:39,762 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22530, num_updates: 21500, {'loss': 0.16798074543476105, 'DER': 0.2666048237476809, 'ACC': 0.9288671875, 'MI': 0.12244897959183673, 'FA': 0.07291280148423006, 'CF': 0.0712430426716141}, batch size: 64, grad_norm: 1.0072931051254272, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:19:39,762 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:19:39,762 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 22530, num_updates: 21500, {'loss': 0.1788981556892395, 'DER': 0.29791271347248577, 'ACC': 0.9201953125, 'MI': 0.12144212523719165, 'FA': 0.08671726755218216, 'CF': 0.08975332068311195}, batch size: 64, grad_norm: 1.0072931051254272, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:19:39,762 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:21:00,529 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22530,  validation: loss=0.1238, DER=0.1718, ACC=0.9465, MI=0.04463, FA=0.04484, CF=0.08234, over 0.00 frames. 
2024-12-19 23:21:00,530 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 23:21:01,187 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 22530,  validation: loss=0.126, DER=0.1754, ACC=0.9451, MI=0.04464, FA=0.04589, CF=0.08491, over 0.00 frames. 
2024-12-19 23:21:01,188 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 23:38:35,384 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 23030, num_updates: 22000, {'loss': 0.20862504839897156, 'DER': 0.3404488232074439, 'ACC': 0.899765625, 'MI': 0.15070242656449553, 'FA': 0.062032475825579275, 'CF': 0.1277139208173691}, batch size: 64, grad_norm: 0.9861189723014832, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:38:35,385 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:38:35,385 (train_accelerate_ddp:698) INFO: [Train] - Epoch 11, batch_idx_train: 23030, num_updates: 22000, {'loss': 0.17575514316558838, 'DER': 0.3127841425713766, 'ACC': 0.915078125, 'MI': 0.18166939443535188, 'FA': 0.048554282596835786, 'CF': 0.08256046553918894}, batch size: 64, grad_norm: 0.9861189723014832, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:38:35,385 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-19 23:39:56,610 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 23030,  validation: loss=0.1225, DER=0.1708, ACC=0.9469, MI=0.04445, FA=0.04464, CF=0.08168, over 0.00 frames. 
2024-12-19 23:39:56,611 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 23:39:57,141 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 11, batch_idx_train: 23030,  validation: loss=0.1243, DER=0.1724, ACC=0.946, MI=0.04452, FA=0.04472, CF=0.08318, over 0.00 frames. 
2024-12-19 23:39:57,141 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-19 23:43:27,707 (train_accelerate_ddp:709) INFO: end of epoch 11, batch_idx: 2102 batch_idx_train: 23132, {'loss': 0.1891699731349945, 'DER': 0.31952435749904107, 'ACC': 0.91296875, 'MI': 0.12313003452243959, 'FA': 0.08860759493670886, 'CF': 0.1077867280398926}, batch size: 64, grad_norm: 1.1548359394073486, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:43:27,710 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-11.pt
2024-12-19 23:43:27,712 (train_accelerate_ddp:709) INFO: end of epoch 11, batch_idx: 2102 batch_idx_train: 23132, {'loss': 0.19974221289157867, 'DER': 0.3215940685820204, 'ACC': 0.9151953125, 'MI': 0.15940685820203893, 'FA': 0.08137164040778498, 'CF': 0.08081556997219648}, batch size: 64, grad_norm: 1.1548359394073486, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:43:32,032 (train_accelerate_ddp:557) INFO:  end of epoch 11, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-11.pt 
2024-12-19 23:43:50,470 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23133, num_updates: 22000, {'loss': 0.21160520613193512, 'DER': 0.355674709562109, 'ACC': 0.895078125, 'MI': 0.1810545129579982, 'FA': 0.050223413762287755, 'CF': 0.12439678284182305}, batch size: 64, grad_norm: 0.9773111939430237, grad_scale: , lr: 0.00e+00, 
2024-12-19 23:43:50,476 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23133, num_updates: 22000, {'loss': 0.13080523908138275, 'DER': 0.185001833516685, 'ACC': 0.948359375, 'MI': 0.08745874587458746, 'FA': 0.040154015401540157, 'CF': 0.057389072240557386}, batch size: 64, grad_norm: 0.9773111939430237, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:01:17,481 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23633, num_updates: 22500, {'loss': 0.16890574991703033, 'DER': 0.26028169014084507, 'ACC': 0.9276953125, 'MI': 0.09765258215962441, 'FA': 0.07530516431924883, 'CF': 0.08732394366197183}, batch size: 64, grad_norm: 0.9009153246879578, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:01:17,482 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:01:17,488 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 23633, num_updates: 22500, {'loss': 0.1381894052028656, 'DER': 0.21541574687270051, 'ACC': 0.94140625, 'MI': 0.10816777041942605, 'FA': 0.04672553348050037, 'CF': 0.0605224429727741}, batch size: 64, grad_norm: 0.9009153246879578, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:01:17,488 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:02:38,775 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 23633,  validation: loss=0.1243, DER=0.1726, ACC=0.9463, MI=0.04697, FA=0.04311, CF=0.08253, over 0.00 frames. 
2024-12-20 00:02:38,775 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 00:02:39,174 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 23633,  validation: loss=0.1259, DER=0.1755, ACC=0.9452, MI=0.04701, FA=0.04393, CF=0.08461, over 0.00 frames. 
2024-12-20 00:02:39,174 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 00:15:36,472 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-24000.pt
2024-12-20 00:20:19,862 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24133, num_updates: 23000, {'loss': 0.18409094214439392, 'DER': 0.31881371640407785, 'ACC': 0.9149609375, 'MI': 0.15736793327154772, 'FA': 0.07673772011121409, 'CF': 0.08470806302131603}, batch size: 64, grad_norm: 1.1487722396850586, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:20:19,863 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24133, num_updates: 23000, {'loss': 0.19471311569213867, 'DER': 0.3297504798464491, 'ACC': 0.90875, 'MI': 0.1343570057581574, 'FA': 0.07677543186180422, 'CF': 0.11861804222648753}, batch size: 64, grad_norm: 1.1487722396850586, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:20:19,863 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:20:19,863 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:21:40,714 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24133,  validation: loss=0.1251, DER=0.1729, ACC=0.946, MI=0.04382, FA=0.04562, CF=0.08346, over 0.00 frames. 
2024-12-20 00:21:40,714 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 00:21:41,807 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24133,  validation: loss=0.127, DER=0.1768, ACC=0.9448, MI=0.04429, FA=0.04673, CF=0.08574, over 0.00 frames. 
2024-12-20 00:21:41,807 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 00:39:09,220 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24633, num_updates: 23500, {'loss': 0.16971148550510406, 'DER': 0.26788012502298214, 'ACC': 0.9196875, 'MI': 0.08917080345651775, 'FA': 0.06857878286449715, 'CF': 0.11013053870196728}, batch size: 64, grad_norm: 1.0570437908172607, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:39:09,221 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:39:09,223 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 24633, num_updates: 23500, {'loss': 0.19122980535030365, 'DER': 0.32321755027422305, 'ACC': 0.9070703125, 'MI': 0.1497257769652651, 'FA': 0.061791590493601466, 'CF': 0.11170018281535649}, batch size: 64, grad_norm: 1.0570437908172607, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:39:09,223 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:40:30,883 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24633,  validation: loss=0.1244, DER=0.1724, ACC=0.9462, MI=0.04582, FA=0.04359, CF=0.08296, over 0.00 frames. 
2024-12-20 00:40:30,883 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 00:40:30,965 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 24633,  validation: loss=0.1265, DER=0.1766, ACC=0.9448, MI=0.04605, FA=0.04457, CF=0.08601, over 0.00 frames. 
2024-12-20 00:40:30,966 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 00:58:23,484 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 25133, num_updates: 24000, {'loss': 0.1883789747953415, 'DER': 0.3333333333333333, 'ACC': 0.9130078125, 'MI': 0.11923001374975446, 'FA': 0.10999803574936162, 'CF': 0.10410528383421724}, batch size: 64, grad_norm: 0.7624699473381042, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:58:23,485 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:58:23,490 (train_accelerate_ddp:698) INFO: [Train] - Epoch 12, batch_idx_train: 25133, num_updates: 24000, {'loss': 0.16100788116455078, 'DER': 0.27145169994603346, 'ACC': 0.9234765625, 'MI': 0.14552977154164418, 'FA': 0.044972117287281885, 'CF': 0.08094981111710739}, batch size: 64, grad_norm: 0.7624699473381042, grad_scale: , lr: 0.00e+00, 
2024-12-20 00:58:23,491 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 00:59:44,992 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 25133,  validation: loss=0.1228, DER=0.1716, ACC=0.9465, MI=0.04406, FA=0.0446, CF=0.08294, over 0.00 frames. 
2024-12-20 00:59:44,993 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 00:59:45,702 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 12, batch_idx_train: 25133,  validation: loss=0.1243, DER=0.1729, ACC=0.9457, MI=0.04443, FA=0.04446, CF=0.08401, over 0.00 frames. 
2024-12-20 00:59:45,703 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 01:03:30,679 (train_accelerate_ddp:709) INFO: end of epoch 12, batch_idx: 2102 batch_idx_train: 25235, {'loss': 0.1992674022912979, 'DER': 0.3213641284734753, 'ACC': 0.91015625, 'MI': 0.16492241068206423, 'FA': 0.06279321544568747, 'CF': 0.09364850234572357}, batch size: 64, grad_norm: 1.1466842889785767, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:03:30,679 (train_accelerate_ddp:709) INFO: end of epoch 12, batch_idx: 2102 batch_idx_train: 25235, {'loss': 0.19907639920711517, 'DER': 0.2759770327838489, 'ACC': 0.916484375, 'MI': 0.11168734950916837, 'FA': 0.04426745693646972, 'CF': 0.12002222633821077}, batch size: 64, grad_norm: 1.1466842889785767, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:03:30,681 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-12.pt
2024-12-20 01:03:34,876 (train_accelerate_ddp:557) INFO:  end of epoch 12, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-12.pt 
2024-12-20 01:03:38,149 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25236, num_updates: 24000, {'loss': 0.2173757702112198, 'DER': 0.35472600220669365, 'ACC': 0.9028125, 'MI': 0.19271791099668997, 'FA': 0.05921294593600589, 'CF': 0.10279514527399779}, batch size: 64, grad_norm: 1.0034676790237427, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:03:38,154 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25236, num_updates: 24000, {'loss': 0.1639879047870636, 'DER': 0.2581282557930663, 'ACC': 0.92859375, 'MI': 0.14226693012394467, 'FA': 0.04562601041853781, 'CF': 0.0702353152505838}, batch size: 64, grad_norm: 1.0034676790237427, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:13:10,266 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-25500.pt
2024-12-20 01:13:23,388 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 01:13:23,537 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 01:21:44,654 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25736, num_updates: 24500, {'loss': 0.20544034242630005, 'DER': 0.3163356593308284, 'ACC': 0.9110546875, 'MI': 0.1730184290570764, 'FA': 0.05224548219717302, 'CF': 0.091071748076579}, batch size: 64, grad_norm: 1.2425286769866943, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:21:44,655 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:21:44,660 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 25736, num_updates: 24500, {'loss': 0.25142523646354675, 'DER': 0.3720254314259764, 'ACC': 0.8918717667345979, 'MI': 0.17275204359673024, 'FA': 0.0701180744777475, 'CF': 0.12915531335149863}, batch size: 64, grad_norm: 1.2425286769866943, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:21:44,660 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:23:04,334 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 25736,  validation: loss=0.1228, DER=0.1708, ACC=0.9469, MI=0.0445, FA=0.04451, CF=0.08179, over 0.00 frames. 
2024-12-20 01:23:04,335 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 01:23:05,735 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 25736,  validation: loss=0.1247, DER=0.1729, ACC=0.9458, MI=0.04461, FA=0.04466, CF=0.08363, over 0.00 frames. 
2024-12-20 01:23:05,735 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 01:41:02,645 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26236, num_updates: 25000, {'loss': 0.1801331490278244, 'DER': 0.29513572599744947, 'ACC': 0.920625, 'MI': 0.15394425214064492, 'FA': 0.06613226452905811, 'CF': 0.0750592093277464}, batch size: 64, grad_norm: 0.9867572784423828, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:41:02,645 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:41:02,646 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26236, num_updates: 25000, {'loss': 0.15514393150806427, 'DER': 0.23606907601922736, 'ACC': 0.933671875, 'MI': 0.1210610646252448, 'FA': 0.04878048780487805, 'CF': 0.0662275235891045}, batch size: 64, grad_norm: 0.9867572784423828, grad_scale: , lr: 0.00e+00, 
2024-12-20 01:41:02,647 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 01:42:23,007 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26236,  validation: loss=0.1228, DER=0.1709, ACC=0.9469, MI=0.04556, FA=0.04368, CF=0.08166, over 0.00 frames. 
2024-12-20 01:42:23,008 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 01:42:23,988 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26236,  validation: loss=0.1246, DER=0.1727, ACC=0.946, MI=0.04569, FA=0.04371, CF=0.08328, over 0.00 frames. 
2024-12-20 01:42:23,988 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 02:00:22,330 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26736, num_updates: 25500, {'loss': 0.18886539340019226, 'DER': 0.30632551528073915, 'ACC': 0.913671875, 'MI': 0.1632906894100924, 'FA': 0.056680881307746976, 'CF': 0.08635394456289978}, batch size: 64, grad_norm: 1.2505974769592285, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:00:22,331 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:00:22,336 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 26736, num_updates: 25500, {'loss': 0.21249783039093018, 'DER': 0.33903345724907064, 'ACC': 0.9004296875, 'MI': 0.1557620817843866, 'FA': 0.04851301115241636, 'CF': 0.13475836431226765}, batch size: 64, grad_norm: 1.2505974769592285, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:00:22,337 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:01:43,473 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26736,  validation: loss=0.1236, DER=0.1708, ACC=0.9467, MI=0.04483, FA=0.04368, CF=0.08228, over 0.00 frames. 
2024-12-20 02:01:43,474 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 02:01:43,965 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 26736,  validation: loss=0.1255, DER=0.1743, ACC=0.9455, MI=0.04532, FA=0.04453, CF=0.08442, over 0.00 frames. 
2024-12-20 02:01:43,965 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 02:11:09,228 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-27000.pt
2024-12-20 02:11:14,679 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 02:19:43,766 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 27236, num_updates: 26000, {'loss': 0.2093866914510727, 'DER': 0.31900919497091385, 'ACC': 0.91109375, 'MI': 0.140739350722462, 'FA': 0.07018202289360105, 'CF': 0.10808782135485082}, batch size: 64, grad_norm: 1.2812055349349976, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:19:43,766 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:19:43,776 (train_accelerate_ddp:698) INFO: [Train] - Epoch 13, batch_idx_train: 27236, num_updates: 26000, {'loss': 0.2271040380001068, 'DER': 0.3688359303391384, 'ACC': 0.8969921875, 'MI': 0.18221814848762602, 'FA': 0.07204399633363887, 'CF': 0.1145737855178735}, batch size: 64, grad_norm: 1.2812055349349976, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:19:43,777 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:21:03,750 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 27236,  validation: loss=0.1248, DER=0.1709, ACC=0.9467, MI=0.04536, FA=0.04282, CF=0.08274, over 0.00 frames. 
2024-12-20 02:21:03,751 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 02:21:04,939 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 13, batch_idx_train: 27236,  validation: loss=0.1267, DER=0.1737, ACC=0.9456, MI=0.04557, FA=0.04346, CF=0.08463, over 0.00 frames. 
2024-12-20 02:21:04,940 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 02:24:48,517 (train_accelerate_ddp:709) INFO: end of epoch 13, batch_idx: 2102 batch_idx_train: 27338, {'loss': 0.1844039112329483, 'DER': 0.3032222015272863, 'ACC': 0.915078125, 'MI': 0.13075060532687652, 'FA': 0.07077668094617247, 'CF': 0.1016949152542373}, batch size: 64, grad_norm: 1.4201589822769165, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:24:48,525 (train_accelerate_ddp:709) INFO: end of epoch 13, batch_idx: 2102 batch_idx_train: 27338, {'loss': 0.22299516201019287, 'DER': 0.374484052532833, 'ACC': 0.8956640625, 'MI': 0.16435272045028143, 'FA': 0.08348968105065666, 'CF': 0.12664165103189493}, batch size: 64, grad_norm: 1.4201589822769165, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:24:48,527 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-13.pt
2024-12-20 02:24:52,784 (train_accelerate_ddp:557) INFO:  end of epoch 13, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-13.pt 
2024-12-20 02:24:57,561 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27339, num_updates: 26000, {'loss': 0.18625162541866302, 'DER': 0.29313142239048134, 'ACC': 0.9149609375, 'MI': 0.14530376780241572, 'FA': 0.04849468180998738, 'CF': 0.09933297277807825}, batch size: 64, grad_norm: 1.1068178415298462, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:24:57,566 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27339, num_updates: 26000, {'loss': 0.23815391957759857, 'DER': 0.3787961447535916, 'ACC': 0.890234375, 'MI': 0.18385160938352427, 'FA': 0.06273867975995635, 'CF': 0.13220585561011092}, batch size: 64, grad_norm: 1.1068178415298462, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:43:10,046 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27839, num_updates: 26500, {'loss': 0.1751599907875061, 'DER': 0.27480510276399717, 'ACC': 0.925546875, 'MI': 0.16211906449326718, 'FA': 0.04978738483345145, 'CF': 0.06289865343727853}, batch size: 64, grad_norm: 0.8279331922531128, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:43:10,047 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:43:10,053 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 27839, num_updates: 26500, {'loss': 0.15856561064720154, 'DER': 0.22734584450402145, 'ACC': 0.9364453125, 'MI': 0.11474530831099196, 'FA': 0.049151027703306524, 'CF': 0.06344950848972297}, batch size: 64, grad_norm: 0.8279331922531128, grad_scale: , lr: 0.00e+00, 
2024-12-20 02:43:10,053 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 02:44:29,607 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 27839,  validation: loss=0.1242, DER=0.1719, ACC=0.9465, MI=0.04551, FA=0.04436, CF=0.08208, over 0.00 frames. 
2024-12-20 02:44:29,607 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 02:44:30,865 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 27839,  validation: loss=0.1261, DER=0.1745, ACC=0.9454, MI=0.04549, FA=0.04465, CF=0.08437, over 0.00 frames. 
2024-12-20 02:44:30,866 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 03:02:28,492 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28339, num_updates: 27000, {'loss': 0.15234410762786865, 'DER': 0.2405224015962271, 'ACC': 0.93171875, 'MI': 0.11699619082169417, 'FA': 0.04697986577181208, 'CF': 0.07654634500272084}, batch size: 64, grad_norm: 0.9157541990280151, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:02:28,493 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28339, num_updates: 27000, {'loss': 0.179900124669075, 'DER': 0.2985842985842986, 'ACC': 0.9212109375, 'MI': 0.1667585953300239, 'FA': 0.05956977385548814, 'CF': 0.07225592939878654}, batch size: 64, grad_norm: 0.9157541990280151, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:02:28,493 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:02:28,493 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:03:49,253 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28339,  validation: loss=0.1233, DER=0.1707, ACC=0.9468, MI=0.04554, FA=0.04299, CF=0.08219, over 0.00 frames. 
2024-12-20 03:03:49,254 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 03:03:50,484 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28339,  validation: loss=0.1251, DER=0.173, ACC=0.9458, MI=0.04568, FA=0.04359, CF=0.08373, over 0.00 frames. 
2024-12-20 03:03:50,484 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 03:09:42,616 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-28500.pt
2024-12-20 03:09:48,166 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 03:09:48,274 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 03:21:58,012 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28839, num_updates: 27500, {'loss': 0.1992305964231491, 'DER': 0.31756389342033714, 'ACC': 0.912265625, 'MI': 0.17128874388254486, 'FA': 0.05673373210077941, 'CF': 0.08954141743701287}, batch size: 64, grad_norm: 1.1320478916168213, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:21:58,012 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 28839, num_updates: 27500, {'loss': 0.17423942685127258, 'DER': 0.27825159914712155, 'ACC': 0.922578125, 'MI': 0.1634683724235963, 'FA': 0.040867093105899074, 'CF': 0.07391613361762615}, batch size: 64, grad_norm: 1.1320478916168213, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:21:58,012 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:21:58,012 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:23:18,224 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28839,  validation: loss=0.1237, DER=0.1717, ACC=0.9466, MI=0.04685, FA=0.04288, CF=0.08198, over 0.00 frames. 
2024-12-20 03:23:18,225 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 03:23:19,417 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 28839,  validation: loss=0.1255, DER=0.1748, ACC=0.9454, MI=0.04708, FA=0.04336, CF=0.08437, over 0.00 frames. 
2024-12-20 03:23:19,417 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 03:41:31,388 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 29339, num_updates: 28000, {'loss': 0.17579607665538788, 'DER': 0.287170474516696, 'ACC': 0.91984375, 'MI': 0.1694200351493849, 'FA': 0.04428822495606327, 'CF': 0.0734622144112478}, batch size: 64, grad_norm: 1.0153571367263794, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:41:31,389 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:41:31,393 (train_accelerate_ddp:698) INFO: [Train] - Epoch 14, batch_idx_train: 29339, num_updates: 28000, {'loss': 0.1823580414056778, 'DER': 0.2878568847126852, 'ACC': 0.9201171875, 'MI': 0.15178894109143476, 'FA': 0.05439103722443079, 'CF': 0.08167690639681965}, batch size: 64, grad_norm: 1.0153571367263794, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:41:31,393 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 03:42:51,915 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 29339,  validation: loss=0.1214, DER=0.1706, ACC=0.9471, MI=0.04558, FA=0.04428, CF=0.08071, over 0.00 frames. 
2024-12-20 03:42:51,915 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 03:42:53,418 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 14, batch_idx_train: 29339,  validation: loss=0.1232, DER=0.1727, ACC=0.9461, MI=0.04571, FA=0.04447, CF=0.08251, over 0.00 frames. 
2024-12-20 03:42:53,418 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 03:46:27,779 (train_accelerate_ddp:709) INFO: end of epoch 14, batch_idx: 2102 batch_idx_train: 29441, {'loss': 0.18337604403495789, 'DER': 0.2803603603603604, 'ACC': 0.92078125, 'MI': 0.1464864864864865, 'FA': 0.04882882882882883, 'CF': 0.08504504504504505}, batch size: 64, grad_norm: 0.8964853882789612, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:46:27,780 (train_accelerate_ddp:709) INFO: end of epoch 14, batch_idx: 2102 batch_idx_train: 29441, {'loss': 0.15360262989997864, 'DER': 0.24180109909590497, 'ACC': 0.93265625, 'MI': 0.13915972345328842, 'FA': 0.03882290374047155, 'CF': 0.06381847190214501}, batch size: 64, grad_norm: 0.8964853882789612, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:46:27,782 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-14.pt
2024-12-20 03:46:32,028 (train_accelerate_ddp:557) INFO:  end of epoch 14, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-14.pt 
2024-12-20 03:46:35,840 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29442, num_updates: 28000, {'loss': 0.1591637283563614, 'DER': 0.27594650593332076, 'ACC': 0.92703125, 'MI': 0.12657750988886796, 'FA': 0.07346016198907515, 'CF': 0.07590883405537766}, batch size: 64, grad_norm: 1.0398876667022705, grad_scale: , lr: 0.00e+00, 
2024-12-20 03:46:35,845 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29442, num_updates: 28000, {'loss': 0.1798083484172821, 'DER': 0.30642533936651584, 'ACC': 0.913984375, 'MI': 0.1411764705882353, 'FA': 0.07312217194570136, 'CF': 0.09212669683257918}, batch size: 64, grad_norm: 1.0398876667022705, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:04:45,998 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29942, num_updates: 28500, {'loss': 0.1747448593378067, 'DER': 0.25656565656565655, 'ACC': 0.927421875, 'MI': 0.11533516988062442, 'FA': 0.05656565656565657, 'CF': 0.08466483011937558}, batch size: 64, grad_norm: 0.9453999400138855, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:04:45,998 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 29942, num_updates: 28500, {'loss': 0.16365130245685577, 'DER': 0.22956909361069835, 'ACC': 0.934921875, 'MI': 0.09119613670133729, 'FA': 0.05850668647845468, 'CF': 0.07986627043090638}, batch size: 64, grad_norm: 0.9453999400138855, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:04:45,999 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:04:45,999 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:06:05,847 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 29942,  validation: loss=0.1233, DER=0.1704, ACC=0.9469, MI=0.04397, FA=0.04468, CF=0.08179, over 0.00 frames. 
2024-12-20 04:06:05,848 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 04:06:06,433 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 29942,  validation: loss=0.125, DER=0.1725, ACC=0.946, MI=0.04401, FA=0.045, CF=0.08348, over 0.00 frames. 
2024-12-20 04:06:06,434 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 04:08:08,596 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-30000.pt
2024-12-20 04:08:14,257 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 04:08:14,379 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 04:24:02,246 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30442, num_updates: 29000, {'loss': 0.17597301304340363, 'DER': 0.28172004954875246, 'ACC': 0.91875, 'MI': 0.15413201203326846, 'FA': 0.0412316404176252, 'CF': 0.08635639709785879}, batch size: 64, grad_norm: 1.0197913646697998, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:24:02,246 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30442, num_updates: 29000, {'loss': 0.15275229513645172, 'DER': 0.24528647263408385, 'ACC': 0.936171875, 'MI': 0.1491854292513271, 'FA': 0.04228445908841296, 'CF': 0.053816584294343765}, batch size: 64, grad_norm: 1.0197913646697998, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:24:02,246 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:24:02,246 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:25:22,386 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30442,  validation: loss=0.123, DER=0.1712, ACC=0.9467, MI=0.04518, FA=0.04322, CF=0.08276, over 0.00 frames. 
2024-12-20 04:25:22,387 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 04:25:23,922 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30442,  validation: loss=0.1246, DER=0.1724, ACC=0.946, MI=0.04555, FA=0.0434, CF=0.08342, over 0.00 frames. 
2024-12-20 04:25:23,922 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 04:42:51,426 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30942, num_updates: 29500, {'loss': 0.15032503008842468, 'DER': 0.2583597982439753, 'ACC': 0.9345703125, 'MI': 0.14066878385951803, 'FA': 0.06314216327293107, 'CF': 0.05454885111152625}, batch size: 64, grad_norm: 0.7957472801208496, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:42:51,427 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:42:51,430 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 30942, num_updates: 29500, {'loss': 0.15063177049160004, 'DER': 0.2596655277827729, 'ACC': 0.93140625, 'MI': 0.14889408379787808, 'FA': 0.054666426901636395, 'CF': 0.05610501708325841}, batch size: 64, grad_norm: 0.7957472801208496, grad_scale: , lr: 0.00e+00, 
2024-12-20 04:42:51,430 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 04:44:12,203 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30942,  validation: loss=0.1238, DER=0.1712, ACC=0.9468, MI=0.04689, FA=0.04274, CF=0.08155, over 0.00 frames. 
2024-12-20 04:44:12,203 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 04:44:13,242 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 30942,  validation: loss=0.1256, DER=0.1744, ACC=0.9455, MI=0.04722, FA=0.04312, CF=0.08405, over 0.00 frames. 
2024-12-20 04:44:13,242 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 05:01:48,669 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 31442, num_updates: 30000, {'loss': 0.1660281866788864, 'DER': 0.2864445285876562, 'ACC': 0.9258984375, 'MI': 0.14388489208633093, 'FA': 0.06985990155244226, 'CF': 0.072699734948883}, batch size: 64, grad_norm: 1.1950569152832031, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:01:48,669 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:01:48,670 (train_accelerate_ddp:698) INFO: [Train] - Epoch 15, batch_idx_train: 31442, num_updates: 30000, {'loss': 0.17090165615081787, 'DER': 0.27661089355642576, 'ACC': 0.925078125, 'MI': 0.15806336774652902, 'FA': 0.053755784976860094, 'CF': 0.06479174083303667}, batch size: 64, grad_norm: 1.1950569152832031, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:01:48,670 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:03:07,844 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 31442,  validation: loss=0.1269, DER=0.1738, ACC=0.9456, MI=0.04522, FA=0.04395, CF=0.08462, over 0.00 frames. 
2024-12-20 05:03:07,845 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 05:03:08,627 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 15, batch_idx_train: 31442,  validation: loss=0.1289, DER=0.179, ACC=0.944, MI=0.04557, FA=0.04571, CF=0.08777, over 0.00 frames. 
2024-12-20 05:03:08,627 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 05:05:11,099 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-31500.pt
2024-12-20 05:05:16,731 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 05:06:52,002 (train_accelerate_ddp:709) INFO: end of epoch 15, batch_idx: 2102 batch_idx_train: 31544, {'loss': 0.19027434289455414, 'DER': 0.2931255728689276, 'ACC': 0.9228515625, 'MI': 0.17286892758936756, 'FA': 0.051329055912007336, 'CF': 0.06892758936755271}, batch size: 64, grad_norm: 0.9623149037361145, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:06:52,004 (train_accelerate_ddp:709) INFO: end of epoch 15, batch_idx: 2102 batch_idx_train: 31544, {'loss': 0.170878604054451, 'DER': 0.28307123034227566, 'ACC': 0.9230859375, 'MI': 0.12321924144310824, 'FA': 0.0786308973172988, 'CF': 0.08122109158186865}, batch size: 64, grad_norm: 0.9623149037361145, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:06:52,005 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-15.pt
2024-12-20 05:06:56,189 (train_accelerate_ddp:557) INFO:  end of epoch 15, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-15.pt 
2024-12-20 05:07:13,350 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 31545, num_updates: 30000, {'loss': 0.1515427678823471, 'DER': 0.2388979517853906, 'ACC': 0.9346484375, 'MI': 0.1295994199746239, 'FA': 0.04495196664854088, 'CF': 0.06434656516222585}, batch size: 64, grad_norm: 1.0394994020462036, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:07:13,351 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 31545, num_updates: 30000, {'loss': 0.21503600478172302, 'DER': 0.321996708721887, 'ACC': 0.9053515625, 'MI': 0.13293106600841104, 'FA': 0.06801974766867801, 'CF': 0.12104589504479796}, batch size: 64, grad_norm: 1.0394994020462036, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:24:47,262 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32045, num_updates: 30500, {'loss': 0.1975412219762802, 'DER': 0.276219403251742, 'ACC': 0.9204296875, 'MI': 0.13793103448275862, 'FA': 0.050562801500804, 'CF': 0.08772556726817939}, batch size: 64, grad_norm: 1.3668195009231567, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:24:47,262 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:24:47,268 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32045, num_updates: 30500, {'loss': 0.2486298829317093, 'DER': 0.37052593133674216, 'ACC': 0.890987460815047, 'MI': 0.17366691015339664, 'FA': 0.05934989043097151, 'CF': 0.137509130752374}, batch size: 64, grad_norm: 1.3668195009231567, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:24:47,269 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:26:07,459 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32045,  validation: loss=0.1223, DER=0.1703, ACC=0.947, MI=0.0437, FA=0.045, CF=0.08156, over 0.00 frames. 
2024-12-20 05:26:07,460 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 05:26:07,852 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32045,  validation: loss=0.1243, DER=0.1737, ACC=0.9457, MI=0.04418, FA=0.04575, CF=0.08376, over 0.00 frames. 
2024-12-20 05:26:07,852 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 05:43:37,526 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32545, num_updates: 31000, {'loss': 0.19219771027565002, 'DER': 0.3298579902930074, 'ACC': 0.9053125, 'MI': 0.15243573611360778, 'FA': 0.07154413086464138, 'CF': 0.10587812331475822}, batch size: 64, grad_norm: 0.8191502094268799, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:43:37,527 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:43:37,531 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 32545, num_updates: 31000, {'loss': 0.14690373837947845, 'DER': 0.25341018251681074, 'ACC': 0.932578125, 'MI': 0.09798270893371758, 'FA': 0.07723342939481267, 'CF': 0.07819404418828049}, batch size: 64, grad_norm: 0.8191502094268799, grad_scale: , lr: 0.00e+00, 
2024-12-20 05:43:37,531 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 05:44:58,432 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32545,  validation: loss=0.1227, DER=0.1708, ACC=0.9469, MI=0.04465, FA=0.04451, CF=0.08165, over 0.00 frames. 
2024-12-20 05:44:58,433 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 05:44:59,474 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 32545,  validation: loss=0.1247, DER=0.1741, ACC=0.9457, MI=0.04497, FA=0.0453, CF=0.08382, over 0.00 frames. 
2024-12-20 05:44:59,475 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 06:00:45,032 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-33000.pt
2024-12-20 06:00:50,567 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 06:00:50,703 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 06:02:30,926 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33045, num_updates: 31500, {'loss': 0.16840007901191711, 'DER': 0.2822040208488459, 'ACC': 0.9205078125, 'MI': 0.12174236783320923, 'FA': 0.06384959046909904, 'CF': 0.0966120625465376}, batch size: 64, grad_norm: 1.047432541847229, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:02:30,927 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:02:30,931 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33045, num_updates: 31500, {'loss': 0.20853446424007416, 'DER': 0.35565777368905244, 'ACC': 0.901015625, 'MI': 0.17589696412143516, 'FA': 0.06918123275068998, 'CF': 0.11057957681692732}, batch size: 64, grad_norm: 1.047432541847229, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:02:30,931 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:03:52,469 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33045,  validation: loss=0.1234, DER=0.1708, ACC=0.9468, MI=0.04588, FA=0.04293, CF=0.08198, over 0.00 frames. 
2024-12-20 06:03:52,470 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 06:03:53,340 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33045,  validation: loss=0.1253, DER=0.1736, ACC=0.9457, MI=0.04599, FA=0.04357, CF=0.08407, over 0.00 frames. 
2024-12-20 06:03:53,341 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 06:21:20,011 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33545, num_updates: 32000, {'loss': 0.17041416466236115, 'DER': 0.2350726196880043, 'ACC': 0.9309765625, 'MI': 0.09359870898332437, 'FA': 0.05970952124798279, 'CF': 0.08176438945669715}, batch size: 64, grad_norm: 0.917753279209137, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:21:20,012 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:21:20,017 (train_accelerate_ddp:698) INFO: [Train] - Epoch 16, batch_idx_train: 33545, num_updates: 32000, {'loss': 0.16480055451393127, 'DER': 0.2929027412692452, 'ACC': 0.927890625, 'MI': 0.15452497183627487, 'FA': 0.08467893353360871, 'CF': 0.05369883589936162}, batch size: 64, grad_norm: 0.917753279209137, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:21:20,017 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:22:40,316 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33545,  validation: loss=0.1237, DER=0.1717, ACC=0.9466, MI=0.04602, FA=0.04386, CF=0.08182, over 0.00 frames. 
2024-12-20 06:22:40,317 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 06:22:41,115 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 16, batch_idx_train: 33545,  validation: loss=0.1253, DER=0.1739, ACC=0.9457, MI=0.0462, FA=0.04419, CF=0.08352, over 0.00 frames. 
2024-12-20 06:22:41,116 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 06:26:08,839 (train_accelerate_ddp:709) INFO: end of epoch 16, batch_idx: 2102 batch_idx_train: 33647, {'loss': 0.14875921607017517, 'DER': 0.23776871756856932, 'ACC': 0.936640625, 'MI': 0.11082283172720533, 'FA': 0.06412157153446998, 'CF': 0.062824314306894}, batch size: 64, grad_norm: 1.0347018241882324, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:26:08,842 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-16.pt
2024-12-20 06:26:08,844 (train_accelerate_ddp:709) INFO: end of epoch 16, batch_idx: 2102 batch_idx_train: 33647, {'loss': 0.19715990126132965, 'DER': 0.30575085631873083, 'ACC': 0.9121484375, 'MI': 0.14674598882278708, 'FA': 0.05931133946277267, 'CF': 0.09969352803317108}, batch size: 64, grad_norm: 1.0347018241882324, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:26:13,077 (train_accelerate_ddp:557) INFO:  end of epoch 16, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-16.pt 
2024-12-20 06:26:16,159 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 33648, num_updates: 32000, {'loss': 0.1840520054101944, 'DER': 0.30925339787749023, 'ACC': 0.91578125, 'MI': 0.13963880096816236, 'FA': 0.07745298827034072, 'CF': 0.09216160863898715}, batch size: 64, grad_norm: 0.8967223167419434, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:26:16,166 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 33648, num_updates: 32000, {'loss': 0.16369378566741943, 'DER': 0.2689199118295371, 'ACC': 0.930546875, 'MI': 0.13703159441587068, 'FA': 0.07421013960323292, 'CF': 0.0576781778104335}, batch size: 64, grad_norm: 0.8967223167419434, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:43:35,804 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34148, num_updates: 32500, {'loss': 0.1844940185546875, 'DER': 0.30803738317757007, 'ACC': 0.911875, 'MI': 0.12560747663551403, 'FA': 0.06878504672897197, 'CF': 0.11364485981308411}, batch size: 64, grad_norm: 1.1108038425445557, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:43:35,804 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34148, num_updates: 32500, {'loss': 0.17553910613059998, 'DER': 0.28644876973925815, 'ACC': 0.9237109375, 'MI': 0.14965112008813808, 'FA': 0.06463459419757621, 'CF': 0.07216305545354389}, batch size: 64, grad_norm: 1.1108038425445557, grad_scale: , lr: 0.00e+00, 
2024-12-20 06:43:35,805 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:43:35,805 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 06:44:55,931 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34148,  validation: loss=0.1249, DER=0.1721, ACC=0.9464, MI=0.0462, FA=0.04328, CF=0.08264, over 0.00 frames. 
2024-12-20 06:44:55,932 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 06:44:56,742 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34148,  validation: loss=0.1268, DER=0.1762, ACC=0.945, MI=0.04635, FA=0.04436, CF=0.08548, over 0.00 frames. 
2024-12-20 06:44:56,742 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 06:57:15,002 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-34500.pt
2024-12-20 06:57:20,608 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 06:57:20,757 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 07:02:34,712 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34648, num_updates: 33000, {'loss': 0.1535414308309555, 'DER': 0.2491718807508281, 'ACC': 0.9334765625, 'MI': 0.13894000736106, 'FA': 0.04600662495399337, 'CF': 0.06422524843577475}, batch size: 64, grad_norm: 0.9940725564956665, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:02:34,713 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:02:34,719 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 34648, num_updates: 33000, {'loss': 0.1840330809354782, 'DER': 0.26675163874726876, 'ACC': 0.9230859375, 'MI': 0.12654770575382374, 'FA': 0.0484340859431901, 'CF': 0.09176984705025491}, batch size: 64, grad_norm: 0.9940725564956665, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:02:34,719 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:03:55,626 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34648,  validation: loss=0.1254, DER=0.1726, ACC=0.946, MI=0.04311, FA=0.04565, CF=0.08383, over 0.00 frames. 
2024-12-20 07:03:55,627 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 07:03:56,213 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 34648,  validation: loss=0.1273, DER=0.1764, ACC=0.9446, MI=0.04317, FA=0.04647, CF=0.08675, over 0.00 frames. 
2024-12-20 07:03:56,213 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 07:21:15,384 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35148, num_updates: 33500, {'loss': 0.17801599204540253, 'DER': 0.29233610341643584, 'ACC': 0.92078125, 'MI': 0.15604801477377656, 'FA': 0.0541089566020314, 'CF': 0.08217913204062789}, batch size: 64, grad_norm: 0.8682695031166077, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:21:15,385 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:21:15,385 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35148, num_updates: 33500, {'loss': 0.13615630567073822, 'DER': 0.21310888252148996, 'ACC': 0.9425, 'MI': 0.12625358166189113, 'FA': 0.036353868194842404, 'CF': 0.05050143266475645}, batch size: 64, grad_norm: 0.8682695031166077, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:21:15,385 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:22:35,757 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35148,  validation: loss=0.1222, DER=0.1709, ACC=0.947, MI=0.04624, FA=0.04311, CF=0.08153, over 0.00 frames. 
2024-12-20 07:22:35,758 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 07:22:37,032 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35148,  validation: loss=0.1237, DER=0.1729, ACC=0.946, MI=0.0465, FA=0.04334, CF=0.08302, over 0.00 frames. 
2024-12-20 07:22:37,032 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 07:40:03,736 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35648, num_updates: 34000, {'loss': 0.16753500699996948, 'DER': 0.2791044776119403, 'ACC': 0.9259375, 'MI': 0.13022388059701492, 'FA': 0.07425373134328359, 'CF': 0.07462686567164178}, batch size: 64, grad_norm: 1.0590012073516846, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:40:03,736 (train_accelerate_ddp:698) INFO: [Train] - Epoch 17, batch_idx_train: 35648, num_updates: 34000, {'loss': 0.19774729013442993, 'DER': 0.3098488985982159, 'ACC': 0.91359375, 'MI': 0.15729109776078645, 'FA': 0.05971236118696523, 'CF': 0.09284543965046423}, batch size: 64, grad_norm: 1.0590012073516846, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:40:03,737 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:40:03,737 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 07:41:24,026 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35648,  validation: loss=0.1231, DER=0.1702, ACC=0.947, MI=0.04484, FA=0.04369, CF=0.0817, over 0.00 frames. 
2024-12-20 07:41:24,027 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 07:41:25,090 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 17, batch_idx_train: 35648,  validation: loss=0.125, DER=0.1728, ACC=0.9459, MI=0.04513, FA=0.04404, CF=0.08363, over 0.00 frames. 
2024-12-20 07:41:25,090 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 07:44:57,255 (train_accelerate_ddp:709) INFO: end of epoch 17, batch_idx: 2102 batch_idx_train: 35750, {'loss': 0.1805703043937683, 'DER': 0.29433339446176415, 'ACC': 0.921328125, 'MI': 0.15954520447460113, 'FA': 0.05978360535485054, 'CF': 0.0750045846323125}, batch size: 64, grad_norm: 0.9648563861846924, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:44:57,258 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-17.pt
2024-12-20 07:44:57,261 (train_accelerate_ddp:709) INFO: end of epoch 17, batch_idx: 2102 batch_idx_train: 35750, {'loss': 0.17860431969165802, 'DER': 0.29652238253792085, 'ACC': 0.9128515625, 'MI': 0.11838697743248243, 'FA': 0.06196818349981502, 'CF': 0.11616722160562339}, batch size: 64, grad_norm: 0.9648563861846924, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:45:01,473 (train_accelerate_ddp:557) INFO:  end of epoch 17, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-17.pt 
2024-12-20 07:45:05,219 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 35751, num_updates: 34000, {'loss': 0.1770574301481247, 'DER': 0.2967072948881208, 'ACC': 0.91828125, 'MI': 0.15754047662361287, 'FA': 0.05530289248681099, 'CF': 0.08386392577769693}, batch size: 64, grad_norm: 1.0292726755142212, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:45:05,226 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 35751, num_updates: 34000, {'loss': 0.16786552965641022, 'DER': 0.27844311377245506, 'ACC': 0.9284765625, 'MI': 0.14876497005988024, 'FA': 0.0654940119760479, 'CF': 0.06418413173652694}, batch size: 64, grad_norm: 1.0292726755142212, grad_scale: , lr: 0.00e+00, 
2024-12-20 07:53:57,372 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-36000.pt
2024-12-20 07:54:02,971 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 07:54:03,098 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 08:02:59,598 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36251, num_updates: 34500, {'loss': 0.14889687299728394, 'DER': 0.24689987044234685, 'ACC': 0.9348608070065687, 'MI': 0.13751619470664445, 'FA': 0.04793633166759208, 'CF': 0.06144734406811031}, batch size: 64, grad_norm: 0.7975366115570068, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:02:59,599 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:02:59,602 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36251, num_updates: 34500, {'loss': 0.1628459244966507, 'DER': 0.2703313253012048, 'ACC': 0.925234375, 'MI': 0.13158885542168675, 'FA': 0.04875753012048193, 'CF': 0.08998493975903614}, batch size: 64, grad_norm: 0.7975366115570068, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:02:59,602 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:04:20,736 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36251,  validation: loss=0.122, DER=0.1706, ACC=0.9471, MI=0.04533, FA=0.04389, CF=0.0814, over 0.00 frames. 
2024-12-20 08:04:20,737 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 08:04:21,195 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36251,  validation: loss=0.1238, DER=0.1721, ACC=0.9461, MI=0.04554, FA=0.04365, CF=0.08294, over 0.00 frames. 
2024-12-20 08:04:21,196 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 08:21:49,905 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36751, num_updates: 35000, {'loss': 0.1552286297082901, 'DER': 0.2694151486097795, 'ACC': 0.9355078125, 'MI': 0.1407478427612656, 'FA': 0.08149568552253116, 'CF': 0.047171620325982745}, batch size: 64, grad_norm: 0.8326618075370789, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:21:49,906 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 36751, num_updates: 35000, {'loss': 0.20910125970840454, 'DER': 0.3639947437582129, 'ACC': 0.9042578125, 'MI': 0.18058944997184156, 'FA': 0.08729115825042237, 'CF': 0.09611413553594894}, batch size: 64, grad_norm: 0.8326618075370789, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:21:49,906 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:21:49,906 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:23:10,669 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36751,  validation: loss=0.1244, DER=0.1736, ACC=0.9457, MI=0.0447, FA=0.04505, CF=0.08388, over 0.00 frames. 
2024-12-20 08:23:10,669 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 08:23:11,109 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 36751,  validation: loss=0.1223, DER=0.171, ACC=0.9468, MI=0.0444, FA=0.04487, CF=0.0817, over 0.00 frames. 
2024-12-20 08:23:11,109 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 08:40:36,210 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37251, num_updates: 35500, {'loss': 0.2250480204820633, 'DER': 0.3330339500628705, 'ACC': 0.9050390625, 'MI': 0.1632836357104365, 'FA': 0.06610382611819651, 'CF': 0.10364648823423747}, batch size: 64, grad_norm: 1.172492265701294, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:40:36,210 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:40:36,216 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37251, num_updates: 35500, {'loss': 0.18231721222400665, 'DER': 0.3061828952239911, 'ACC': 0.912109375, 'MI': 0.13217326915957053, 'FA': 0.06368011847463902, 'CF': 0.11032950758978156}, batch size: 64, grad_norm: 1.172492265701294, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:40:36,217 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:41:57,460 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37251,  validation: loss=0.1232, DER=0.1707, ACC=0.947, MI=0.04463, FA=0.04436, CF=0.08167, over 0.00 frames. 
2024-12-20 08:41:57,461 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 08:41:58,403 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37251,  validation: loss=0.125, DER=0.1728, ACC=0.9459, MI=0.04473, FA=0.04452, CF=0.08358, over 0.00 frames. 
2024-12-20 08:41:58,403 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 08:50:39,919 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-37500.pt
2024-12-20 08:50:45,416 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 08:59:42,208 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37751, num_updates: 36000, {'loss': 0.2059059888124466, 'DER': 0.3112453366494937, 'ACC': 0.9125, 'MI': 0.16947948125777226, 'FA': 0.055071948836383014, 'CF': 0.08669390655533843}, batch size: 64, grad_norm: 0.9714236259460449, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:59:42,209 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 08:59:42,211 (train_accelerate_ddp:698) INFO: [Train] - Epoch 18, batch_idx_train: 37751, num_updates: 36000, {'loss': 0.15052670240402222, 'DER': 0.22920783186635532, 'ACC': 0.9399609375, 'MI': 0.11981318483923119, 'FA': 0.06251122687264235, 'CF': 0.04688342015448177}, batch size: 64, grad_norm: 0.9714236259460449, grad_scale: , lr: 0.00e+00, 
2024-12-20 08:59:42,212 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:01:02,666 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37751,  validation: loss=0.1241, DER=0.1721, ACC=0.9464, MI=0.04544, FA=0.04402, CF=0.08267, over 0.00 frames. 
2024-12-20 09:01:02,666 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 09:01:03,500 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 18, batch_idx_train: 37751,  validation: loss=0.1259, DER=0.175, ACC=0.9452, MI=0.04571, FA=0.0446, CF=0.08473, over 0.00 frames. 
2024-12-20 09:01:03,501 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 09:04:34,165 (train_accelerate_ddp:709) INFO: end of epoch 18, batch_idx: 2102 batch_idx_train: 37853, {'loss': 0.17339138686656952, 'DER': 0.27967953386744354, 'ACC': 0.9194921875, 'MI': 0.12527312454479242, 'FA': 0.05881281864530226, 'CF': 0.09559359067734888}, batch size: 64, grad_norm: 0.9999637007713318, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:04:34,172 (train_accelerate_ddp:709) INFO: end of epoch 18, batch_idx: 2102 batch_idx_train: 37853, {'loss': 0.17918157577514648, 'DER': 0.27763448784082534, 'ACC': 0.92203125, 'MI': 0.13927781871775977, 'FA': 0.048268238761974946, 'CF': 0.09008843036109064}, batch size: 64, grad_norm: 0.9999637007713318, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:04:34,174 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-18.pt
2024-12-20 09:04:38,783 (train_accelerate_ddp:557) INFO:  end of epoch 18, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-18.pt 
2024-12-20 09:04:56,205 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 37854, num_updates: 36000, {'loss': 0.19299374520778656, 'DER': 0.32208702064896755, 'ACC': 0.9126953125, 'MI': 0.17311946902654868, 'FA': 0.058997050147492625, 'CF': 0.08997050147492626}, batch size: 64, grad_norm: 0.8261610269546509, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:04:56,210 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 37854, num_updates: 36000, {'loss': 0.17876000702381134, 'DER': 0.29528676888131744, 'ACC': 0.9259375, 'MI': 0.1321219004353587, 'FA': 0.09956464130229037, 'CF': 0.06360022714366836}, batch size: 64, grad_norm: 0.8261610269546509, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:22:27,456 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38354, num_updates: 36500, {'loss': 0.21721719205379486, 'DER': 0.3694706308919507, 'ACC': 0.8952734375, 'MI': 0.19398114575779551, 'FA': 0.05891950688905004, 'CF': 0.11656997824510515}, batch size: 64, grad_norm: 1.2749296426773071, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:22:27,456 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38354, num_updates: 36500, {'loss': 0.22383518517017365, 'DER': 0.35054545454545455, 'ACC': 0.89546875, 'MI': 0.15236363636363637, 'FA': 0.06218181818181818, 'CF': 0.136}, batch size: 64, grad_norm: 1.2749296426773071, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:22:27,457 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:22:27,457 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:23:48,999 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38354,  validation: loss=0.1249, DER=0.1743, ACC=0.9456, MI=0.0467, FA=0.04404, CF=0.08353, over 0.00 frames. 
2024-12-20 09:23:49,000 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 09:23:49,021 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38354,  validation: loss=0.1231, DER=0.1716, ACC=0.9467, MI=0.04635, FA=0.04354, CF=0.08168, over 0.00 frames. 
2024-12-20 09:23:49,021 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 09:41:20,829 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38854, num_updates: 37000, {'loss': 0.1836475133895874, 'DER': 0.30530098831985625, 'ACC': 0.9161328125, 'MI': 0.1761006289308176, 'FA': 0.04869721473495058, 'CF': 0.08050314465408805}, batch size: 64, grad_norm: 0.952710747718811, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:41:20,830 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 38854, num_updates: 37000, {'loss': 0.17061588168144226, 'DER': 0.28160511363636365, 'ACC': 0.922578125, 'MI': 0.16548295454545456, 'FA': 0.04580965909090909, 'CF': 0.0703125}, batch size: 64, grad_norm: 0.952710747718811, grad_scale: , lr: 0.00e+00, 
2024-12-20 09:41:20,830 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:41:20,830 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 09:42:40,229 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38854,  validation: loss=0.1239, DER=0.1712, ACC=0.9467, MI=0.04663, FA=0.04239, CF=0.08219, over 0.00 frames. 
2024-12-20 09:42:40,230 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 09:42:40,980 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 38854,  validation: loss=0.1256, DER=0.1737, ACC=0.9457, MI=0.04698, FA=0.04297, CF=0.08377, over 0.00 frames. 
2024-12-20 09:42:40,980 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 09:47:46,854 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-39000.pt
2024-12-20 09:47:52,620 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 09:47:52,745 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 10:00:09,194 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39354, num_updates: 37500, {'loss': 0.15945234894752502, 'DER': 0.26568331259996447, 'ACC': 0.92515625, 'MI': 0.15603341034298915, 'FA': 0.03483205971210236, 'CF': 0.07481784254487293}, batch size: 64, grad_norm: 0.9361556172370911, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:00:09,195 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:00:09,201 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39354, num_updates: 37500, {'loss': 0.20428456366062164, 'DER': 0.32450927426616244, 'ACC': 0.9023046875, 'MI': 0.14046461372231225, 'FA': 0.05816675670808572, 'CF': 0.12587790383576444}, batch size: 64, grad_norm: 0.9361556172370911, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:00:09,201 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:01:30,614 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39354,  validation: loss=0.124, DER=0.1719, ACC=0.9463, MI=0.04325, FA=0.04553, CF=0.08315, over 0.00 frames. 
2024-12-20 10:01:30,614 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 10:01:31,610 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39354,  validation: loss=0.1258, DER=0.1765, ACC=0.9449, MI=0.0439, FA=0.0468, CF=0.08577, over 0.00 frames. 
2024-12-20 10:01:31,611 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 10:18:57,709 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39854, num_updates: 38000, {'loss': 0.17553098499774933, 'DER': 0.29582806573957016, 'ACC': 0.9163671875, 'MI': 0.14376015893082897, 'FA': 0.061224489795918366, 'CF': 0.09084341701282282}, batch size: 64, grad_norm: 1.0215203762054443, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:18:57,709 (train_accelerate_ddp:698) INFO: [Train] - Epoch 19, batch_idx_train: 39854, num_updates: 38000, {'loss': 0.23112475872039795, 'DER': 0.3865181268882175, 'ACC': 0.89109375, 'MI': 0.15823262839879154, 'FA': 0.08836858006042296, 'CF': 0.13991691842900303}, batch size: 64, grad_norm: 1.0215203762054443, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:18:57,710 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:18:57,710 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:20:18,568 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39854,  validation: loss=0.1226, DER=0.1712, ACC=0.9468, MI=0.04474, FA=0.0442, CF=0.08223, over 0.00 frames. 
2024-12-20 10:20:18,569 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 10:20:19,366 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 19, batch_idx_train: 39854,  validation: loss=0.1242, DER=0.1725, ACC=0.946, MI=0.0451, FA=0.04414, CF=0.08327, over 0.00 frames. 
2024-12-20 10:20:19,366 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 10:23:59,874 (train_accelerate_ddp:709) INFO: end of epoch 19, batch_idx: 2102 batch_idx_train: 39956, {'loss': 0.21348457038402557, 'DER': 0.3003924366749911, 'ACC': 0.9096484375, 'MI': 0.13467713164466644, 'FA': 0.053514092044238314, 'CF': 0.11220121298608633}, batch size: 64, grad_norm: 1.2465662956237793, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:23:59,876 (train_accelerate_ddp:709) INFO: end of epoch 19, batch_idx: 2102 batch_idx_train: 39956, {'loss': 0.20419785380363464, 'DER': 0.3528728961114335, 'ACC': 0.913359375, 'MI': 0.17411491584445735, 'FA': 0.10253433933062488, 'CF': 0.07622364093635133}, batch size: 64, grad_norm: 1.2465662956237793, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:23:59,878 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-19.pt
2024-12-20 10:24:04,101 (train_accelerate_ddp:557) INFO:  end of epoch 19, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-19.pt 
2024-12-20 10:24:09,050 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 39957, num_updates: 38000, {'loss': 0.1908634454011917, 'DER': 0.3235835172921266, 'ACC': 0.912734375, 'MI': 0.17476085356880058, 'FA': 0.06144223693892568, 'CF': 0.08738042678440029}, batch size: 64, grad_norm: 0.956902265548706, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:24:09,051 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 39957, num_updates: 38000, {'loss': 0.15884238481521606, 'DER': 0.25236376138903216, 'ACC': 0.9289453125, 'MI': 0.1738009283135637, 'FA': 0.018222451435447824, 'CF': 0.06034038164002063}, batch size: 64, grad_norm: 0.956902265548706, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:41:55,713 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40457, num_updates: 38500, {'loss': 0.20230703055858612, 'DER': 0.30643647963970727, 'ACC': 0.917734375, 'MI': 0.15087258397447925, 'FA': 0.06680427847626197, 'CF': 0.08875961718896604}, batch size: 64, grad_norm: 1.174885869026184, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:41:55,713 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:41:55,714 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40457, num_updates: 38500, {'loss': 0.17368830740451813, 'DER': 0.29067245119305857, 'ACC': 0.915078125, 'MI': 0.14768618944323933, 'FA': 0.04067245119305857, 'CF': 0.10231381055676067}, batch size: 64, grad_norm: 1.174885869026184, grad_scale: , lr: 0.00e+00, 
2024-12-20 10:41:55,714 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 10:43:16,378 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40457,  validation: loss=0.1237, DER=0.1704, ACC=0.9469, MI=0.0424, FA=0.04574, CF=0.08221, over 0.00 frames. 
2024-12-20 10:43:16,379 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 10:43:17,153 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40457,  validation: loss=0.1256, DER=0.1734, ACC=0.9457, MI=0.04262, FA=0.04637, CF=0.08442, over 0.00 frames. 
2024-12-20 10:43:17,154 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 10:44:45,198 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-40500.pt
2024-12-20 10:44:50,737 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 10:44:50,848 (checkpoint:356) INFO: currently, remove unused epoch-*.pt
2024-12-20 11:01:21,837 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40957, num_updates: 39000, {'loss': 0.21918588876724243, 'DER': 0.31068139963167585, 'ACC': 0.91, 'MI': 0.13406998158379374, 'FA': 0.06298342541436464, 'CF': 0.1136279926335175}, batch size: 64, grad_norm: 1.3988457918167114, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:01:21,837 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:01:21,842 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 40957, num_updates: 39000, {'loss': 0.22247274219989777, 'DER': 0.34725746632832327, 'ACC': 0.90640625, 'MI': 0.13371071637712278, 'FA': 0.09310950614874097, 'CF': 0.1204372438024595}, batch size: 64, grad_norm: 1.3988457918167114, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:01:21,842 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:02:42,414 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40957,  validation: loss=0.1248, DER=0.1705, ACC=0.9466, MI=0.04396, FA=0.04355, CF=0.08304, over 0.00 frames. 
2024-12-20 11:02:42,414 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 11:02:42,685 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 40957,  validation: loss=0.1264, DER=0.1728, ACC=0.9457, MI=0.04414, FA=0.04417, CF=0.08446, over 0.00 frames. 
2024-12-20 11:02:42,686 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 11:20:47,170 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41457, num_updates: 39500, {'loss': 0.15946213901042938, 'DER': 0.2752310536044362, 'ACC': 0.9265625, 'MI': 0.13530499075785582, 'FA': 0.06765249537892791, 'CF': 0.0722735674676525}, batch size: 64, grad_norm: 0.8871500492095947, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:20:47,171 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:20:47,171 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41457, num_updates: 39500, {'loss': 0.18963201344013214, 'DER': 0.29976194836110603, 'ACC': 0.918984375, 'MI': 0.16334004761032778, 'FA': 0.05639992675334188, 'CF': 0.08002197399743637}, batch size: 64, grad_norm: 0.8871500492095947, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:20:47,171 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:22:08,088 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41457,  validation: loss=0.1235, DER=0.1708, ACC=0.9467, MI=0.04363, FA=0.04478, CF=0.08238, over 0.00 frames. 
2024-12-20 11:22:08,089 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 11:22:08,873 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41457,  validation: loss=0.1253, DER=0.1729, ACC=0.9457, MI=0.04364, FA=0.04524, CF=0.08406, over 0.00 frames. 
2024-12-20 11:22:08,873 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 11:39:43,354 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41957, num_updates: 40000, {'loss': 0.1269804835319519, 'DER': 0.20471146822498174, 'ACC': 0.9448828125, 'MI': 0.09715120525931337, 'FA': 0.05460189919649379, 'CF': 0.05295836376917458}, batch size: 64, grad_norm: 0.8226029276847839, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:39:43,354 (train_accelerate_ddp:698) INFO: [Train] - Epoch 20, batch_idx_train: 41957, num_updates: 40000, {'loss': 0.19674265384674072, 'DER': 0.31239193083573485, 'ACC': 0.9166796875, 'MI': 0.1367915465898175, 'FA': 0.07819404418828049, 'CF': 0.09740634005763689}, batch size: 64, grad_norm: 0.8226029276847839, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:39:43,355 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:39:43,355 (train_accelerate_ddp:716) INFO: Computing validation loss
2024-12-20 11:41:04,645 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41957,  validation: loss=0.1225, DER=0.1701, ACC=0.947, MI=0.04396, FA=0.04436, CF=0.08178, over 0.00 frames. 
2024-12-20 11:41:04,645 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 11:41:04,964 (train_accelerate_ddp:724) INFO: [Eval] - Epoch 20, batch_idx_train: 41957,  validation: loss=0.1243, DER=0.1722, ACC=0.9461, MI=0.04413, FA=0.04481, CF=0.08323, over 0.00 frames. 
2024-12-20 11:41:04,964 (train_accelerate_ddp:728) INFO: Maximum memory allocated so far is 8722MB
2024-12-20 11:42:36,728 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/checkpoint-42000.pt
2024-12-20 11:42:42,359 (checkpoint:467) INFO: currently, remove unused checkpoint-*.pt
2024-12-20 11:44:43,555 (train_accelerate_ddp:709) INFO: end of epoch 20, batch_idx: 2102 batch_idx_train: 42059, {'loss': 0.1622532457113266, 'DER': 0.2505226480836237, 'ACC': 0.92484375, 'MI': 0.12857142857142856, 'FA': 0.037282229965156795, 'CF': 0.08466898954703833}, batch size: 64, grad_norm: 0.8195577263832092, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:44:43,558 (checkpoint:75) INFO: Saving checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-20.pt
2024-12-20 11:44:43,561 (train_accelerate_ddp:709) INFO: end of epoch 20, batch_idx: 2102 batch_idx_train: 42059, {'loss': 0.1661595106124878, 'DER': 0.26305329719963866, 'ACC': 0.9287109375, 'MI': 0.14200542005420055, 'FA': 0.05438121047877145, 'CF': 0.06666666666666667}, batch size: 64, grad_norm: 0.8195577263832092, grad_scale: , lr: 0.00e+00, 
2024-12-20 11:44:43,563 (train_accelerate_ddp:1058) INFO: Done!
hltsz02:2559200:2559234 [1] NCCL INFO [Service thread] Connection closed by localRank 1
hltsz02:2559200:2559200 [1] NCCL INFO comm 0xbdf2f00 rank 1 nranks 2 cudaDev 1 busId 88000 - Abort COMPLETE
2024-12-20 11:44:47,786 (train_accelerate_ddp:557) INFO:  end of epoch 20, Saved checkpoint to /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/epoch-20.pt 
2024-12-20 11:44:47,786 (train_accelerate_ddp:1058) INFO: Done!
hltsz02:2559199:2559235 [0] NCCL INFO [Service thread] Connection closed by localRank 0
hltsz02:2559199:2559199 [0] NCCL INFO comm 0xc139340 rank 0 nranks 2 cudaDev 0 busId 3e000 - Abort COMPLETE
2024-12-20 11:44:58,688 (infer:252) INFO: infer data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=1, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/mntcephfs/lee_dataset/asr/musan', rir_path='/mntcephfs/lee_dataset/asr/RIRS_NOISES')
2024-12-20 11:44:58,688 (infer:253) INFO: currently, it will infer dev set.
  0%|          | 0/38 [00:00<?, ?it/s] 68%|██████▊   | 26/38 [00:00<00:00, 254.09it/s]100%|██████████| 38/38 [00:00<00:00, 255.87it/s]
2024-12-20 11:44:58,900 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 11:44:58,902 (ts_vad_dataset:160) INFO: loaded sentence=35613, shortest sent=3200.0, longest sent=64000.0, rs_len=4, segment_shift=1,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
2024-12-20 11:44:59,195 (infer:275) INFO: Device: cuda:0
2024-12-20 11:44:59,195 (infer:290) INFO: infer model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
2024-12-20 11:45:01,108 (infer:201) INFO: params.model_file: /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/best-valid-der.pt
2024-12-20 11:51:22,671 (infer:89) INFO: frame_len: 0.04!!
self.wavlm_fuse_feat_post_norm: False
Model DER:  0.1713474027723726
Model ACC:  0.9465173264114749
  0%|          | 0/38 [00:00<?, ?it/s]  3%|▎         | 1/38 [00:00<00:36,  1.01it/s]  5%|▌         | 2/38 [00:01<00:35,  1.02it/s]  8%|▊         | 3/38 [00:02<00:34,  1.02it/s] 11%|█         | 4/38 [00:03<00:33,  1.02it/s] 13%|█▎        | 5/38 [00:04<00:32,  1.01it/s] 16%|█▌        | 6/38 [00:05<00:31,  1.01it/s] 18%|█▊        | 7/38 [00:06<00:31,  1.01s/it] 21%|██        | 8/38 [00:08<00:37,  1.24s/it] 24%|██▎       | 9/38 [00:09<00:33,  1.15s/it] 26%|██▋       | 10/38 [00:10<00:30,  1.09s/it] 29%|██▉       | 11/38 [00:11<00:28,  1.06s/it] 32%|███▏      | 12/38 [00:12<00:27,  1.04s/it] 34%|███▍      | 13/38 [00:13<00:25,  1.03s/it] 37%|███▋      | 14/38 [00:14<00:24,  1.03s/it] 39%|███▉      | 15/38 [00:15<00:23,  1.02s/it] 42%|████▏     | 16/38 [00:16<00:22,  1.01s/it] 45%|████▍     | 17/38 [00:17<00:21,  1.02s/it] 47%|████▋     | 18/38 [00:18<00:20,  1.03s/it] 50%|█████     | 19/38 [00:19<00:19,  1.02s/it] 53%|█████▎    | 20/38 [00:20<00:18,  1.01s/it] 55%|█████▌    | 21/38 [00:22<00:20,  1.18s/it] 58%|█████▊    | 22/38 [00:23<00:18,  1.13s/it] 61%|██████    | 23/38 [00:24<00:16,  1.08s/it] 63%|██████▎   | 24/38 [00:25<00:14,  1.06s/it] 66%|██████▌   | 25/38 [00:26<00:13,  1.05s/it] 68%|██████▊   | 26/38 [00:27<00:12,  1.04s/it] 71%|███████   | 27/38 [00:28<00:11,  1.02s/it] 74%|███████▎  | 28/38 [00:29<00:09,  1.00it/s] 76%|███████▋  | 29/38 [00:30<00:08,  1.01it/s] 79%|███████▉  | 30/38 [00:31<00:07,  1.02it/s] 82%|████████▏ | 31/38 [00:32<00:06,  1.02it/s] 84%|████████▍ | 32/38 [00:33<00:05,  1.01it/s] 87%|████████▋ | 33/38 [00:34<00:04,  1.00it/s] 89%|████████▉ | 34/38 [00:35<00:04,  1.19s/it] 92%|█████████▏| 35/38 [00:36<00:03,  1.12s/it] 95%|█████████▍| 36/38 [00:37<00:02,  1.07s/it] 97%|█████████▋| 37/38 [00:38<00:01,  1.05s/it]100%|██████████| 38/38 [00:39<00:00,  1.04s/it]100%|██████████| 38/38 [00:39<00:00,  1.05s/it]
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000224 speaking more than once at time 233.386
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000248 speaking more than once at time 34.336
WARNING:  speaker G00000248 speaking more than once at time 197.323
WARNING:  speaker G00000248 speaking more than once at time 209.232
WARNING:  speaker G00000248 speaking more than once at time 277.837
WARNING:  speaker G00000248 speaking more than once at time 303.912
WARNING:  speaker G00000248 speaking more than once at time 324.518
WARNING:  speaker G00000248 speaking more than once at time 419.083
WARNING:  speaker G00000248 speaking more than once at time 637.003
WARNING:  speaker G00000248 speaking more than once at time 706.578
WARNING:  speaker G00000248 speaking more than once at time 711.854
WARNING:  speaker G00000248 speaking more than once at time 736.329
WARNING:  speaker G00000249 speaking more than once at time 802.637
WARNING:  speaker G00000249 speaking more than once at time 808.872
WARNING:  speaker G00000249 speaking more than once at time 901.090
WARNING:  speaker G00000249 speaking more than once at time 1177.512
WARNING:  speaker G00000249 speaking more than once at time 1367.795
WARNING:  speaker G00000249 speaking more than once at time 1372.752
WARNING:  speaker G00000249 speaking more than once at time 1481.134
WARNING:  speaker G00000248 speaking more than once at time 1642.569
WARNING:  speaker G00000248 speaking more than once at time 1785.376
WARNING:  speaker G00000248 speaking more than once at time 1796.237
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000462 speaking more than once at time 512.672
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000070 speaking more than once at time 235.105
WARNING:  speaker G00000070 speaking more than once at time 338.560
WARNING:  speaker G00000070 speaking more than once at time 531.848
WARNING:  speaker G00000070 speaking more than once at time 1256.024
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000619 speaking more than once at time 703.496
WARNING:  speaker G00000619 speaking more than once at time 931.210
WARNING:  speaker G00000619 speaking more than once at time 1324.073
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000096 speaking more than once at time 1058.160
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000200 speaking more than once at time 467.112
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000144 speaking more than once at time 1965.712
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000079 speaking more than once at time 1240.704
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000058 speaking more than once at time 236.752
WARNING:  speaker G00000058 speaking more than once at time 381.798
WARNING:  speaker G00000058 speaking more than once at time 398.176
WARNING:  speaker G00000058 speaking more than once at time 424.320
WARNING:  speaker G00000058 speaking more than once at time 609.793
WARNING:  speaker G00000058 speaking more than once at time 615.312
WARNING:  speaker G00000058 speaking more than once at time 705.696
WARNING:  speaker G00000058 speaking more than once at time 844.644
WARNING:  speaker G00000058 speaking more than once at time 1217.812
WARNING:  speaker G00000058 speaking more than once at time 1318.835
WARNING:  speaker G00000058 speaking more than once at time 1732.634
WARNING:  speaker G00000058 speaking more than once at time 1804.383
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000073 speaking more than once at time 1649.290
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
WARNING:  speaker G00000448 speaking more than once at time 86.595
WARNING:  speaker G00000448 speaking more than once at time 751.088
WARNING:  speaker G00000448 speaking more than once at time 1162.346
WARNING:  speaker G00000448 speaking more than once at time 1285.546
Eval for threshold 0.2 DER=31.31, miss=0.50, falarm=27.90, confusion=2.91


Eval for threshold 0.3 DER=24.93, miss=0.93, falarm=19.37, confusion=4.63


Eval for threshold 0.35 DER=21.68, miss=1.21, falarm=14.86, confusion=5.61


Eval for threshold 0.4 DER=18.89, miss=1.53, falarm=10.83, confusion=6.52


Eval for threshold 0.45 DER=17.14, miss=2.04, falarm=8.03, confusion=7.07


Eval for threshold 0.5 DER=16.37, miss=3.01, falarm=6.10, confusion=7.26


Eval for threshold 0.55 DER=16.98, miss=5.03, falarm=5.22, confusion=6.73


Eval for threshold 0.6 DER=18.54, miss=7.96, falarm=4.77, confusion=5.81


Eval for threshold 0.7 DER=23.08, miss=15.49, falarm=3.96, confusion=3.64


Eval for threshold 0.8 DER=27.48, miss=22.77, falarm=3.10, confusion=1.61


2024-12-20 11:52:19,900 (infer:252) INFO: infer data_cfg: TSVADDataConfig(data_dir='/data/maduo/exp/speaker_diarization/ts_vad2/data/magicdata-ramc', ts_len=6000, rs_len=4, segment_shift=1, spk_path='/data/maduo/model_hub/ts_vad/spk_embed/magicdata-ramc/SpeakerEmbedding', speech_encoder_type='CAM++', speaker_embedding_name_dir='cam++_zh-cn_200k_feature_dir', speaker_embed_dim=192, noise_ratio=0.8, zero_ratio=0.3, sample_rate=16000, max_num_speaker=4, dataset_name='alimeeting', embed_input=False, embed_len=1, embed_shift=0.4, label_rate=25, random_channel=False, random_mask_speaker_prob=0.0, random_mask_speaker_step=0, musan_path='/mntcephfs/lee_dataset/asr/musan', rir_path='/mntcephfs/lee_dataset/asr/RIRS_NOISES')
2024-12-20 11:52:19,900 (infer:253) INFO: currently, it will infer test set.
  0%|          | 0/86 [00:00<?, ?it/s] 29%|██▉       | 25/86 [00:00<00:00, 248.38it/s] 58%|█████▊    | 50/86 [00:00<00:00, 215.18it/s] 84%|████████▎ | 72/86 [00:00<00:00, 134.96it/s]100%|██████████| 86/86 [00:00<00:00, 160.81it/s]
2024-12-20 11:52:20,568 (ts_vad_dataset:152) INFO: model expect fbank as input , fbank_input should be True !!!
2024-12-20 11:52:20,571 (ts_vad_dataset:160) INFO: loaded sentence=74337, shortest sent=640.0, longest sent=64000.0, rs_len=4, segment_shift=1,  rir=False, musan=False, noise_ratio=0.8, zero_ratio=0.3 
2024-12-20 11:52:20,865 (infer:275) INFO: Device: cuda:0
2024-12-20 11:52:20,866 (infer:290) INFO: infer model_cfg: TSVADConfig(speech_encoder_path='/data/maduo/model_hub/speaker_pretrain_model/zh_cn/modelscope/speech_campplus_sv_zh-cn_16k-common/campplus_cn_common.bin', speech_encoder_type='CAM++', freeze_speech_encoder_updates=4000, num_attention_head=4, num_transformer_layer=2, transformer_embed_dim=384, transformer_ffn_embed_dim=1536, speaker_embed_dim=192, dropout=0.1, use_spk_embed=True, feature_grad_mult=0.1, whisper_n_mels=80, select_encoder_layer_nums=6, wavlm_fuse_feat_post_norm=False, speech_encoder_config='/mntcephfs/lab_data/maduo/model_hub/speaker_pretrain_model/wav-bert2.0/config.json', single_backend_type='mamba', multi_backend_type='mamba')
2024-12-20 11:52:22,158 (infer:201) INFO: params.model_file: /data/maduo/exp/speaker_diarization/ts_vad2/magicdata-ramc-ts_vad2_two_gpus_freeze_with_musan_rirs_cam++_200k_zh_cn_epoch20_front_fix_seed_lr2e4_both_mamba/best-valid-der.pt
2024-12-20 12:05:54,715 (infer:89) INFO: frame_len: 0.04!!
self.wavlm_fuse_feat_post_norm: False
Model DER:  0.17740034809784908
Model ACC:  0.9502174430493259
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:01<01:28,  1.04s/it]  2%|▏         | 2/86 [00:02<01:27,  1.04s/it]  3%|▎         | 3/86 [00:02<01:06,  1.25it/s]  5%|▍         | 4/86 [00:03<00:56,  1.46it/s]  6%|▌         | 5/86 [00:03<00:49,  1.64it/s]  7%|▋         | 6/86 [00:04<00:45,  1.77it/s]  8%|▊         | 7/86 [00:04<00:43,  1.82it/s]  9%|▉         | 8/86 [00:05<00:42,  1.85it/s] 10%|█         | 9/86 [00:05<00:40,  1.91it/s] 12%|█▏        | 10/86 [00:06<00:38,  1.96it/s] 13%|█▎        | 11/86 [00:06<00:38,  1.97it/s] 14%|█▍        | 12/86 [00:07<00:37,  1.96it/s] 15%|█▌        | 13/86 [00:07<00:37,  1.96it/s] 16%|█▋        | 14/86 [00:08<00:36,  1.98it/s] 17%|█▋        | 15/86 [00:09<00:48,  1.48it/s] 19%|█▊        | 16/86 [00:10<00:55,  1.25it/s] 20%|█▉        | 17/86 [00:11<01:00,  1.14it/s] 21%|██        | 18/86 [00:12<01:03,  1.08it/s] 22%|██▏       | 19/86 [00:13<01:03,  1.06it/s] 23%|██▎       | 20/86 [00:14<01:02,  1.05it/s] 24%|██▍       | 21/86 [00:15<01:03,  1.03it/s] 26%|██▌       | 22/86 [00:16<01:03,  1.01it/s] 27%|██▋       | 23/86 [00:18<01:27,  1.39s/it] 28%|██▊       | 24/86 [00:19<01:19,  1.29s/it] 29%|██▉       | 25/86 [00:20<01:13,  1.20s/it] 30%|███       | 26/86 [00:21<01:08,  1.14s/it] 31%|███▏      | 27/86 [00:22<01:05,  1.12s/it] 33%|███▎      | 28/86 [00:23<01:03,  1.10s/it] 34%|███▎      | 29/86 [00:24<01:01,  1.08s/it] 35%|███▍      | 30/86 [00:25<00:59,  1.07s/it] 36%|███▌      | 31/86 [00:26<00:58,  1.06s/it] 37%|███▋      | 32/86 [00:28<00:57,  1.06s/it] 38%|███▊      | 33/86 [00:29<00:55,  1.05s/it] 40%|███▉      | 34/86 [00:30<00:54,  1.05s/it] 41%|████      | 35/86 [00:31<00:53,  1.06s/it] 42%|████▏     | 36/86 [00:32<00:52,  1.06s/it] 43%|████▎     | 37/86 [00:33<00:51,  1.06s/it] 44%|████▍     | 38/86 [00:34<00:50,  1.06s/it] 45%|████▌     | 39/86 [00:35<00:49,  1.06s/it] 47%|████▋     | 40/86 [00:36<00:48,  1.06s/it] 48%|████▊     | 41/86 [00:37<00:47,  1.05s/it] 49%|████▉     | 42/86 [00:38<00:45,  1.04s/it] 50%|█████     | 43/86 [00:39<00:44,  1.04s/it] 51%|█████     | 44/86 [00:40<00:43,  1.04s/it] 52%|█████▏    | 45/86 [00:41<00:42,  1.03s/it] 53%|█████▎    | 46/86 [00:42<00:41,  1.04s/it] 55%|█████▍    | 47/86 [00:43<00:40,  1.04s/it] 56%|█████▌    | 48/86 [00:45<00:52,  1.39s/it] 57%|█████▋    | 49/86 [00:47<00:48,  1.30s/it] 58%|█████▊    | 50/86 [00:48<00:44,  1.24s/it] 59%|█████▉    | 51/86 [00:49<00:41,  1.18s/it] 60%|██████    | 52/86 [00:50<00:38,  1.13s/it] 62%|██████▏   | 53/86 [00:51<00:36,  1.10s/it] 63%|██████▎   | 54/86 [00:52<00:34,  1.08s/it] 64%|██████▍   | 55/86 [00:53<00:33,  1.07s/it] 65%|██████▌   | 56/86 [00:54<00:31,  1.06s/it] 66%|██████▋   | 57/86 [00:55<00:30,  1.06s/it] 67%|██████▋   | 58/86 [00:56<00:29,  1.06s/it] 69%|██████▊   | 59/86 [00:57<00:28,  1.06s/it] 70%|██████▉   | 60/86 [00:58<00:27,  1.05s/it] 71%|███████   | 61/86 [00:59<00:26,  1.06s/it] 72%|███████▏  | 62/86 [01:00<00:25,  1.06s/it] 73%|███████▎  | 63/86 [01:01<00:24,  1.05s/it] 74%|███████▍  | 64/86 [01:02<00:23,  1.05s/it] 76%|███████▌  | 65/86 [01:03<00:21,  1.04s/it] 77%|███████▋  | 66/86 [01:04<00:20,  1.03s/it] 78%|███████▊  | 67/86 [01:05<00:19,  1.04s/it] 79%|███████▉  | 68/86 [01:06<00:18,  1.04s/it] 80%|████████  | 69/86 [01:07<00:17,  1.05s/it] 81%|████████▏ | 70/86 [01:08<00:16,  1.06s/it] 83%|████████▎ | 71/86 [01:10<00:15,  1.06s/it] 84%|████████▎ | 72/86 [01:12<00:19,  1.38s/it] 85%|████████▍ | 73/86 [01:13<00:16,  1.30s/it] 86%|████████▌ | 74/86 [01:14<00:14,  1.24s/it] 87%|████████▋ | 75/86 [01:15<00:13,  1.19s/it] 88%|████████▊ | 76/86 [01:16<00:11,  1.15s/it] 90%|████████▉ | 77/86 [01:17<00:10,  1.12s/it] 91%|█████████ | 78/86 [01:18<00:08,  1.10s/it] 92%|█████████▏| 79/86 [01:19<00:07,  1.08s/it] 93%|█████████▎| 80/86 [01:20<00:06,  1.07s/it] 94%|█████████▍| 81/86 [01:21<00:05,  1.05s/it] 95%|█████████▌| 82/86 [01:22<00:04,  1.04s/it] 97%|█████████▋| 83/86 [01:23<00:03,  1.05s/it] 98%|█████████▊| 84/86 [01:24<00:02,  1.06s/it] 99%|█████████▉| 85/86 [01:25<00:01,  1.05s/it]100%|██████████| 86/86 [01:26<00:00,  1.05s/it]100%|██████████| 86/86 [01:26<00:00,  1.01s/it]
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000169 speaking more than once at time 124.835
WARNING:  speaker G00000168 speaking more than once at time 359.176
WARNING:  speaker G00000168 speaking more than once at time 388.331
WARNING:  speaker G00000168 speaking more than once at time 906.248
WARNING:  speaker G00000168 speaking more than once at time 1238.088
WARNING:  speaker G00000169 speaking more than once at time 1363.458
WARNING:  speaker G00000168 speaking more than once at time 1471.048
WARNING:  speaker G00000169 speaking more than once at time 1780.168
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000464 speaking more than once at time 452.378
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000131 speaking more than once at time 528.962
WARNING:  speaker G00000131 speaking more than once at time 532.848
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000185 speaking more than once at time 1421.653
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000290 speaking more than once at time 1526.856
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000583 speaking more than once at time 280.592
WARNING:  speaker G00000583 speaking more than once at time 406.701
WARNING:  speaker G00000583 speaking more than once at time 971.588
WARNING:  speaker G00000584 speaking more than once at time 1181.130
WARNING:  speaker G00000583 speaking more than once at time 1333.227
WARNING:  speaker G00000583 speaking more than once at time 1438.255
WARNING:  speaker G00000583 speaking more than once at time 1455.758
WARNING:  speaker G00000583 speaking more than once at time 1714.851
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000639 speaking more than once at time 1024.776
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000364 speaking more than once at time 324.970
WARNING:  speaker G00000364 speaking more than once at time 1355.424
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000231 speaking more than once at time 371.369
WARNING:  speaker G00000231 speaking more than once at time 933.760
WARNING:  speaker G00000231 speaking more than once at time 1173.609
WARNING:  speaker G00000230 speaking more than once at time 1183.944
WARNING:  speaker G00000231 speaking more than once at time 1236.736
WARNING:  speaker G00000231 speaking more than once at time 1503.376
WARNING:  speaker G00000231 speaking more than once at time 1542.736
WARNING:  speaker G00000231 speaking more than once at time 1584.552
WARNING:  speaker G00000231 speaking more than once at time 1600.000
WARNING:  speaker G00000231 speaking more than once at time 1656.000
WARNING:  speaker G00000231 speaking more than once at time 1671.752
WARNING:  speaker G00000230 speaking more than once at time 1737.416
WARNING:  speaker G00000230 speaking more than once at time 1752.809
WARNING:  speaker G00000231 speaking more than once at time 1802.152
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000283 speaking more than once at time 1151.936
WARNING:  speaker G00000282 speaking more than once at time 1329.656
WARNING:  speaker G00000283 speaking more than once at time 1500.328
WARNING:  speaker G00000283 speaking more than once at time 1501.544
WARNING:  speaker G00000283 speaking more than once at time 1765.672
WARNING:  speaker G00000283 speaking more than once at time 1795.024
WARNING:  speaker G00000283 speaking more than once at time 1844.736
WARNING:  speaker G00000283 speaking more than once at time 1915.304
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000671 speaking more than once at time 922.378
WARNING:  speaker G00000672 speaking more than once at time 1106.322
WARNING:  speaker G00000672 speaking more than once at time 1110.122
WARNING:  speaker G00000672 speaking more than once at time 1151.858
WARNING:  speaker G00000672 speaking more than once at time 1172.058
WARNING:  speaker G00000671 speaking more than once at time 1292.322
WARNING:  speaker G00000672 speaking more than once at time 1841.738
WARNING:  speaker G00000672 speaking more than once at time 1848.514
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000045 speaking more than once at time 103.272
WARNING:  speaker G00000045 speaking more than once at time 506.368
WARNING:  speaker G00000045 speaking more than once at time 735.968
WARNING:  speaker G00000046 speaking more than once at time 1088.608
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000092 speaking more than once at time 1142.634
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000243 speaking more than once at time 73.258
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000537 speaking more than once at time 297.482
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000284 speaking more than once at time 357.992
WARNING:  speaker G00000284 speaking more than once at time 482.832
WARNING:  speaker G00000285 speaking more than once at time 1135.592
WARNING:  speaker G00000284 speaking more than once at time 1421.712
WARNING:  speaker G00000285 speaking more than once at time 1442.656
WARNING:  speaker G00000284 speaking more than once at time 1508.323
WARNING:  speaker G00000284 speaking more than once at time 1630.563
WARNING:  speaker G00000284 speaking more than once at time 1798.032
WARNING:  speaker G00000285 speaking more than once at time 1886.980
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000326 speaking more than once at time 387.722
WARNING:  speaker G00000327 speaking more than once at time 632.136
WARNING:  speaker G00000326 speaking more than once at time 839.436
WARNING:  speaker G00000327 speaking more than once at time 1241.096
WARNING:  speaker G00000327 speaking more than once at time 1695.756
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000490 speaking more than once at time 405.167
WARNING:  speaker G00000489 speaking more than once at time 678.490
WARNING:  speaker G00000490 speaking more than once at time 1127.747
WARNING:  speaker G00000490 speaking more than once at time 1860.903
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000528 speaking more than once at time 1780.944
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000534 speaking more than once at time 116.488
WARNING:  speaker G00000533 speaking more than once at time 543.008
WARNING:  speaker G00000533 speaking more than once at time 666.832
WARNING:  speaker G00000533 speaking more than once at time 1035.624
WARNING:  speaker G00000533 speaking more than once at time 1793.704
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000621 speaking more than once at time 377.309
WARNING:  speaker G00000621 speaking more than once at time 477.125
WARNING:  speaker G00000622 speaking more than once at time 855.493
WARNING:  speaker G00000622 speaking more than once at time 864.773
WARNING:  speaker G00000621 speaking more than once at time 899.869
WARNING:  speaker G00000621 speaking more than once at time 916.613
WARNING:  speaker G00000621 speaking more than once at time 992.837
WARNING:  speaker G00000621 speaking more than once at time 1235.525
WARNING:  speaker G00000621 speaking more than once at time 1722.885
WARNING:  speaker G00000622 speaking more than once at time 1774.429
WARNING:  speaker G00000621 speaking more than once at time 1818.725
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000680 speaking more than once at time 130.024
WARNING:  speaker G00000680 speaking more than once at time 256.099
WARNING:  speaker G00000680 speaking more than once at time 804.904
WARNING:  speaker G00000680 speaking more than once at time 1129.232
WARNING:  speaker G00000680 speaking more than once at time 1223.816
WARNING:  speaker G00000680 speaking more than once at time 1411.528
WARNING:  speaker G00000680 speaking more than once at time 1536.809
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000392 speaking more than once at time 795.224
WARNING:  speaker G00000391 speaking more than once at time 809.608
WARNING:  speaker G00000391 speaking more than once at time 1701.352
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000430 speaking more than once at time 1307.916
WARNING:  speaker G00000430 speaking more than once at time 1667.180
WARNING:  speaker G00000429 speaking more than once at time 1823.436
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000662 speaking more than once at time 31.144
WARNING:  speaker G00000661 speaking more than once at time 358.408
WARNING:  speaker G00000661 speaking more than once at time 752.928
WARNING:  speaker G00000661 speaking more than once at time 900.488
WARNING:  speaker G00000662 speaking more than once at time 1328.008
WARNING:  speaker G00000662 speaking more than once at time 1504.744
WARNING:  speaker G00000662 speaking more than once at time 1852.488
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000141 speaking more than once at time 1731.995
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000322 speaking more than once at time 1789.851
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000155 speaking more than once at time 1816.256
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000341 speaking more than once at time 1127.215
WARNING:  speaker G00000094 speaking more than once at time 1272.240
WARNING:  speaker G00000094 speaking more than once at time 1272.240
Eval for threshold 0.2 DER=27.74, miss=0.64, falarm=24.59, confusion=2.52


Eval for threshold 0.3 DER=22.42, miss=1.16, falarm=17.76, confusion=3.50


Eval for threshold 0.35 DER=20.23, miss=1.54, falarm=14.57, confusion=4.11


Eval for threshold 0.4 DER=18.22, miss=2.10, falarm=11.21, confusion=4.91


Eval for threshold 0.45 DER=16.61, miss=3.09, falarm=7.83, confusion=5.69


Eval for threshold 0.5 DER=16.59, miss=5.25, falarm=5.95, confusion=5.39


Eval for threshold 0.55 DER=17.80, miss=8.22, falarm=5.19, confusion=4.39


Eval for threshold 0.6 DER=19.27, miss=10.98, falarm=4.72, confusion=3.57


Eval for threshold 0.7 DER=22.25, miss=15.80, falarm=3.97, confusion=2.48


Eval for threshold 0.8 DER=25.93, miss=20.88, falarm=3.26, confusion=1.79


